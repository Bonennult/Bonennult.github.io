<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>泛函分析笔记7：弱收敛与弱星收敛</title>
    <url>/2020/12/26/functional-analysis/ch4-4-convergence/</url>
    <content><![CDATA[<p>一致有界性原理的一个应用就是序列和算子的收敛性分析。</p>
<h2 id="1-序列收敛性"><a href="#1-序列收敛性" class="headerlink" title="1. 序列收敛性"></a>1. 序列收敛性</h2><p>$(X,\Vert\cdot\Vert)$，有 $x_n,x\in X$，称 $x_n$ <strong>强收敛</strong>到 $x$，若 $\Vert x_n-x\Vert \to 0$；称 $x_n$ <strong>弱收敛</strong>到 $x$ 若 $\forall f\in X’$ 都有 $f(x_n)\to f(x)$，记为 $x_n \stackrel{w}{\longrightarrow} x.$</p>
<p>关于弱收敛有以下几条性质：</p>
<ul>
<li>若 $x_n \stackrel{w}{\longrightarrow} x, x_n \stackrel{w}{\longrightarrow} y$，则 $x=y$；</li>
<li>若 $x_n \stackrel{w}{\longrightarrow} x$，则存在 $c\ge0, \Vert x_n\Vert \le c.$</li>
</ul>
<a id="more"></a>
<p>证明：仅证第二条。这个性质说明 $x_n$ 有界，因此容易想到需要用一致有界性原理证明，但是该原理说明的是算子的一致有界，这里是元素 $x_n$ 有界，因此又可以想到上一篇讲到的典范映射 $J:X\to X’’$ 从元素映射到算子。因此这里考虑 $X’$ 上的线性泛函 $g_n= J(x_n):X’\to \mathbb{R}$，有 $g_n(f)=f(x_n),\forall f\in X’.$ 于是有 $f(x_n)\to f(x)$，因而固定任一 $f$，都有 $\sup_n g_n(f) &lt; \infty$，同时由于 $X’$ 总为 Banach 空间，利用一致有界性原理有 $\sup_n \Vert g_n\Vert =\sup_n \Vert x_n\Vert &lt; \infty$。证毕。</p>
<blockquote>
<p><strong>定理</strong>：$(X,\Vert\cdot\Vert)$，有 $x_n,x\in X$，则 $x_n \stackrel{w}{\longrightarrow} x$ <strong>当且仅当</strong>：</p>
<ol>
<li>存在 $c\ge0,\Vert x_n\Vert\le c$；</li>
<li>并且存在 $M\subset X’,\overline{\text{span}M}=X’$，对 $\forall f\in M, f(x_n)\to f(x).$（此时 $M$ 称为<strong>完全集</strong>）</li>
</ol>
<p><strong>NOTE</strong>：该定理简化了弱收敛的判断条件，只需要在 $X’$ 的一个子集上判断函数值是否收敛。</p>
</blockquote>
<p>证明：$”\Longrightarrow”$ 易证；</p>
<p>$”\Longleftarrow”$，首先考虑 $\forall f\in \text{span}M$，容易得到 $f(x_n)\to f(x)$。然后对 $\forall g\in X’$，那么存在 $f_m\in\text{span}M$ 使得 $\Vert f_m-g\Vert \le 1/m$，因此</p>
<script type="math/tex; mode=display">
\begin{align}
|g(x_n)-g(x)|&\le |g(x_n)-f_m(x_n)|+|f_m(x_n)-f_m(x)|+|f_m(x)-g(x)| \\
&\le \frac{1}{m}(\Vert x_n\Vert+\Vert x\Vert)+|f_m(x_n)-f_m(x)| \to 0(m,n\to\infty)
\end{align}</script><p>证毕。</p>
<p><strong><em>例子 1</em></strong>：考虑 $X=\ell^p(1&lt;p&lt;\infty)$，有 $(\ell^p)’=\ell^q, 1/p+1/q=1.$ 考虑线性泛函 $f_y(x)=\sum_i y_ix_i,y\in \ell^q$，有 $\Vert f_y\Vert=\Vert y\Vert_q$。我们考虑 $X’$ 的子空间 $M=\{e_n,n\ge1\}$，其中 $e_n=(…,0,1,0,…)$ 表示只有第 $n$ 个分量为 1，其余为 0。那么 $\overline{\text{span}M}=X’$，因此要想验证 $x_n$ 是否弱收敛到 $x$ 就只需要验证：1）其有界性；2）对每个 $f_{e_k},k\ge1$ 是否有 $f_{e_k}(x_n)\to f_{e_k}(x)(n\to\infty).$</p>
<p>强收敛与弱收敛之间有如下关系：</p>
<ul>
<li>$x_n\to x \Longrightarrow x_n \stackrel{w}{\longrightarrow} x$(即强收敛可以导出弱收敛)；</li>
<li>若 $\text{dim}X&lt;\infty$，则 $x_n \stackrel{w}{\longrightarrow} x\Longrightarrow x_n\to x$(<strong>有限维</strong>赋范空间中，强收敛与弱收敛等价)；</li>
</ul>
<p>证明：仅证第二条。设 $\text{dim}X=n&lt;\infty$，有限维赋范空间中我们可以找到一组基，$x_k=\lambda_{k,1}e_1+\cdots+\lambda_{k,n}e_n$，$x=\lambda_1 e_1+\cdots+\lambda_n e_n$。那么</p>
<script type="math/tex; mode=display">
\lambda_{k,1}f(e_1)+\cdots+\lambda_{k,n}f(e_n) \to \lambda_{1}f(e_1)+\cdots+\lambda_{n}f(e_n), \quad\forall f\in X'</script><p>由于 $f\in X’$ 任取，那么我们可以取 $f_i(y)=\mu_i$，其中 $y=\mu_1 e_1+\cdots+\mu_n e_n$，即 $f_i$ 取出来第 $i$ 个坐标系数。由此可以得到 $\lambda_{k,i}\to\lambda_i(k\to\infty)$，然后就容易得到 $x_n\to x.$ 证毕。</p>
<p><strong><em>例子 2</em></strong>：有些无穷维空间中也可以得到 $x_n \stackrel{w}{\longrightarrow} x\iff x_n\to x$，例如 $\ell^1.$</p>
<p><strong><em>例子 3</em></strong>：无穷维 Hilbert 空间（$\ell^2$，注意只有 $2-$范数才能定义出对应的内积），考虑 $\{e_1,e_2,\ldots\}$ 为 $H$ 的标准正交集，那么有 $e_n \stackrel{w}{\longrightarrow} 0$ 但是 $e_n \nrightarrow 0$。考虑 $\forall f\in H’$，存在唯一的 $z_0\in H, f(x)=\langle x,z_0\rangle$，由Bessel方程 $\sum_n|\langle e_n,z_0\rangle|^2\le \Vert z_0\Vert^2$，因此 $f(e_n)\to 0(n\to \infty),\forall f\in H’$，但另一方面 $\Vert e_n\Vert=1\nrightarrow0$。</p>
<h2 id="2-线性泛函收敛性"><a href="#2-线性泛函收敛性" class="headerlink" title="2. 线性泛函收敛性"></a>2. 线性泛函收敛性</h2><p>对于算子的收敛性，如线性泛函 $f_n\in X’$ 或者有界线性算子 $T\in B(X,Y)$，收敛性的定义跟上面序列的收敛性是相似的，但是又略有不同。下面就先给出线性泛函收敛性的分析。</p>
<p>同样考虑赋范空间 $X$，$f,f_n\in X’$，称 $f_n$ <strong>弱星收敛</strong>到 $f$，若任取 $x\in X$ 都有 $f_n(x)\to f(x)$，记为 $f_n \stackrel{w\star}{\longrightarrow} f.$</p>
<p><strong>NOTE</strong>：实际上这里的弱星收敛跟序列的弱收敛是完全对称的，因此他们的性质也是类似的。</p>
<ul>
<li>弱星收敛极限 $f$ 唯一；</li>
<li>$\{f_n\}$ 的任意子列均弱星收敛到 $f$；</li>
<li>若 $X$ 为 Banach 空间，则 $\{f_n\}$ 在 $X’$ 中为<strong>有界集</strong>。</li>
</ul>
<p>证明：仅证第三条，对于任意 $x\in X$，有 $f_n(x)\to f(x)$，因此 $f_n(x)$ 有界，由一致有界性原理，$\sup_n \Vert f_n\Vert&lt;\infty.$ 证毕。</p>
<blockquote>
<p><strong>定理</strong>：$X$ 为 <strong>Banach 空间</strong>，$f_n,f\in X’$，则 $f_n \stackrel{w\star}{\longrightarrow} f$ <strong>当且仅当</strong>：</p>
<ol>
<li>存在 $c\ge0,\Vert f_n\Vert\le c$；</li>
<li>并且存在 $M\subset X,\overline{\text{span}M}=X$，对 $\forall x\in M, f_n(x)\to f(x).$</li>
</ol>
<p><strong>NOTE</strong>：该性质与序列弱收敛的性质完全对称，证明省略。</p>
<p><strong>NOTE</strong>：对于 $X’$ 中的线性算子 $f$，也有范数的定义，因此我们也可以按照序列的收敛性来定义算子的收敛性。这个时候就用 $X’$ 代替上面的 $X$，用 $X’’$ 代替上面的 $X’$。我们可以得到什么样的强收敛和弱收敛定义呢？（下面并不是标准的数学定义，只是我为了引出之后的内容做的解释）</p>
<p>对于 $f,f_n\in X’$，若满足 $\Vert f_n-f\Vert\to 0$，则称 $f_n$ <strong>一致收敛</strong>到 $f$；若对 $\forall g\in X’’$，都有 $g(f_n)\to g(f)$，那么称 $f_n$ <strong>强收敛</strong>到 $f$；弱收敛的定义暂且不管。</p>
<p>注意从这个定义的字面意思来看，这里的一致收敛对应于上面序列的强收敛；这里的强收敛对应上面序列的弱收敛，它实际上也就对应于弱星收敛。这里就有两个值得思考的问题：1）<strong>一致收敛和强收敛的区别是什么？</strong>2）这里的强收敛为什么对应上面的弱收敛？</p>
<p>先看第2个问题：讲 Hahn-Banach 定理应用的时候我们讲到了典范映射，如果 $X$ 为自反的，那么任意一个 $g_0\in X’’$ 都唯一的对应于 $X$ 中的元素 $x_0$，并且满足 $g_0(f)=f(x_0),\forall f\in X’$。假设 $X$ 是自反的，那么上面的强收敛定义就可以表述为 $\forall x\in X$，都有 $f_n(x)\to f(x)$，注意看！这是不是就是线性泛函弱星收敛的定义！也对应了序列的弱收敛。不过弱星收敛的定义里面并没有要求 $X$ 是自反的。</p>
<p>那么再看第1个问题：一致收敛中要求 $\Vert f_n-f\Vert\to0$，线性算子的范数是针对整个源空间考虑的；而强收敛中对每个 $x\in X$，关注 $f_n(x)\to f(x)$，也就是说关注的是每一个局部点。因此一致收敛要强于强收敛。</p>
</blockquote>
<h2 id="3-一般有界线性算子收敛性"><a href="#3-一般有界线性算子收敛性" class="headerlink" title="3. 一般有界线性算子收敛性"></a>3. 一般有界线性算子收敛性</h2><p>实际上一致收敛、强收敛、弱收敛的概念可以扩展到任意的有界线性算子定义。</p>
<p>设 $X,Y$ 为赋范空间，$T_n\in B(X,Y),T:X\to Y$ 为线性算子，有三种收敛性：</p>
<ul>
<li>$\{T_n\}$ <strong>一致收敛</strong>到 $T$，若 $\Vert T_n-T\Vert \to 0$；</li>
<li>$\{T_n\}$ <strong>强收敛</strong>到 $T$，若 $\forall x\in X,T_n x\to Tx$；</li>
<li>$\{T_n\}$ <strong>弱收敛</strong>到 $T$，若任取 $x\in X, f\in Y’$，$f(T_nx)\to f(Tx)$。</li>
</ul>
<p>容易看出来一致收敛 $\Longrightarrow$ 强收敛 $\Longrightarrow$ 弱收敛，但是反向则不成立，可以举出对应的反例。</p>
<p><strong><em>例子 4</em></strong>(强收敛 $\nRightarrow$ 一致收敛)：$X=Y=\ell^2$，$T_n:\ell^2\to\ell^2$ 有</p>
<script type="math/tex; mode=display">
T_n: (x_1,x_2,\cdots) \mapsto (0,\cdots,0,x_{n+1},x_{n+2},\cdots)</script><p>容易验证 $T_n$ 为有界线性算子，$\Vert T_n\Vert=1$。可以验证 $T_n$ 强收敛到 $0$ 算子，即 $T_0x\equiv 0$。但是 $\Vert T_n-T_0\Vert=1\nrightarrow 0$，即不满足一致收敛。</p>
<p><strong><em>例子 5</em></strong>(弱收敛 $\nRightarrow$ 强收敛)：$X=Y=\ell^2$，$T_n:\ell^2\to\ell^2$ 有</p>
<script type="math/tex; mode=display">
T_n:(x_1,x_2,\cdots)\mapsto(0_1,\cdots,0_n,x_1,x_2,\cdots)</script><p>可以验证 $T_n$ 为有界线性算子，并且 $\Vert T_n\Vert=1$。是否有 $T_n$ 弱收敛到某个 $T$ 呢？考虑任意 $f\in(\ell^2)’$，都存在唯一的 $z\in\ell^2$，$f(x)=\langle x,z\rangle$，所以 $f(T_nx)=x_1\overline{z_{n+1}}+x_2\overline{z_{n+2}}+\cdots$，因此 </p>
<script type="math/tex; mode=display">
|f(T_nx)|\le\sum_{k=1}^\infty |x_k|\cdot|z_{n+k}| \le \Vert x\Vert\left(\sum_{k=n+1}^\infty |z_k|^2\right)^{1/2} \to 0</script><p>所以有 $f(T_nx) \to 0$ 对任意 $f\in (\ell^2)’$ 成立，因此 $f(T_nx)\to f(T_0x)\equiv f(0)=0$。所以 $T_n$ 弱收敛到 $T_0=0$ 算子，但是总有 $\Vert T_nx\Vert=\Vert x\Vert\nrightarrow 0$，因此 $T_nx\nrightarrow T_0x$，即不满足强收敛。</p>
<p><strong>命题</strong>：对于一般有界线性算子，若 $T_n$ <strong>一致收敛</strong>到 $T$，则 $T$ 也是有界的，这是因为 $\Vert T\Vert\le \Vert T-T_n\Vert+\Vert T_n\Vert \le \infty$；若只能得到 $T_n$ <strong>强收敛</strong>到 $T$，那么 $T$ <strong>不一定是有界</strong>的。</p>
<p><strong><em>例子 6</em></strong>(强收敛极限未必有界)：$X=Y=\{(x_n),\exists N,\forall n\ge N, x_n=0 \}$，考虑 $T_n:X\to Y$ 有</p>
<script type="math/tex; mode=display">
\begin{aligned}
T_n:& (x_1,x_2,\cdots)\mapsto (x_1,2x_2,\cdots,nx_n,x_{n+1},x_{n+2},\cdots) \\
T:& (x_1,x_2,\cdots)\mapsto (x_1,2x_2,\cdots)
\end{aligned}</script><p>那么 $\Vert T_n\Vert=n$，取可以验证对于 $\forall x\in X$，$T_nx\to Tx$，即 $T_n$ 强收敛到 $T$，但是 $T$ 不是有界算子。</p>
<p>那么什么情况下可以保证强/弱收敛极限也是有界算子呢？</p>
<blockquote>
<p><strong>定理</strong>：设 $X$ 为 <strong>Banach 空间</strong>，$Y$ 为赋范空间，$T_n\in B(X,Y),T:X\to Y$ 为线性算子。设 $T_n$ <strong>弱收敛</strong>到 $T$，则 $\sup_{n\ge1}\Vert T_n\Vert &lt; \infty, T\in B(X,Y)$ 并且 $\Vert T\Vert \le \sup_{n\ge1}\Vert T_n\Vert &lt;\infty.$</p>
</blockquote>
<p><strong>证明</strong>：由于 $T_n$ 弱收敛到 $T$，即 $\forall x\in X,f\in Y’$ 都有 $f(T_nx)\to f(Tx)$，因此有 $T_nx \stackrel{w}{\longrightarrow} Tx$。那么根据序列弱收敛的性质，存在 $c_x$ 满足 $\sup_n \Vert T_nx\Vert \le c_x$，再由一致有界性原理，有 $\sup_n \Vert T_n\Vert &lt; \infty$。</p>
<p>然后考虑 $T$，$\forall x\in X$，由 Hahn-Banach 定理的推论，都存在 $f\in Y’,\Vert f\Vert=1$ 满足 </p>
<script type="math/tex; mode=display">
\Vert Tx\Vert=|f(Tx)| = \lim_{n\to\infty} |f(T_nx)| \le \lim_{n\to\infty}\Vert T_nx\Vert</script><p>因此 $\Vert T\Vert\le \sup_n\Vert T_n\Vert.$ 证毕。</p>
<blockquote>
<p><strong>定理</strong>：设 $X$ 为 <strong>Banach 空间</strong>，$Y$ 为赋范空间，$T_n,T\in B(X,Y)$，则 $T_n$ <strong>强收敛</strong>到 $T$ <strong>当且仅当</strong>：</p>
<ol>
<li>$\sup_n \Vert T_n\Vert &lt; \infty$；</li>
<li>存在 $M\subset X,\overline{\text{span}M}=X$，对 $\forall x\in M, T_n(x)\to T(x).$</li>
</ol>
<p><strong>NOTE</strong>：这跟线性泛函弱星收敛的等价条件是完全一样的，证明省略。</p>
</blockquote>
<h2 id="4-应用举例"><a href="#4-应用举例" class="headerlink" title="4. 应用举例"></a>4. 应用举例</h2><p><strong><em>例子 7</em></strong>(求积分的数值方法)：考虑实值函数 $x\in C[a,b]$，并赋予无穷范数，那么 $(C[a,b],\Vert\cdot\Vert)$ 为 Banach 空间，求 $\int_a^b x(t)dt.$</p>
<p>既然是在本节举的这个例子，那就要用到算子收敛性。先定义有界线性算子 $f(x)=\int_a^b x(t)dt$，$\Vert f\Vert=b-a$。我们现在的目标就是找一列有界线性泛函 $f_n$ 弱收敛到 $f$。回忆我们在学微积分的时候，往往是用分段的矩形面积求和来逼近积分。在 $[a,b]$ 上取 $n+1$ 个结点 $a=t_{n,0}&lt;t_{n,1}&lt;\cdots&lt;t_{n,n}=b$，再取 $n+1$ 个实数 $a_{n,0},\cdots,a_{n,n}$，令</p>
<script type="math/tex; mode=display">
f_n(x) = \sum_{k=0}^n a_{n,k} x(t_{n,k})</script><p>$f_n$ 是 $C[a,b]$ 上的线性泛函，并且 $\Vert f_n\Vert \le \sum_{k=0}^n|a_{n,k}|$，另外我们总能够造出一个 $x\in C[a,b]$ 满足 $x(t_{n,k})=\text{sgn}(a_{n,k})$ 并且 $\Vert x\Vert_\infty=1$，此时就有 $f(x)=\sum_{k=0}^n|a_{n,k}|$，于是可以得到 $\Vert f_n\Vert = \sum_{k=0}^n|a_{n,k}|$。现在的问题就是我们能否找到合适的系数 $a_{n,k}$ 使得 $f_n\stackrel{w}{\longrightarrow} f$ ？</p>
<p>这里我们提出一个额外的要求，就是对于次数小于 $n$ 的多项式 $p$，需要 $f_n(p)$ 能获得精确积分结果，即 $f_n(p)=\int_a^b p(t)dt$。由于 $\{1,t,\ldots,t^n\}$ 构成次数小于 $n$ 的多项式空间的 Hamel 基，所以只需要验证对每个基有 $f_n(e_k)= f(e_k)$ 即可。这就要求</p>
<script type="math/tex; mode=display">
\begin{cases}
\begin{matrix}
a_{n,0} & + & a_{n,1} & + & \cdots & + & a_{n,n} & = & b-a \\
a_{n,0}t_{n,0} & + & a_{n,1}t_{n,1} & + & \cdots & + & a_{n,n}t_{n,n} & = & \frac{b^2-a^2}{2} \\
& & & & \ldots & \\
a_{n,0}t_{n,0}^n & + & a_{n,1}t_{n,1}^n & + & \cdots & + & a_{n,n}t_{n,n}^n & = & \frac{b^{n+1}-a^{n+1}}{n+1}
\end{matrix}
\end{cases}</script><p>上式左侧可以用 Vandermonde 矩阵表示，因此存在唯一解 $a_{n,k},k=1,…,n$。</p>
<p>接下来对于任意的 $x\in C[a,b]$，能否找到 $a_{n,k}$ 满足的条件使得 $f_n(x)\to f(x)$ 呢？根据 Stone-Weierstrass 定理，多项式的集合在 $C[a,b]$ 中是<strong>稠密的</strong>，因此对于任意次数 $N$ 的多项式 $p$ 总有 $f_n(p)\to f(p)$。那么再应用前面的定理（即只需要判断完全集 $M\subset X$ 中的元素是否满足条件即可），可以有如下结论</p>
<p><strong>定理(G.Polya)</strong>：设数值积分 $f_n$ 满足前面对于有限次多项式的要求（即 $a_{n,k}$ 为 Vandermonde 矩阵方程的解）则任取 $x\in C[a,b],f_n(x)\to f(x)$ 当且仅当存在常数 $C\ge0$，使得任取 $n\ge1$，有 $\sum_{k=1}^na_{n,k}\le C.$</p>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>强收敛</tag>
        <tag>弱收敛</tag>
        <tag>弱星收敛</tag>
        <tag>一致收敛</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记6：一致有界性原理</title>
    <url>/2020/12/26/functional-analysis/ch4-3-banach-steinhauss/</url>
    <content><![CDATA[<p>Hahn-Banach定理主要是用于泛函的延拓，在较小的子空间上满足某个性质之后我们就可以将对应的泛函延拓至整个空间。而这一节要讲的一致有界性原理恰如其名，主要讨论一族有界线性算子一致有界的条件。他也是后续讨论序列弱收敛性以及泛函弱星收敛性的基础。</p>
<h2 id="1-Baire范畴定理"><a href="#1-Baire范畴定理" class="headerlink" title="1. Baire范畴定理"></a>1. Baire范畴定理</h2><p>一致有界性原理的证明需要用到Baire范畴定理（也叫Baire纲定理）。</p>
<p>$(X,d)$，若 $\bar{M}\subset X$ 没有内点，则称 $M$ 是<strong>无处稠密</strong>的。若 $N=\cup^\infty_n N_n$，$N_n$ 均为无处稠密的，则称 $N$ 为<strong>第一范畴</strong>。不为第一范畴的子集称为<strong>第二范畴</strong>。</p>
<a id="more"></a>
<p><em>例子 1</em>：$X=\mathbb{R},d(s,t)=|s-t|$，任意有限集均为无处稠密的，因此可数集均为第一范畴。</p>
<blockquote>
<p><strong>定理(范畴定理)</strong>：$(X,d)$ 为<strong>非空完备</strong>的，则 $X$ 必为<strong>第二范畴</strong>的。</p>
</blockquote>
<p><strong>证明</strong>：第二范畴意味着 $X$ 不能表示为可数个没有内点的集合的并集。假设 $X$ 属于第一范畴，即 $X=\cup_n M_n$，并且不妨设 $M_n$ 均为闭集（否则可以取 $X=\cup_n  \bar{M}_n$），并且 $M_n$ 都没有内点。</p>
<p>首先考虑 $Y_1=M_1^c$ 为开集，因此存在某个 $x_1\in Y_1,r_1\in(0,1/2)$ 使得 $B(x_1,r_1)\subset Y_1$。由于 $M_2$ 也没有内点并且为闭集，因此 $Y_2=B(x_1,r_1) \cap M_2^c \ne \varnothing$ 也为开集，因此可以找到某个 $x_2\in Y_2,r_2\in(0,r_1/2)$ 使得 $B(x_2,r_2)\subset Y_2$。依此类推，可以找到 $B(x_1,r_1) \supset \cdots \supset B(x_n,r_n)\supset \cdots$，并且有 $r_n\in (0,1/2^n)$。容易验证 $\{x_n\}$ 为柯西列，因此存在收敛值 $x_n\to x,x\in X$。对于 $k\ge1$ 考虑 $x_{n+k},x\in \bar{B}(x_n,r_n/2)\subset B(x_n,r_n)$，于是有 $x\in B(x_n,r_n),\forall n\ge1$，因此就有 $x\notin M_n,\forall n\ge1$，因此 $x\notin X$，导出矛盾。证毕。</p>
<h2 id="2-一致有界性原理"><a href="#2-一致有界性原理" class="headerlink" title="2. 一致有界性原理"></a>2. 一致有界性原理</h2><blockquote>
<p><strong>一致有界性原理</strong>：假设 $X$ 为 <strong>Banach 空间</strong>，$Y$ 为赋范空间，$T_i\in B(X,Y),\forall i\in \mathcal{I}$，并且对任取 $x\in X$ 有</p>
<script type="math/tex; mode=display">
\sup_{i\in\mathcal{I}} \Vert T_ix\Vert < \infty</script><p>则 $\sup_{i\in\mathcal{I}}\Vert T_i\Vert&lt;\infty.$</p>
<p><strong>NOTE</strong>：条件当中针对的是固定任意一个 $x\in X$，$T_i x$ 有界，也即是说所有的 $T_ix,i\in\mathcal{I}$ 存在一个上界 $c_x$，该上界与 $x$ 有关。对于线性泛函，我们只需要考虑 $\Vert x\Vert=1$ 的情况，但即便如此，一般而言由该条件并不能推导出对于所有的 $\Vert x\Vert, c_x$ 存在一个共同的上界，因为随着 $x$ 的变化 $c_x$ 有可能趋于无穷。而一致有界性原理则说明当 $X$ 为 Banach 空间的时候，一定存在这样一个上界，从而说明 $\Vert T_i\Vert$ 有上确界。</p>
</blockquote>
<p><strong>证明</strong>：根据上面的分析，我们在寻找 $\sup_i\Vert T_i\Vert$ 的时候不能局限在 $\Vert x\Vert=1$ 的情况，下面的证明方法很巧妙。</p>
<p>首先考虑 $X$ 完备，根据 Baire 范畴定理，不能表示为可数个没有内点的集合的并集。那么假如我们将其表示为可数个集合的并集，则一定存在某个集合有内点。因此考虑 $M_n=\{x\in X, \sup_i\Vert T_ix\Vert \le n\}$，因此就有 $X=\cup_n^\infty M_n$，一定存在某个 $N\ge1$ 使得 $M_N$ 有内点，此时找到 $x_0\in M_N,r&gt;0$ 使得 $\bar{B}(x_0,r)\subset M_N$，并且 $\forall y\in X,\Vert y\Vert=1$，都有</p>
<script type="math/tex; mode=display">
\Vert T_i(x_0)\Vert \le N,\ \Vert T_i(x_0+ry)\Vert \le N,\quad \forall i\in\mathcal{I},\Vert y\Vert=1 \\
\Longrightarrow \Vert T_iy\Vert \le 2N/r, \quad \forall i\in\mathcal{I},\Vert y\Vert=1 \\
\Longrightarrow \sup_{i\in\mathcal{I}}\Vert T_i\Vert \le 2N/r</script><p>证毕。</p>
<blockquote>
<p><strong>共鸣定理</strong>：假设 $X$ 为 <strong>Banach 空间</strong>，$Y$ 为赋范空间，$T_i\in B(X,Y),\forall i\in \mathcal{I}$，设 $\sup_i\Vert T_i\Vert = \infty$，则 $\exists x\in X$ 使得 $\sup_{i} \Vert T_i x\Vert= \infty$，其中 $x$ 即为 $T_i$ 的共鸣点。</p>
<p><strong>NOTE</strong>：实际上共鸣定理就是一致有界性原理的逆反命题。</p>
</blockquote>
<h2 id="3-应用举例"><a href="#3-应用举例" class="headerlink" title="3. 应用举例"></a>3. 应用举例</h2><p>一致有界性原理的 Banach 空间假设是必不可少的，下面的例子将进行解释。</p>
<p><strong><em>例子 2</em></strong>：$X$ 为所有的多项式，即 $p\in X$ 可以表示为 $p(t)=a_0+a_1 t+\cdots a_N t^N$，定义 $\Vert p\Vert =\max_{0\le i\le N}|a_i|$。取一列线性泛函 $f_n(p)=a_0+a_1+\cdots a_n$，那么可以验证 $f_n\in X’$ 并且有 $\Vert f_n\Vert=n+1$。</p>
<p>此时显然我们有对任意固定的 $p=a_0+a_1 t+\cdots a_N t^N\in X$，$\Vert f_n(p)\Vert \le |a_0|+\cdots+|a_N|$，但是同时我们也有 $\sup_n \Vert f_n\Vert=\infty$，这似乎与一致有界性原理矛盾？其实原因是 $X$ 并不是 Banach 空间。</p>
<p>考虑 $p_n(t)=1+\frac{1}{2}t+\cdots+\frac{1}{n+1}t^n$，那么可以验证 $\{p_n\}$ 为柯西列，但是其收敛值却并不在 $X$ 内部（$X$ 中只包含有限项的多项式），因此 $X$ 不完备。</p>
<p>一致有界性原理还可用于讨论 Fourier 级数的收敛问题。</p>
<p><strong><em>例子 3</em></strong>：考虑 $t\in[0,2\pi]$，$f(t)=a_0+\sum_{n=1}^N\left(a_n \cos nt+b_n\sin nt\right).$</p>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>Baire范畴定理</tag>
        <tag>无处稠密</tag>
        <tag>第一范畴、第二范畴</tag>
        <tag>一致有界性原理</tag>
        <tag>共鸣定理</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记5：Hahn-Banach定理的应用</title>
    <url>/2020/12/26/functional-analysis/ch4-2-application/</url>
    <content><![CDATA[<h2 id="1-共轭算子"><a href="#1-共轭算子" class="headerlink" title="1. 共轭算子"></a>1. 共轭算子</h2><p>赋范空间 $X,Y$，$T\in B(X,Y)$，对于任意的 $f\in Y’$，$X \stackrel{T}{\longrightarrow}Y\stackrel{f}{\longrightarrow}\mathbb{K}$，可以得到 $f\circ T\in X’$。因此我们可以定义映射</p>
<script type="math/tex; mode=display">
\begin{aligned}
T^{\times}:Y' &\to X' \\
f &\mapsto f\circ T
\end{aligned}</script><p>称其为<strong>共轭算子</strong>。他有如下性质（容易验证，不再证明）：</p>
<a id="more"></a>
<ul>
<li>$T^{\times}\in B(Y’,X’)$</li>
<li>$\Vert T^{\times}\Vert=\Vert T\Vert$（证明过程用到了 Hahn-Banach定理）</li>
<li>$(S+T)^{\times}=S^\times + T^\times,\ S,T\in B(X,Y)$</li>
<li>$(\lambda T)^\times=\lambda T^\times$</li>
<li>$(AB)^\times=B^\times A^\times,\ A\in B(X,Y),B\in B(Y,Z)$</li>
</ul>
<p><em>例子 1</em>：设 $X=Y=\mathbb{C}^n,T\in B(\mathbb{C}^n,\mathbb{C}^n)$，则 $\exists!A$ 为 $n$ 阶方阵使得 $Tx=Ax$，而 $T^\times\in B((\mathbb{C}^n)’,(\mathbb{C}^n)’)$。实际上 $f\in(\mathbb{C}^n)’$ 可以表示为 $f(x)=\sum_i^n \alpha_i x_i=\alpha^T x$，$T^\times f(x)=\sum_i^n\beta_i x_i=\beta^T x$。容易验证 $\beta=A^T\alpha.$</p>
<p>在这里，我们可以联想到<strong>伴随算子</strong>，$T\in B(H_1,H_2)$，其伴随算子 $T^\star\in B(H_2,H_1)$ 满足 $\langle Tx,y\rangle=\langle x,T^\star y\rangle.$ 而这里的共轭算子则是 $T^\times\in B(H_2’, H_1’).$ </p>
<p>回忆第二章我们讲等距同构概念的时候，提到了 $(\mathbb{K}^n,\Vert\cdot\Vert_2)’ = (\mathbb{K}^n,\Vert\cdot\Vert_2)$，也就是说实际上我们可以找到某个映射 $A:H’\to H$。如果能找到这样的一个双射，就可以认为 $T^\star$ 与 $T^\times$ 是等价的。</p>
<p>一般的空间未必有如此良好的性质，但对于 <strong>Hilbert 空间</strong>来说，任意 $f\in H’$ 都可以唯一地表示为 $f(x)=\langle x,z\rangle, z\in H$，那么我们就可以定义映射</p>
<script type="math/tex; mode=display">
\begin{aligned}
A:H' &\to H \\
f &\mapsto z
\end{aligned}</script><p>容易证明 $A$ 时<strong>共轭线性</strong>的（并且是<strong>双射</strong>），即 $A(\lambda f+\mu g)=\bar{\lambda}Af+\bar{\mu}Ag$。</p>
<p>此时我们可以得到如下图所示的映射关系，可以看到实际上 $T^\times = A_1^{-1}T^\star A_2$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/ch4-1-conjugate-operator.png" alt="conjugate operator"></p>
<p><em>例子 2</em>：考虑 Hilbert 空间 $H_1,H_2$，$g\in H_2’$ 可以表示为 $g(y)=\langle y,y_0\rangle$，因此实际上有 $A_2g=y_0\in H_2$，令 $f=T^\times g\in H_1’$，因此可以表示为 $f(x)=\langle x,x_0\rangle$，这可以表示为 $A_1f=x_0\in H_1$，因此有 $A_1T^\times g=x_0$，结合 $A_1T^\times = T^\star A_2$ 就有 $x_0=T^\star y_0.$</p>
<h2 id="2-自反空间"><a href="#2-自反空间" class="headerlink" title="2. 自反空间"></a>2. 自反空间</h2><p>前面我们研究了 $X$ 与 $X’$ 的关系，当 $X$ 为 Hilbert 空间时二者等距同构。这一小节我们想再研究研究 $X$ 与 $X’’$ 的关系。为什么要研究他们的关系呢？因为不论原始空间 $X$ 怎么样，<strong>对偶空间 $X’$ 总是 Banach 空间</strong>。友情提示，接下来这部分会比较绕。</p>
<h3 id="2-1-典范映射"><a href="#2-1-典范映射" class="headerlink" title="2.1 典范映射"></a>2.1 典范映射</h3><p>如果想要研究 $X$ 与 $X’’$ 的关系，考虑映射 $J:X\to X’’$，那么就需要考虑 $J$ 是否是等距同构的，或者是否是单射、满射、双射？如何定义 $J$ 呢？考虑 $J(x)\in X’’$，记 $g_x=J(x): X’\to\mathbb{K}$，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
J:X&\to X'' \\
x&\mapsto g_x
\end{aligned}</script><p>那么对于任意 $f\in X’$，定义 $g_x(f)=f(x)\in\mathbb{K}$，因此 $(J(x))(f)=f(x)$。其中 $x$ 为 $J$ 的自变量，$J(x)$ 是一个泛函，$f$ 为 $J(x)$ 的自变量。</p>
<p>首先来看按照上面的方法给出的 $J$ 的定义是否满足 $J:X\to X’’.$ 首先来看 $J(x)=g_x$ 是否为 $X’$ 上的线性泛函？$g_x(\lambda f+\mu h)=\lambda f(x)+\mu h(x)=\lambda g_x(f)+\mu g_x(h)$，因此 $g_x\in (X’)^\star$，接下来还需要验证 $\Vert g_x\Vert$ 是否是有界的。</p>
<script type="math/tex; mode=display">
\Vert g_x\Vert = \sup_{f\in X'}\frac{\Vert g_x(f)\Vert}{\Vert f\Vert} = \sup_{f\in X'}\frac{|f(x)|}{\Vert f\Vert} = \Vert x\Vert</script><p>因此有 $J(x)=g_x\in X’’,\forall x\in X$，说明我们定义的 $J$ 确实是 $X\to X’’$ 的映射，我们称之为<strong>典范映射</strong>。</p>
<p>那么这个映射有什么性质呢？$J$ 是否为线性映射？$g_{\lambda x+\mu y}=J(\lambda x+\mu y)\in X’’$，对于 $\forall f\in X’$，都有 </p>
<script type="math/tex; mode=display">
g_{\lambda x+\mu y}(f)=f(\lambda x+\mu y)=\lambda f(x)+\mu g(y)=\lambda g_x(f)+\mu g_y(f) \\
\Longrightarrow g_{\lambda x+\mu y} = \lambda g_x+\mu g_y\\
\Longrightarrow J(\lambda x+\mu y) = \lambda J(x) + \mu J(y)</script><p>这说明 $J$ 是线性映射，那么 $\Vert J\Vert$ 是多少？是否是有界线性映射？</p>
<script type="math/tex; mode=display">
\Vert J\Vert = \sup_{x\in X} \frac{\Vert J(x)\Vert}{\Vert x\Vert},\quad \Vert J(x)\Vert=\Vert x\Vert</script><p>因此 $\Vert J\Vert = 1.$</p>
<p>由于任意 $x\in X$ 都可以得到 $J(x)\in X’’$，因此 $X$ 可以视为 $X’’$ 的“赋范子空间”，也就是说 $X$ 的势小于 $X’’$。那么是否有 $X’’$ 的势就等于 $X$ 的势呢？如果二者势相等，由于 $J$ 是保范映射，就说明 $X$ 与 $X’’$ 是等距同构的！但是遗憾的是并不是任意赋范空间 $X$ 都有这个结论，只有某些条件下，比如 Hilbert 空间有这个性质。</p>
<p>若 $J$ 为满射，则称 $X$ 为<strong>自反空间</strong>（意味着 $X$ 与 $X’’$ <strong>等距同构</strong>），这有如下两种等价表示：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\iff& \forall F\in X'',\quad \exists! x\in X,\quad F=J(x) \\
\iff& \forall F\in X'',\quad \exists! x\in X,\quad F(f)=f(x),\forall f\in X' 
\end{aligned}</script><p>如果 $X$ 为自反的，由于 $X’’$ 为 Banach 空间，那么 $X$ 也是 Banach 空间。</p>
<p><strong>命题</strong>：Hilbert 空间均为自反的。</p>
<p><strong>证明</strong>：这个证明的思路非常巧妙！直接按照上面的定义来证明很难证出来。为此我们首先考虑 $H’$，我们已经知道他是 Banach 空间了，但它实际上是一个 Hilbert 空间，怎么证明呢？</p>
<p>对 $\forall f,g\in H’$，我们前面提到 $f(x)=\langle x,Af\rangle, Af\in H$，因此定义</p>
<script type="math/tex; mode=display">
\langle f,g\rangle_1 = \langle Ag, Af\rangle</script><p>验证内积的定义可以证明 <strong>$\langle \cdot,\cdot\rangle_1$ 就是 $H’$ 上的内积</strong>，因此 <strong>$H’$ 是 Hilbert 空间</strong>。这样的话就太好了，因为 $\forall F\in H’’$，都存在唯一的 $f_0\in H’$，使得 $F(f)=\langle f,f_0\rangle_1=\langle Af_0, Af\rangle=f(Af_0)$！因此 $H$ 为自反空间。证毕。</p>
<p><em>例子 1</em>：若 $1&lt;p&lt;\infty$，$\ell^p$ 是自反的。</p>
<p><strong>证明</strong>：我们首先知道 $(\ell^p)’$ 与 $\ell^q$ 是等距同构的，而 $(\ell^p)’’=(\ell^q)’=\ell^p$，因此 $\ell^p$ 是自反的（$1/p+1/q=1$）。</p>
<p>不过这个证明不太严谨，也可以利用自反空间的等价定义（跟上面的表述是等价的）。任意的 $f\in (\ell^p)’$，可以表示为 $f(x)=\langle x,y\rangle,x\in\ell^p,y\in\ell^q$，对任意的 $F\in(\ell^p)’’$，现在问题来了，$F(f)$ 是个什么东西？我们怎么定义 $F(f)$？$f$ 是一个函数，如何将其映射到 $\mathbb{K}$ 上去呢？这是时候还是要应用等距同构的性质，$(\ell^p)’$ 与 $\ell^q$ 等距同构，因此我们可以用 $y\in\ell^q$ 来等价的代替 $f\in(\ell^p)’$，这样的话 $F(f)$ 就很容易定义了，$F(f)\triangleq F_1(y)\in\mathbb{K}$，那么我们就能找到唯一的 $x_0\in\ell^p$，使得 $F_1(y)=\langle y,x_0\rangle=\langle x_0,y\rangle=f(x_0)$，这样的话任意 $F\in (\ell^p)’’$，我们都能找到对应的唯一的 $x_0\in \ell^p$，使得 $F(f)=f(x_0)$。证毕。</p>
<p><strong><em>例子 2</em></strong>：有限维赋范空间自反。</p>
<p>证明：假设 $\text{dim}X=n&lt;\infty$，那么应用 Hamel 基的性质（参考教材例 2.5.3）$\text{dim}X^\star=n$。再应用下面的引理（可参考第2章），可以知道有限维赋范空间的 $X’=X^\star$ 等距同构，因此 $\text{dim}X’=n$，因此也有 $\text{dim}X’’=n$，因此 $J:X\to X’’$ 为满射，$X$ 为自反的。证毕。</p>
<blockquote>
<p><strong>引理</strong>：$X,Y$ 为 $\mathbb{K}$ 上的赋范空间，假设 $\text{dim}X=n&lt;\infty$，$T:X\to Y$ 是线性算子，那么 $T$ 一定是有界的。</p>
</blockquote>
<h3 id="2-2-可分性"><a href="#2-2-可分性" class="headerlink" title="2.2 可分性"></a>2.2 可分性</h3><p>研究集合的可分性，可以通过验证典范映射 $J$ 是否为双射，也有另一个思路，就是下面要讲的可分性。</p>
<blockquote>
<p><strong>定理(Hahn-Banach) 5</strong>：$(X,\Vert\cdot\Vert)$，$Y$ 为 $X$ 的线性子空间，$Y\subsetneq X$，对 $\forall x_0\in Y^c$，令</p>
<script type="math/tex; mode=display">
\delta = \rho(x_0,Y)=\inf_{y\in Y}\Vert x_0-y\Vert</script><p>则存在 $f\in X’$，使得 $\Vert f\Vert=1, f|_Y=0,f(x_0)=\delta.$</p>
<p><strong>NOTE</strong>：这个定理在说什么事呢？前面在讲内积空间的时候，我们提到了 Hilbert 空间上的有界线性泛函实际上可以表示为 $f(x)=\langle x,z_0\rangle$，这实际上可以看成是以 $z_0$ 为法向量的超平面，因此 $N(f)^{\perp}$ 是一维的，也就是说 $f$ 在 $z_0$ 方向上是非零的，在正交于 $z_0$ 的平面内都是 0。而这个定理当中，寻找这个 $f$ 要做的就是找到一个合适的法向量，使得 $z_0\perp Y$，并且添加一个线性系数使得刚好有 $f(x_0)=\delta$。不过这个定理更加强大的一个地方在于不要求 $X$ 是 Hilbert 空间，只要求赋范空间即可。</p>
</blockquote>
<p><strong>证明</strong>：考虑 $M=\text{span}(Y\cup \{x_0\})$ 是 $X$ 的线性子空间，则 $\forall x\in M$，存在唯一的分解方式 $x=y+\lambda x_0$，$y\in Y,\lambda\in\mathbb{K}$，定义 $f_0(x)=\lambda \delta.$ 容易验证 $f_0\in M’, \Vert f_0\Vert\le1$，也可以验证 $\Vert f_0\Vert \ge1$（需要思考一下）。因此存在 $f\in X’$ 使得 $\Vert f\Vert=\Vert f_0\Vert$，$f|_Y=f_0|_Y=0$，并且 $f(x_0)=f_0(x_0)=\delta.$ 证毕。</p>
<blockquote>
<p><strong>推论</strong>：$(X,\Vert\cdot\Vert)$，若 $X’$ 为可分空间，则 $X$ 为可分空间。</p>
<p><strong>NOTE</strong>：这个推论可用于证明某个空间不是自反空间：如果 $X$ 可分，但是 $X’$ 不可分，那么 $X$ 一定不自反。否则的话 $X’’=X$ 是可分的，应该有 $X’$ 也是可分的，矛盾。</p>
</blockquote>
<p>证明： 参考教材，略。</p>
<p><em>例子 3</em>：$c_o,\ \ell^1,\ \ell^\infty,\ C[a,b]$ 都不是自反空间。</p>
<p><strong>证明</strong>：$c_0’=\ell^1,(\ell^1)’=\ell^\infty, (\ell^\infty)’=\ell^1$（参考课本 P68），但是由于 <strong>$c_0,\ell^1$ 是可分的，而 $\ell^\infty$ 不是可分的</strong>，因此 $\ell^1$ 不自反。</p>
<p>$C[a,b]’$ 是不可分的，$C[a,b]$ 是可分的， 细节参考课本，懒得写了……后面心情好了再补吧……</p>
<h2 id="3-Riemann-Stieltjes积分"><a href="#3-Riemann-Stieltjes积分" class="headerlink" title="3. Riemann-Stieltjes积分"></a>3. Riemann-Stieltjes积分</h2>]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>共轭算子</tag>
        <tag>典范映射</tag>
        <tag>自反空间</tag>
        <tag>Riemann-Stieltjes积分</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记4：Hahn-Banach定理</title>
    <url>/2020/12/14/functional-analysis/ch4-1-hahn-banach/</url>
    <content><![CDATA[<p>前面三章讲了很多东西，但实际上都只是开胃小菜 &gt;__&lt;，度量空间、赋范空间、内积空间等等都是为了我们接下来要讲的四大定理做铺垫。泛函中的四大定理，即 Hahn-Banach 定理、一致有界性原理、开映射定理和闭图像定理，是整个泛函分析的基石（前面三章的内容是基石的基石）。</p>
<a id="more"></a>
<h2 id="1-Hahn-Banach-定理"><a href="#1-Hahn-Banach-定理" class="headerlink" title="1. Hahn-Banach 定理"></a>1. Hahn-Banach 定理</h2><p>首先介绍几个要用到的概念。对于定义了序关系 $\preceq$ 的集合 $X$，$N\subset X$，称 $x_0\in X$ 为 $N$ 的<strong>上界</strong>，若 $x\preceq x_0,\forall x\in N$；称 $y_0\in N$ 为<strong>极大元</strong>，若 $y_0\le y,\forall y\in N \Longrightarrow y=y_0.$</p>
<p><strong>Zorn引理</strong>：非空半序集合 $(X,\preceq)$，若 $X$ 的任意非空全序子集均有上界，则 $X$ 必有极大元。</p>
<p>Hahn-Banach 定理有很多种不同的形式，在给出第一个 Hahn-Banach 定理之前，我们先引入次线性泛函。对于线性空间 $X$，称 $p:X\to \mathbb{K}$ 为<strong>次线性泛函</strong>，若：</p>
<ol>
<li>$\forall x,y\in X, p(x+y)\le p(x)+p(y)$</li>
<li>$\forall x\in X, a\ge0,p(ax)=ap(x).$</li>
</ol>
<blockquote>
<p><strong>定理(Hahn-Banach) 1</strong>：对于<strong>实线性空间</strong> $X$，$X_0$ 为 $X$ 的线性子空间，$p$ 为 $X$ 上的<strong>次线性泛函</strong>，$f_0\in X_0^\star$ 为线性泛函并且满足 $f_0(x)\le p(x),\forall x\in X_0$，那么存在 $f\in X^\star$ 使得 $f|_{X_0}=f_0$，并且 $f(x)\le p(x),\forall x\in X.$</p>
</blockquote>
<p><strong>证明</strong>：要证明 $f$ 的存在性我们就需要找到满足条件的 $f$，但是给出的 $f_0$ 只在 $X_0$ 上有定义，怎么把它扩展到整个 $X$ 上得到我们想要的 $f$ 呢？下面的构造方法非常的“山路十八弯”，我自己是想不到 orz。</p>
<p>首先定义 $\mathcal{E}=\{(Y,g): Y为X的线性子空间,X_0\subset Y,\quad g\in Y^\star ,g|_{X_0}=f_0, g(x)\le p(x),\forall x\in Y\}$ （这个集合的构造就不是一般人能想到的[狗头]）。由于 $(X_0,f_0)\in \mathcal{E}$，因此 $\mathcal{E}$ 非空，我们可以定义 $\mathcal{E}$ 上的序关系 </p>
<script type="math/tex; mode=display">
(Y_1,g_1)\preceq(Y_2,g_2) \iff Y_1\subset Y_2,\ g_2|_{Y_1}=g_1</script><p>那么取 $\mathcal{E}$ 的非空全序子集 $\mathcal{E}_1$，令 $W=\cup_{(Y,g)\in \mathcal{E}_1}Y$，对应的 $\forall x\in W$，我们可以找到 $(Y,g)\in \mathcal{E}_1, x\in Y$，此时定义 $h(x)=g(x)$。容易验证 $W$ 为 $X$ 的线性子空间，另外由于 $\mathcal{E}_1$ 是全序的，也可以验证 $h(x),\forall x\in W$ 的定义是唯一的（与 $(Y,g)$ 的选取无关），并且 $h(x)$ 为 $W$ 上的线性泛函。因此我们有 $(W,h)\in\mathcal{E}.$</p>
<p>由于任意 $(Y,g)\in\mathcal{E}_1$，都有 $(Y,g)\preceq(W,h)$，即 $(W,h)$ 为 $\mathcal{E}_1$ 的上界，$\mathcal{E}$ 中任意非空全序子集都有上界，根据 Zorn 引理，$\mathcal{E}$ 有极大元 $(Y_0,g_0).$</p>
<p>到这里我们就把 $(X_0,f_0)$ 扩展到了一个更大的空间 $(Y_0,g_0)$ 上面，但是 $Y_0$ 是否就是 $X$ 呢？感觉应该是，但是需要证明一下。</p>
<p>假如 $Y_0\subsetneq X$，可以取 $y\in Y_0^c$，考虑 $V=\text{span}(Y_0\cup \{y\})$，那么 $\forall x\in V$ 都可以唯一的表示为 $x=w+\lambda y,w\in Y_0,\lambda\in\mathbb{R}$，此时定义</p>
<script type="math/tex; mode=display">
h(x)=g_0(w)+\lambda a</script><p>容易验证 $h$ 为 $V$ 上的线性泛函，并且有 $h_{Y_0}=g_0$，因此我们就对 $g_0$ 又进行了扩展，只需要证明 $(V,h)\in\mathcal{E}$ 就能导出矛盾（因为已经证明 $(Y_0,g_0)$ 为极大元）从而说明 $Y_0=X$。这就需要满足 $h(x)\le p(x),\forall x\in V$。考虑 $\lambda&gt;0$，这等价于 $h(x/\lambda)=g_0(w/\lambda)+a\le p(w/\lambda+y)$，也等价于 $\forall w\in Y_0, g_0(w)+a\le p(w+y)$。类似得取 $x=w-\lambda y,\lambda&gt;0$ 就可以得到 $g_0(w)-a\le p(w-y)$。因此 $a$ 需要满足以下条件：</p>
<script type="math/tex; mode=display">
g_0(w)-p(w-y) \le a \le p(w+y) - g_0(w)</script><p>因此就需要满足 $2g_0(w) \le p(w-y)+p(w+y)$，由于 $2g_0(w)\le p(2w) \le p(w+y+w-y)\le p(w+y)+p(w-y)$，因此上式一定可以满足，即能找到合适的 $a$ 使得 $(V,h)\in\mathcal{E}$，因为 $Y_0\subsetneq V$，这就说明 $(Y_0,g_0)$ 不是 $\mathcal{E}$ 的极大元，导出矛盾，因此有 $Y_0=X$。至此我们就找到了 $g_0$ 满足定理要求的全部条件。</p>
<p>证毕。</p>
<p>上面的定理只研究了实线性空间的情况，对于复线性空间有相似的结论，不过我们需要对条件稍加修改。</p>
<p>首先将上面 $p$ 的次线性泛函假设改为更强的假设——半范。对于线性空间 $X$，$p:X\to\mathbb{R}$ 称为 $X$ 上的<strong>半范</strong>，若满足：</p>
<ol>
<li>$p(x)\ge0, \forall x\in X$</li>
<li>$p(x+y)\le p(x)+p(y),\forall x,y\in X$</li>
<li>$p(ax)=|a|p(x),\forall x\in X,a\in\mathbb{K}$</li>
</ol>
<p>半范与范数的概念相比，少了一个条件即 $p(x)=0\iff x=0$，因此说明可能存在 $x\ne0,p(x)=0$。另外半范可以导出次线性。</p>
<blockquote>
<p><strong>定理(Hahn-Banach) 2</strong>：$X$ 为 $\mathbb{K}$ 上的线性空间，$X_0$ 为 $X$ 的线性子空间，$p:X\to \mathbb{R}$ 为半范，$f_0\in X_0^\star$ 并且有 $|f_0(x)|\le p(x),\forall x\in X_0$，那么存在 $f\in X^\star$ 使得 $f|_{X_0}=f_0$，并且 $f(x)\le p(x),\forall x\in X.$</p>
</blockquote>
<p><strong>证明</strong>： 要想把原来的结论从实线性空间扩展到复线性空间，首先要考虑复线性空间有什么特殊的性质。如果用两个实线性泛函来表示 $f(x)=f_1(x)+if_2(x)$，我们可以想一下复线性空间可以粗略的看成两个实线性空间的组合，$\forall a\in\mathbb{R},f(ax)=af_1(x)+iaf_2(x)$。但是实际上并没有这么简单，因为 $a$ 还可以取自复空间，那么我们令 $a=i$ 就可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
if(x)&=f(ix) \\
if_1(x)-f_2(x)&=f_1(ix)+if_2(x)
\end{aligned}</script><p>由于 $x\in X$ 任取，实部虚部需要分别相等，因此 $f_2(x)=-f_1(ix),\forall x\in X$，即<strong>复线性泛函 $f$ 的实部和虚部是相互约束的</strong>！另一方面，实部和虚部只需要知道其中一个，另一个也就确定了。那么也就是说给定一个复线性泛函 $f$ 我们可以唯一的用某个实线性泛函 $f_1$ 来表示 $f$。</p>
<blockquote>
<p><strong>NOTE</strong>：读者如果了解信号的相干解调的话，这里可以联想到 Hilbert 变换。实际上。</p>
</blockquote>
<p>对于定理中的 $f_0$ 我们可以将其表示为 $f_0(x)=f_1(x)+if_2(x)$。容易验证 $f_1(x)\le p(x)$，根据上一个 Hahn-Banach 定理，就能找到一个实线性泛函 $g_1$，使得 $g_1|_{X_0}=f_1, g_1(x)\le p(x)$。那么我们再定义 $g(x)=g_1(x)-ig_1(ix)$，容易验证 $g\in X^\star,g|_{X_0}=f_0$，接下来验证 $|g(x)|\le p(x)$ 的方法也有一点巧妙：</p>
<script type="math/tex; mode=display">
|g(x)|=g(x)e^{-i\theta} = g(e^{-i\theta}x)=g_1(e^{-i\theta}x)\le p(e^{-i\theta}x)=p(x)</script><p>证毕。</p>
<p>接下来再把 $X$ 限制在赋范空间上，就能得到有界线性泛函的保范延拓定理，这也是 Hahn-Banach 定理最重要的应用。我们这里讲前面两个定理基本上也是为下面一个定理做铺垫。他们的基本含义都是将一个线性泛函从较小的线性子空间延拓到整个空间。</p>
<blockquote>
<p><strong>定理(Hahn-Banach/保范延拓定理) 3</strong>：$(X,\Vert\cdot\Vert)$，$X_0$ 为 $X$ 的线性子空间，$\forall f_0\in X_0’$，则 $\exists f\in X’$，$f|_{X_0}=f_0$ 且 $\Vert f\Vert = \Vert f_0\Vert.$</p>
</blockquote>
<p><strong>证明</strong>：我们还是要对 $f_0$ 进行延拓，那么怎么样才能应用上面两个定理的结论呢？首先需要构造一个半范 $p$，可以定义 $p(x)=\Vert f_0\Vert\cdot \Vert x\Vert$，容易验证其为半范（在这里能看出来半范的好处，由于不要求 $p(x+y)=p(x)+p(y)$，因此 $p$ 可以含有 $\Vert x\Vert$ 项）。那么就能找到 $f\in X^\star,f|_{X_0}=f_0,|f(x)|\le \Vert f_0\Vert\cdot \Vert x\Vert$，之后容易验证 $\Vert f\Vert=\Vert f_0\Vert.$ 证毕。</p>
<p>对上一定义的假设条件进行增强，把 $X$ 限制为 Hilbert 空间，回忆上一章提到了 <strong>Hilbert 空间</strong>上的<strong>线性泛函</strong>都可以<strong>唯一的</strong>表示为 $f(x)=\langle x,z_0\rangle$ 的形式，那么我们可以把上面的保范延拓定理进一步加强，获得唯一性！</p>
<blockquote>
<p><strong>推论 1</strong>：若 $X$ 为 HIlbert 空间，$X_0$ 为 $X$ 的闭线性子空间，$f_0\in X_0’$，则<strong>存在唯一的</strong> $f\in X’$，$f|_{X_0}=f_0$ 且 $\Vert f\Vert = \Vert f_0\Vert.$</p>
</blockquote>
<p><strong>证明</strong>：$X_0$ 也是 Hilbert 空间，因此 $\exists! z_0\in X_0$，使得 $f_0(x)=\langle x,z_0\rangle$，因此很方便地可以取 $f_0(x)=\langle x,z_0\rangle, \forall x\in X$，并且可以验证 $f$ 满足条件 $f|_{X_0}=f_0, \Vert f\Vert = \Vert f_0\Vert=\Vert z_0\Vert.$ </p>
<p>接下来就需要证明 $f$ 的唯一性。假设还有 $g$ 满足条件，那么 $\exists! z_1\in X$，$g(x)=\langle x,z_1\rangle$</p>
<script type="math/tex; mode=display">
\forall x\in X_0.f(x)-g(x)=\langle x,z_0-z_1\rangle=0 \Longrightarrow z_0-z_1\in X_0^{\perp} \\
\Vert g_0\Vert=\Vert z_1\Vert^2 = \Vert z_1-z_0\Vert^2+\Vert z_0\Vert^2=\Vert f_0\Vert^2 \Longrightarrow z_1=z_0</script><p>证毕。</p>
<blockquote>
<p><strong>定理(Hahn-Banach) 4</strong>：$(X,\Vert\cdot\Vert)$，$x_0\in X, x_0\ne 0$，则 $\exists f\in X’, \Vert f\Vert=1$，使得 $f(x_0)=\Vert x_0\Vert.$</p>
</blockquote>
<p><strong>证明</strong>：可以取 $X_0=\mathbb{K}x_0=\{\lambda x_0,\lambda\in\mathbb{K}\}$，定义 $f_0(\lambda x_0)=\lambda \Vert x_0\Vert \in X_0’$，$\Vert f_0\Vert=1$，再应用上一推论自然就出来了。证毕。</p>
<p><strong>NOTE</strong>：对于 $\forall x,y\in X,x\ne y$，那么我们取 $x_0=x-y\ne0$，根据上面的定理，一定存在 $f\in X’$ 使得 $f(x_0)=f(x)-f(y)\ne0$。这说明只要 $x\ne y$，就一定存在某个 $f\in X’,\Vert f\Vert=1$ 使得 $f(x)\ne f(y).$ 可以表述为</p>
<script type="math/tex; mode=display">
x=y \iff f(x)=f(y),\forall f\in X'</script><blockquote>
<p><strong>推论 2</strong>：$X\ne\{0\}$ 为赋范空间，$x_0\in X,x_0\ne 0$，则</p>
<script type="math/tex; mode=display">
\Vert x_0\Vert = \max_{f\in X',\Vert f\Vert=1} |f(x_0)| = \max_{f\in X',\Vert f\Vert\le1} |f(x_0)| = \max_{f\ne0} \frac{|f(x_0)|}{\Vert f\Vert}</script><p><strong>NOTE</strong>：回忆 $\Vert f\Vert=\sup_{x\in X,\Vert x\Vert\le1}|f(x)|$，从这个结论来看，$X$ 与 $X’$ 有某种的对称性。证明略。</p>
</blockquote>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>Hilbert空间</tag>
        <tag>Hahn-Banach定理</tag>
        <tag>次线性泛函</tag>
        <tag>半范</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记3：内积空间</title>
    <url>/2020/12/11/functional-analysis/ch3-hilbert-space/</url>
    <content><![CDATA[<p>我们前面讲了距离空间、赋范空间，距离空间赋予了两个点之间的距离度量，范数赋予了每个点自身的长度度量，而范数则可以导出距离。本章要讲的内积可以看成是更加统一的定义，因为从内积我们可以导出范数，进而导出距离。因此内积空间是一个“更小”的空间，在此基础上结合完备性我们引出了 Hilbert 空间，后续我们的研究也大多集中在 Hilbert 空间上。</p>
<h2 id="1-内积空间"><a href="#1-内积空间" class="headerlink" title="1. 内积空间"></a>1. 内积空间</h2><p><strong>定义</strong>：$X$ 为 $\mathbb{K}$ 上的线性空间，$\langle \cdot,\cdot\rangle :X\times X\to \mathbb{K}$，若满足如下条件</p>
<ol>
<li>$\langle \lambda x+\mu y, z\rangle  = \lambda\langle x,z\rangle +\mu\langle y,z\rangle $</li>
<li>$\overline{\langle x,y\rangle }=\langle y,z\rangle $</li>
<li>$\forall x\in X, \langle x,x\rangle  \ge 0$</li>
<li>$\langle x,x\rangle =0 \iff x=0$</li>
</ol>
<p>则称 $(X,\langle \cdot,\cdot\rangle )$ 为<strong>内积空间</strong>。可以用内积定义范数 $\Vert x\Vert = \langle x,x\rangle ^{1/2}$。若得到的 $(X,\Vert\cdot\Vert)$ 为 Banach 空间，那么 $(X,\langle \cdot,\cdot\rangle )$ 为 <strong>Hilbert 空间</strong>。</p>
<a id="more"></a>
<p><strong>定理</strong>：$(X,\langle \cdot,\cdot\rangle ),\forall x,y\in X$ 有</p>
<ul>
<li>$|\langle x,y\rangle |\le \Vert x\Vert \cdot\Vert y\Vert$（Schwartz 不等式）等号成立 $\iff x,y$ 线性相关 $\iff y=0$ 或 $x=cy$</li>
<li>$\Vert x+y\Vert\le\Vert x\Vert+\Vert y\Vert$ 等号成立 $\iff x,y$ 线性相关 $\iff y=0$ 或 $x=cy$</li>
</ul>
<p><strong>定理</strong>：$(X,\langle \cdot,\cdot\rangle )$，设 $x_n\to x,y_n\to y$，则 $\langle x_n,y_n\rangle  \to \langle x,y\rangle $（此定理说明<strong>内积为连续映射</strong>）。</p>
<p>证明：略。</p>
<p><strong>命题</strong>：若 $\Vert\cdot\Vert$ 为 $X$ 上的范数，若 $\forall x,y\in X$ 都满足平行四边形等式 $\Vert x+y\Vert^2 + \Vert x-y\Vert^2=2(\Vert x\Vert^2 + \Vert y\Vert^2)$，则存在 $X$ 上的内积 $\langle \cdot,\cdot\rangle $，使得 $\Vert x\Vert=\langle \cdot,\cdot\rangle ^{1/2}.$</p>
<p>证明：较复杂，略。</p>
<p><em>例子 1</em>：$X=\mathbb{K}^2,\Vert(x,y)\Vert_\infty$ 不是内积空间，反例比如 $x=(1,1),y=(1,-1)$，验证平行四边形等式不成立即可。</p>
<p><em>例子 2</em>：$(\ell^\infty,\Vert\cdot\Vert_\infty)$ 不是内积空间，反例如 $x=(1,0,0,…),y=(0,-1,0,…)$。</p>
<p>实际上对于空间 $X=\mathbb{K}^n,n&gt;0$，$\Vert x\Vert_p$ <strong>只有在 $p=2$ 的时候才是内积空间</strong>，其余情况均不是内积空间。</p>
<p>对于空间 $(\ell^p,\Vert\cdot\Vert_p)$ 也<strong>只有在 $p=2$ 的时候才是内积空间</strong>。</p>
<p><em>例子 3</em>：$C[0,1],\Vert x\Vert_\infty$ 不是内积空间，反例比如 $x(t)=1+t,y(t)=1-t$，验证平行四边形等式不成立即可(说明找不到合适的内积定义来导出无穷范数)。</p>
<p>类比上面的例子，我们也可以对连续函数定义 $2$ 范数($p$ 范数)。首先定义内积</p>
<script type="math/tex; mode=display">
\langle x,y\rangle =\int_a^b x(t)\overline{y(t)} dt \quad\Rightarrow\quad \Vert x\Vert=\langle x,x\rangle ^{1/2} \\
\Vert x\Vert_p = \left(\int_a^b |x(t)|^p dt\right)^{1/p}</script><p>同样地，<strong>只有在 $p=2$ 的时候 $(C[a,b],\Vert x\Vert_p)$ 才是内积空间。</strong></p>
<p>到这里大家基本了解了内积空间的特点，他比一般的赋范空间更严格，度量空间就更不用说了。根据初中的知识，有了内积我们就能计算夹角了，不过这里我们不讲夹角，而是考虑<strong>正交</strong>和<strong>正交补</strong>的概念。</p>
<blockquote>
<p><strong>小结</strong>：这一部分讲了内积运算的定义，并且由内积可以导出范数的定义，但是内积比范数的要求更严格，因此对于某个范数，可以通过验证平行四边形等式来验证其是否可以由内积运算来导出。</p>
</blockquote>
<h2 id="2-正交补与正交投影"><a href="#2-正交补与正交投影" class="headerlink" title="2. 正交补与正交投影"></a>2. 正交补与正交投影</h2><p>内积空间 $(X,\langle \rangle )$ 中，称 $x,y$ <strong>正交</strong>，若 $\langle x,y\rangle =0$，记为 $x\perp y$。$M\subset X$ 非空，定义其<strong>正交补</strong> $M^{\perp}=\{x\in X:x\perp y,\forall y\in M\}.$</p>
<p><strong>命题</strong>：$M^{\perp}$ 为 $X$ 的<strong>闭集线性子空间</strong>。</p>
<p>证明：易证 $M^{\perp}$ 为线性子空间，然后再用内积的是连续映射证明 $M^{\perp}$ 为闭集。</p>
<p>这里插入一个<strong>最佳逼近元</strong>的概念。定义 $\xi(x_0,M)=\inf_{y\in M}d(x_0,y)$，若存在 $y_0\in M$ 使得 $d(x_0,y_0)=\xi(x_0,M)$，那么就称 $y_0$ 为最佳逼近元。什么情况下最佳逼近元存在呢？</p>
<ol>
<li>当 $M$ 为非空紧集的时候，$y_0$ 存在（因为紧集一定是有界闭集）。</li>
<li>若 $X$ 为赋范空间，$M$ 为 $X$ 的有限维线性子空间，则 $y_0$ 存在（因为有限维赋范空间都完备，$M$ 为闭集）。</li>
</ol>
<p><strong>定理</strong>：$(X,\langle \rangle )$，$M$ 为 $X$ 非空凸子集，且 $M$ 完备，则 $\forall x_0\in X$，$\exists!y_0\in M$ 为最佳逼近元，并且有 $x_0-y_0\in M^{\perp}$。</p>
<p>证明：先找到 $y_n\in M,d(x_0,y_n)\le\xi(x_0,M)+\frac{1}{n}$，证明其为柯西列，由于 $M$ 完备，得到最佳逼近元的存在性。再由 $M$ 的凸性质证明唯一性。证毕。</p>
<p>对于线性空间 $X$，$M,N$ 为 $X$ 的线性子空间，称 $X$ 为 $M,N$ 的<strong>直和</strong>，若 $\forall x\in X,\exists! m\in M,n\in N,x=m+n$，记为 $X=M\oplus N.$</p>
<p>很容易验证 $X=M\oplus N \iff X=\text{span}(M\cup N), M\cap N=\{0\}.$</p>
<p><strong>定理</strong>：设 $H$ 为 Hilbert 空间，$M$ 为 $H$ 的闭线性子空间，则 $H=M\oplus M^{\perp}.$</p>
<p>证明：$M$ 完备，对 $\forall x\in H$，$\exists!y\in M$，使得 $\Vert x-y\Vert=\xi(x,M)$，并且 $x-y\in M^{\perp}$。证毕。</p>
<p><strong>定理</strong>：设 $H$ 为 Hilbert 空间，$M$ 为 $H$ 的闭线性子空间，则 $(M^{\perp})^{\perp}=M.$</p>
<p>证明：略。</p>
<p>上面两条定理当中，只有 $M$ 是 $H$ 的闭线性子空间才有如此良好的性质！因为只有闭线性子空间才能认为 $M$ 是尽可能“丰满”的。举个例子，$\mathbb{R}^3$ 中，令 $M=\{e_1=(1,0,0)\}$，那么 $M^{\perp}$ 为 $y-z$ 平面，$(M^{\perp})^{\perp}$ 为 $x$ 轴，相比于原始的 $M$ 扩展到无穷远处了。</p>
<p><strong>推论</strong>：设 $H$ 为 Hilbert 空间，$M\subset H$ 非空，则 $\overline{\text{span}M}=H \iff M^{\perp}=\{0\}.$</p>
<p><strong>引理</strong>：$M^{\perp}=(\bar{M})^\perp,\quad (\text{span}M)^{\perp}=M^{\perp}.$</p>
<p>证明：略。</p>
<p><strong>NOTE</strong>：这个引理实际上说明了正交补空间在定义的时候已经是“最大化的”，即使原空间 $M$ 稍微扩张一点点，其正交补空间仍然保持不变。</p>
<p>设 $H$ 为 Hilbert 空间，$M$ 为 $H$ 的闭线性子空间，那么对于 $\forall x\in H$，存在唯一的分解 $x=y+z,y\in M,z\in M^\perp.$ 记 $P_Mx=y$，称 $P_M$ 为从 $H$ 到 $M$ 上的<strong>正交投影</strong>，其有如下性质：</p>
<ol>
<li>$P_M$ 为线性算子；</li>
<li>$P_M$ 有界，并且 $P_M=1, M\ne\{0\}$，$P_M=0,M=\{0\}$；</li>
<li>$P_M^2=P_M$；</li>
<li>$R(P_M)=M, N(P_M)=M^{\perp}$。</li>
</ol>
<blockquote>
<p><strong>小结</strong>：本小节给出了正交补的定义</p>
<ol>
<li>不论原空间 $M$ 如何，正交补空间 $M^{\perp}$ 总是有一些良好的性质：1）闭线性子空间；2）“最大化的”；</li>
<li>如果原空间 $M$ 同时有良好的性质（闭线性子空间），那么他们两个就是对整个空间 $H$ 的良好分割。</li>
</ol>
</blockquote>
<h2 id="3-标准正交基"><a href="#3-标准正交基" class="headerlink" title="3. 标准正交基"></a>3. 标准正交基</h2><p>内积空间 $(X,\langle \rangle )$ 中，$M\subset X$ 为<strong>正交集</strong>，若 $\forall x,y\in M,x\ne y,\Rightarrow x\perp y.$ 进一步若 $M$ 中任意元素 $\Vert x\Vert=1$，则称 $M$ 为<strong>标准正交集</strong>。</p>
<p>对于一个标准正交序列 $M=\{e_n,n\ge1\}$，有 <strong>Bessel 不等式</strong></p>
<script type="math/tex; mode=display">
\sum_{i=1}^\infty |\langle x,e_i\rangle |^2 \le \Vert x\Vert^2.</script><p>有了标准正交集的概念，我们很容易联想到正交基。下面列出的正交集的性质都可以类比基，但是随便取一个标准正交集，其并不一定完备，因此二者又有细微的差别。</p>
<p><strong>定理</strong>：$H$ 为 Hilbert 空间，$\{e_1,e_2,…\}$ 为 $H$ 的标准正交集，那么有</p>
<ul>
<li>$\forall \lambda_n\in\mathbb{K}, \sum^\infty_n \lambda_ne_n$ 收敛 $\iff \sum_n^\infty |\lambda_n|^2&lt;\infty$</li>
<li>若 $x=\sum^\infty_n \lambda_n e_n$，则 $\lambda_n=\langle x,e_n\rangle $</li>
<li>$\forall x\in H, \sum^\infty_n\langle x,e_n\rangle e_n$ 收敛</li>
</ul>
<p>证明：由于涉及到无穷级数，因此证明过程中需要首先考虑 $S_N=\sum_{n=1}^N \lambda_n e_n$ 的情况，再证明 $N\to\infty$ 的时候极限成立。细节略。</p>
<p>如果对于标准正交集 $M$ 我们有 $\overline{\text{span}M}=H$，那么称 $M$ 在 $H$ 中为<strong>完全的</strong>。实际上这里很容易联想到完备正交基，我们知道空间中任意一点都可以用基的线性组合来表示，如果 $M$ 是完全的，那么我们可以有相似的结论。</p>
<p><strong>命题</strong>：首先定义 $\forall x\in X, M_x=\{e\in M, \langle x,e\rangle \ne0\}$，那么一定有 $M_x$ 是至多可数的。</p>
<p>证明：定义 $M_{x,n}=\{e\in M, |\langle x,e\rangle |\ge \frac{1}{n}\}$，$M_x=\cup^\infty_{n=1} M_{x,n}$，只需证明 $\forall n\ge1, M_{x,n}$ 为至多可数集。</p>
<p>假设 $M_{x,n}$ 中两两不等的元素可以表示为 $e_1,…,e_N$，那么 $\frac{N}{n^2}\le\sum_{n=1}^N|\langle x,e_i\rangle |^2\le\Vert x\Vert^2 \Rightarrow N\le n^2\Vert x\Vert^2$，说明 $M_{x,n}$ 中的元素个数是有限的。证毕。</p>
<p><strong>定理</strong>：$H$ 为 Hilbert 空间，$M$ 为 $H$ 的标准正交集，则下述命题等价：</p>
<ol>
<li>$M$ 为完全的；</li>
<li>$\forall x\in H,x=\sum_{e\in M}\langle x,e\rangle e$；</li>
<li>$\forall x\in H, \Vert x\Vert^2=\sum_{e\in M}|\langle x,e\rangle |^2.$</li>
<li>$\forall x,y\in H,\langle x,y\rangle =\sum_{e\in M}\langle x,e\rangle \langle e,y\rangle $</li>
</ol>
<p>证明：首先用反证法证明 $2\Rightarrow1,3\Rightarrow1.$ 然后考虑 $\forall x\in H, M_x=\{e_1,e_2,…\}$，令 $y=\sum^\infty_{i=1} \langle x,e_i\rangle e_i$， $z=y-x$，容易证明 $\langle z,e_i\rangle =0,\forall i\ge1$，那么就有 $z\in M^{\perp}=\{0\}$，从而 $x=y=\sum^\infty_{i=1} \langle x,e_i\rangle e_i$，$1\Rightarrow2$。其余证明可以参考课本。此处省略。</p>
<p>对于内积空间 $X$ 的一列线性无关元素 $\{x_1,…,x_n\}$，那么存在一组标准正交序列 $e_1,…,e_n$ 使得</p>
<script type="math/tex; mode=display">
\text{span}\{x_1,...,x_n\} = \text{span}\{e_1,...,e_n\}</script><p>其中一种获取方法为 <strong>Gram-Schmidt 标准正交化方法</strong>，即</p>
<ol>
<li>首先取 $e_1=\frac{x_1}{\Vert x_1\Vert}$</li>
<li>然后 $v_2=x_2-\langle x_2,e_1\rangle e_1, e_2 = \frac{v_1}{\Vert v_2\Vert}$</li>
<li>上述过程重复进行。</li>
</ol>
<p><strong>定理</strong>：$H$ 为 Hilbert 空间，$M$ 为 $H$ 的标准正交集，则存在 $N$ 为完全标准正交集，$M\subset N.$</p>
<p><strong>推论</strong>：任意 Hilbert 空间均有完全标准正交集。</p>
<p>证明：可以根据有限维空间、无限维可分空间进行讨论，应用 Gram-Schmidt 标准正交化方法即可获得完全标准正交集。</p>
<p>实际上如果我们能构造出来  Hilbert 空间的一组标准正交基，那么对于有限维空间 $\text{dim}H=n$，其与 $\mathbb{K}^n$ 等距同构；对于无穷维可分空间，其与 $\ell^2$ 等距同构。</p>
<blockquote>
<p><strong>小结</strong>：</p>
<ol>
<li>本小节讲到的标准正交集可以用于向量的线性分解表示。</li>
<li>当标准正交集是完全的，那么它实际上就是 Hilbert 空间 $H$ 的一组标准正交基。这又可以看作是线性空间中 Hamel 基/Schauder 基的更特殊的情况，因为完备的标准正交集要求相互正交，而 Hamel 基和 Schauder 基只要求线性无关。</li>
<li>实际上根据 Gram-Schmidt 标准正交化方法可以由两种基构造出标准正交基。</li>
</ol>
</blockquote>
<h2 id="4-Hilbert-空间上有界线性泛函表示"><a href="#4-Hilbert-空间上有界线性泛函表示" class="headerlink" title="4. Hilbert 空间上有界线性泛函表示"></a>4. Hilbert 空间上有界线性泛函表示</h2><p>对于内积空间 $X$，取 $z_0\in X$，那么我们可以定义一个线性泛函 $f_{z_0}(x)=\langle x,z_0\rangle $，可以验证 $f_{z_0}\in X^{\star}$，并且有 $\Vert f_{z_0}\Vert = \Vert z_0\Vert$，因此 $f_{z_0}\in X’$。</p>
<p>那么如果反过来，任取 $f\in X’$，我们是否能够断言 $f$ 都可以表示为 $f(x)=\langle x,z_0\rangle $ 的形式呢？可以！这个表示形式如此简洁，以后你会发现这个性质非常有用！但并不是任意赋范空间都可以如此表示，只有 Hilbert 空间有这个良好的性质。</p>
<blockquote>
<p><strong>定理(F.Riesz)</strong>：$H$ 为 Hilbert 空间，则 $\forall f\in H’,\exists! z_0\in H,f(x)=\langle x,z_0\rangle .$</p>
<p><strong>证明</strong>：若 $f=0$，取 $z_0=0.$</p>
<p>若$f\ne0,f\in H’$，那么 $N(f)=\{x\in H,f(x)=0\}$ 为 $H$ 的闭线性子空间，因此有 $H=N(f)\oplus N(f)^{\perp}$，且 $N(f)^{\perp}\ne\{0\}$。接下来的问题就是如何构造一个函数 $\langle x,z\rangle $ 让其等于 $f(x)$？</p>
<p>首先固定 $z_0\in N(f)^{\perp},z_0\ne0$。任给 $x\in H$，考虑 $v=f(x)z_0-f(z_0)x \in H$，显然有 $f(v)=0$，故 $v\in N(f)$，从而有</p>
<script type="math/tex; mode=display">
\langle v,z_0\rangle  = f(x)\langle z_0,z_0\rangle -f(z_0)\langle x,z_0\rangle =0 \\
\Longrightarrow f(x)=\langle x,\frac{\overline{f(z_0)}z_0}{\Vert z_0\Vert^2}\rangle =\langle x,y_0\rangle .</script><p>下面证明 $y_0$ 的唯一性。略。</p>
<p>证毕。</p>
<p><strong>NOTE</strong>：实际上 $N(f)^{\perp}$ 是一维空间，也就是 $\text{span}\{z_0\}$，因此我们随便从 $N(f)^{\perp}$ 中取一个向量 $z_0$ 乘以一个线性系数就能得到 $f(x)$。</p>
<blockquote>
<p> 显然 $g(x)=\langle x,z_0\rangle $ 并不一定等于 $f(x)$，那么我们怎么让他变化一下变成 $f$ 呢？看来需要对 $g$ 做一些映射，最简单的做一个线性变换，考虑 $g’(x)=cg(x)$，要想让 $g’(x)=f(x)$，至少应该满足 $g’(z_0)=c\langle z_0,z_0\rangle =f(z_0)\Rightarrow g’(x)=\langle x,\frac{\overline{f(z_0)}z_0}{\Vert z_0\Vert^2}\rangle =\langle x,y_0\rangle .$ 那么我们来验证一下是否任意的 $x\in H$ 都有 $g(x)=f(x)$？</p>
<p> $z_0\in N(f)^{\perp},\forall x\in N(f),g’(x)=0$ 没毛病，</p>
</blockquote>
</blockquote>
<p>假设 $X,Y$ 为 $\mathbb{K}$ 上的赋范空间，称 $f:X\times Y\to \mathbb{K}$ 为<strong>共轭双线性泛函</strong>，若</p>
<script type="math/tex; mode=display">
f(\lambda_1 x_1+\lambda_2 x_2,y)=\lambda_1 f(x_1,y)+\lambda_2 f(x_2,y) \\
f(x,\lambda_1 y_1+\lambda_2 y_2)=\overline{\lambda_1} f(x,y_1)+\overline{\lambda_2} f(x,y_2)</script><p>称其为有界的，若 $\exists C\ge0$，使</p>
<script type="math/tex; mode=display">
|f(x,y)|\le C\Vert x\Vert \Vert y\Vert, \quad \forall x\in X, y\in Y</script><p>定义其范数为</p>
<script type="math/tex; mode=display">
\Vert f\Vert = \sup_{x\ne0,y\ne0} \frac{|f(x,y)|}{\Vert x\Vert \Vert y\Vert}.</script><p><strong>定理(F.Riesz)</strong>：$H_1,H_2$ 为 Hilbert 空间，$f:H_1\times H_2\to \mathbb{K}$ 为有界共轭双线性泛函，则 $\exists! S\in B(H_1,H_2), f(x,y)=\langle Sx,y\rangle ,x\in X,y\in Y.$</p>
<p>证明：首先固定 $x\in H_1$，只关注 $H_2 \to \mathbb{K}$，即 $y\mapsto \overline{f(x,y)}$ 为有界线性泛函，因此可以 $\exists!S_x\in H_1$，使得</p>
<script type="math/tex; mode=display">
\overline{f(x,y)}=\langle y,S_x\rangle \Rightarrow f(x,y)=\langle S_x,y\rangle</script><p>现在对 $x$ 任取，可以得到 $S:H_1\to H_2$，由于 $f$ 是有界共轭双线性的，容易验证 $S$ 为有界线性算子。证毕。</p>
<p>下面如果我们定义 $g(x,y)=\overline{f(y,x)}=\langle x,Sy\rangle ,x\in H_2,y\in H_1$，那么 $g$ 也是有界共轭双线性算子，并且根据上面的 Riesz 表示定理，$\exists! T\in B(H_2，H_1)$ 使得 $g(x,y)=\langle Tx,y\rangle =\langle x,Sy\rangle .$ 由此我们就导出了伴随算子的定义，我们称 $S$ 为 $T$ 的<strong>伴随算子</strong>，记为 $S=T^{\star}$，并且根据上面的推导过程可以得到伴随算子是<strong>唯一的</strong>。</p>
<p>伴随算子有以下性质：</p>
<ul>
<li>$\Vert T^{\star}\Vert=\Vert T \Vert$</li>
<li>$(\lambda S+\mu T)^{\star}=\bar{\lambda}S^\star + \bar\mu T^{\star}$</li>
<li>$(T^\star)^\star=T$</li>
<li>$\Vert T^{\star}T\Vert=\Vert TT^{\star}\Vert=\Vert T \Vert^2$</li>
<li>$T^{\star}T=0 \iff T=0$</li>
<li>$(PS)^\star=S^\star P^\star$</li>
</ul>
<p><em>例子 1</em>：伴随算子一个最简单的例子就是矩阵。$H_1=H_2=\mathbb{K}^n,T\in B(H_1,H_2),\exists!A$ 为 $n$ 阶方针，使得 $Tx=Ax$，容易验证 $T^\star x=A^H x.$</p>
<p><strong>定理</strong>：$H_1,H_2$ 为 Hilbert 空间，$T\in B(H_1,H_2)$ 为一一映射，则 $T$ 为<strong>等距同构</strong>，当且仅当</p>
<script type="math/tex; mode=display">
TT^\star=I_{H_2},\quad T^\star T=I_{H_1.}</script><p>此时称 $T$ 为 $H_1$ 到 $H_2$ 的<strong>酉算子</strong>。</p>
<p>证明：要证明等距同构只需要验证 $\langle Tx,Ty\rangle _2=\langle x,y\rangle _1,\forall x,y\in H_1$ 或者 $\langle T^\star x,T^\star y\rangle _1=\langle x,y\rangle _2,\forall x,y\in H_2$ 成立，做一下变换即可证明。证毕。</p>
<p><strong>NOTE</strong>：实际上这里可以联想到酉矩阵。</p>
<blockquote>
<p><strong>小结</strong>：这一小节最核心的内容就是 Riesz 表示定理，即 <strong>Hilbert 空间</strong>上任意<strong>有界线性泛函</strong>都可以<strong>唯一地</strong>表示为</p>
<script type="math/tex; mode=display">
f(x) = \langle x,z_0\rangle , \forall x\in H</script></blockquote>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>Hilbert空间</tag>
        <tag>内积空间</tag>
        <tag>共轭双线性泛函</tag>
        <tag>伴随算子</tag>
        <tag>Riesz表示定理</tag>
        <tag>直和</tag>
        <tag>正交补</tag>
        <tag>标准正交基</tag>
        <tag>标准正交集</tag>
        <tag>Bessel不等式</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记2：赋范空间</title>
    <url>/2020/11/10/functional-analysis/ch2-normed-space/</url>
    <content><![CDATA[<p>在度量空间中，我们重点关注的是两个元素之间的距离，而这一部分要引出来的赋范空间中，则对每个元素本身也赋予了“范数”，也就是“长度”。</p>
<h2 id="1-线性空间"><a href="#1-线性空间" class="headerlink" title="1. 线性空间"></a>1. 线性空间</h2><p>线性空间，可以简单理解为对线性运算封闭的集合。那么线性运算的形式是什么呢？常见的线性运算可以写为 $T(x;a)=\sum_i a_ix_i$，因此可以看出来包含了<strong>数乘</strong>和<strong>加法</strong>两种基本运算，因此线性空间必然需要对数乘和加法封闭，除此之外，为了保证运算体系的自洽性，我们还需要规定一些其他的运算规则。最后，给出来线性空间的定义：</p>
<a id="more"></a>
<p>考虑 $\mathbb{K=C\ or\ R}, X\ne \varnothing,\forall x,y\in X,\forall \lambda\in\mathbb{K}$，可以定义 $x+y\in X,\lambda x\in X$，如果满足</p>
<ol>
<li>$x+y=y+x$</li>
<li>$(x+y)+z=x+(y+z)$</li>
<li>$\exists 0\in X,\forall x\in X, x+0=0+x=x$</li>
<li>$\forall x,\exists! y\in X,x+y=0,y=-x$</li>
<li>$\alpha(\beta x)=(\alpha\beta)x$</li>
<li>$1x=x$</li>
<li>$(\alpha+\beta)x=\alpha x+\beta x$</li>
<li>$\alpha(x+y)=\alpha x+\alpha y$</li>
</ol>
<p>则称 $X$ 为 $\mathbb{K}$ 上的<strong>线性空间</strong>。</p>
<p><strong><em>例子 1</em></strong>：$S=\{(x_n)_{n\ge1},x_n\in\mathbb{K} \}$，定义加法 $(x_n)_{n\ge1}+(y_n)_{n\ge1}=(x_n+y_n)_{n\ge1}\in S$，定义数乘 $\lambda(x_n)_{n\ge1}=(\lambda x_n)_{n\ge1}\in S$，那么可以验证 $S$ 为线性空间。</p>
<p><strong><em>例子 2</em></strong>：$C[a,b]$，定义 $(f+g)(t)=f(t)+g(t), (\lambda f)(t)=\lambda f(t)$，可以验证 $C[a,b]$ 为线性空间。</p>
<p><strong>定义</strong>：$X$ 为 $\mathbb{K}$ 上的线性空间，$Y\subset X$，称 $Y$ 为 $X$ 的线性子空间，若 $\forall x,y\in Y,\forall \lambda\in\mathbb{K}$，有 $x+y\in Y,\lambda x\in Y$，并且 $Y$ 本身也是线性空间。</p>
<p><strong><em>例子 3</em></strong>：$\ell^\infty \subset S$ 是线性空间。</p>
<p><strong><em>例子 4</em></strong>：$\ell^p \subset S,1\le p &lt; \infty$ 是线性空间。</p>
<p><strong><em>例子 5</em></strong>：$\ell^p \subset c_0(x_n\to0的序列\{(x_n)\}) \subset c(x_n收敛的序列) \subset \ell^\infty\subset S(无穷维序列)$，每一个都是线性空间。</p>
<p><strong><em>例子 6</em></strong>：$P$ 所有多项式的集合，$P\subset C[a,b]$ 也是线性空间。</p>
<p><strong>命题</strong>：若 $X$ 为线性空间，$(Y_i)_{i\in I}$ 为 $X$ 的一族线性子空间，则 $Y=\bigcap_{i\in I}Y_i$ 仍然是 $X$ 的线性子空间。</p>
<p><strong>定义</strong>：$X$ 为线性空间，$M\subset X$ 非空，令 $\mathcal{E}$ 为所有包含 $M$ 的线性子空间，那么一定有 $X\in\mathcal{E}$，因此 $\mathcal{E}$ 一定非空。令 $Z=\bigcap_{Y\in\mathcal{E}}Y$ 也一定是 $X$ 的线性子空间，记 $Z=\text{span}(M)$ 称为 $M$ <strong>生成的线性子空间</strong>。</p>
<p><strong>命题</strong>：若 $M\subset X$ 非空，那么 $Z=\text{span}(M) = \{\sum_i^n \lambda_i x_i,n\ge1,x_i\in M,\lambda_i\in\mathbb{K}\}$ 为包含 $M$ 的 $X$ 的<strong>最小线性子空间</strong>。</p>
<p>证明：略。</p>
<p><strong><em>例子 1</em></strong>：$X=S,e_n=(0,\cdots,0,1,0,\cdots)\in S, M=\{e_n,n\ge1\}\subset S$，则 $\text{span}(M)=c_{00}$(只有有限个元素非零的无穷维序列)。</p>
<p><strong>定义</strong>：$M\subset X$ 为<strong>线性无关</strong>的，若 $\forall x_1,…,x_n\in M$ 两两不等，都有 $x_1,…,x_n$ 都线性无关(也即任意有限个元素都线性无关)。</p>
<p><strong><em>例子 2</em></strong>：$X=C[a,b],\alpha_1&lt;\alpha_2&lt;\cdots&lt;\alpha_n\le\cdots, f_n=e^{\alpha_n t}\in C[a,b]$，那么取 $M=\{f_1,f_2,\cdots\}$ 是线性无关的 $\iff \forall n\ge1, f_1,…,f_n$ 线性无关。</p>
<p><em>证明</em>：利用线性相关的定义，重复求导、相减的过程，细节略。</p>
<p><strong>定义</strong>： $X$ 为线性空间，若 $X$ 中存在 $n$ 个元素线性无关，但任意 $n+1$ 个元素都线性相关，则称 $X$ 为 $n$ <strong>维</strong>的，记为 $\text{dim}X=n$。若 $\forall n\ge1,\exists x_!,…,x_n$ 线性无关，则称 $X$ 为<strong>无穷维</strong>的。</p>
<p><strong>命题</strong>：$\text{dim}X=n,\forall x\in X$ 都存在唯一的系数 $a_1,…,a_n$ 使得 $x=a_1x_1+\cdots+a_nx_n$。</p>
<p>证明：略。</p>
<p>对于复空间 $\mathbb{C}^n$，认为 $\text{dim}\mathbb{C}^n=2n$。</p>
<p><strong>定义</strong>：$X$ 为线性空间，$M\subset X$ 线性无关，若 $\text{span}(M)=X$，则称 $M$ 为 $X$ 的 <strong>Hamel 基</strong>。</p>
<blockquote>
<p><strong>命题</strong>：$\forall x\in X$，则 $\exists x_1,…,x_n\in M$ 两两不等，$\exists \lambda_1,…,\lambda_n\in\mathbb{K}$ 均不为 0，使得 $x=\lambda_1 x_1+…+\lambda_n x_n$，并且上述<strong>表示唯一</strong>。</p>
<p>证明：略。</p>
</blockquote>
<p><strong><em>例子 1</em></strong>：$\{e_1,e_1,…\}$ 为 $c_{00}$ 的 Hamel 基。</p>
<p><strong>定理</strong>：$X$ 为线性空间，$M\subset X$ 线性无关，则 $\exists N$ 为 $X$ 的 Hamel 基，使得 $M\subset N$。</p>
<blockquote>
<p><strong>小结</strong>：这一部分主要是讲解了线性空间的定义，最关键的概念就在于</p>
<ol>
<li>对于加法和数乘封闭；</li>
<li>可以找到一组基，任意向量都可以用基的线性组合来表示，并且表示方法唯一。</li>
</ol>
</blockquote>
<h2 id="2-赋范空间与Banach空间"><a href="#2-赋范空间与Banach空间" class="headerlink" title="2. 赋范空间与Banach空间"></a>2. 赋范空间与Banach空间</h2><p>假设 $X$ 为 $\mathbb{K}$ 上的线性空间，定义范数运算 $\Vert\cdot\Vert:X\to \mathbb{R}$，满足如下条件：</p>
<ol>
<li>$\Vert x\Vert \ge 0$</li>
<li>$\Vert x\Vert =0\iff x=0$</li>
<li>$\Vert\lambda x\Vert=|\lambda|\cdot\Vert x\Vert$</li>
<li>$\forall x,y\in X, \Vert x+y\Vert\le \Vert x\Vert + \Vert y\Vert$</li>
</ol>
<p>则称 $\Vert\cdot\Vert$ 为 $X$ 上的范数，$(X,\Vert\cdot\Vert)$ 为<strong>赋范空间</strong>。</p>
<p><strong>定义</strong>：在赋范空间中，$\forall x,y\in X$，若定义 $d(x,y)=\Vert x-y\Vert$，那么该方法定义的 $d(x,y)$ 也是度量，称为 $\Vert\cdot\Vert$ <strong>诱导的度量</strong>。若此时 $(X,d)$ 为<strong>完备空间</strong>，则称 $(X,\Vert\cdot\Vert)$ 为 <strong>Banach 空间</strong>（即完备的赋范空间）。</p>
<p><strong>注</strong>：由于范数的定义中第 3 条存在，以及诱导度量只有 $x-y$ 决定，使得诱导度量相比于一般定义的度量还具有一些特别性质，例如：</p>
<ul>
<li>平移不变性 $d(x+a,y+a)=d(x+y)$</li>
<li>齐次性 $d(\lambda x,\lambda y)=|\lambda| d(x,y)$</li>
</ul>
<p><strong><em>例子 1</em></strong>：$(\mathbb{K}^n,\Vert\cdot\Vert_\infty),(\mathbb{K}^n,\Vert\cdot\Vert_p),(\ell^\infty,\Vert\cdot\Vert_\infty),(\ell^p,\Vert\cdot\Vert_p),(c_{0},\Vert\cdot\Vert_\infty),(c,\Vert\cdot\Vert_\infty)$ 均为 Banach 空间。</p>
<p><strong><em>例子 2</em></strong>：$S=\{(x_n)_{n\ge1},x_n\in\mathbb{K}\},d(x,y)=\sum_n \frac{1}{2^n}\frac{|x_n-y_n|}{1+|x_n-y_n|}$，<strong>不存在</strong> $S$ 上的范数 $\Vert\cdot\Vert$ 使 $d$ 被诱导出，因为 $d$ 不满足齐次性。</p>
<p> <strong><em>例子 3</em></strong>：$(C[a,b],\Vert\cdot\Vert_\infty)$ 为 Banach 空间，$(C[a,b],\Vert\cdot\Vert_p)$ 不是 Banach 空间。</p>
<p><strong><em>例子 4</em></strong>：离散度量不满足齐次性，因此不可能由范数诱导出。</p>
<p><strong>命题 1</strong>：$(X,\Vert\cdot\Vert_\infty)$ 为赋范空间，$Y$ 为 $X$ 的线性子空间。若 $Y$ 为 Banach 空间，则 $Y$ 在 $X$ 中必为闭集。</p>
<p><strong>命题 2</strong>：$(X,\Vert\cdot\Vert_\infty)$ 为 Banach 空间，$Y\subset X$ 为闭集线性子空间，则 $Y$ 必为 Banach 空间。</p>
<p>证明：赋范空间的线性子空间仍然是赋范空间，完备空间一定要是闭集。细节略。</p>
<blockquote>
<p>只需要掌握关键点，那么上面的命题都可以由下面的关键点轻松导出：</p>
<ol>
<li>完备空间一定是闭集；完备空间的子空间也完备 $\iff$ 该子空间为闭集；</li>
<li>赋范空间 + 完备性 = Banach 空间。</li>
</ol>
</blockquote>
<p><strong>命题</strong>：若 $(X,\Vert\cdot\Vert)$ 为 Banach 空间，$x_n\in X$，且 $\sum_n^\infty \Vert x_n\Vert &lt; \infty$，则 $\sum^\infty x_n$ 收敛。</p>
<p>证明：完备空间中，证明序列收敛可以证明序列是柯西列。</p>
<p><strong>定义</strong>：$(X,\Vert\cdot\Vert)$ 为赋范空间，$e_n\in X(n\ge 1)$，称 $(e_n)_{n\ge1}$ 为 $X$ 的一组 <strong>Schauder 基</strong>，若 $\forall x\in X, \exists ! \lambda_n,x=\sum^\infty_n \lambda_n e_n$。</p>
<p><strong>命题</strong>：若 $X$ 有 Schauder 基，那么 $X$ 一定是可分的。</p>
<p>证明：可以取 $M=\{\sum^n_{i=1}x_ie_i \in X, n\ge1, x_i\in\mathbb{Q} \}$，是可数个可数集的并。</p>
<blockquote>
<p>Hamel 基与 Schauder 基的区别：</p>
<ol>
<li>最主要的区别是 Hamel 基可以是有限维也可以是无穷维的，这由集合 $X$ 的维数决定，而 Schauder 基则只定义在无穷维上；</li>
<li>前者针对线性空间，后者针对 Banach 空间。</li>
</ol>
</blockquote>
<h2 id="3-有限维赋范空间"><a href="#3-有限维赋范空间" class="headerlink" title="3. 有限维赋范空间"></a>3. 有限维赋范空间</h2><p>有限维赋范空间相比于一般的赋范空间有很多很好的性质，比如所有范数等价、有限维赋范空间都是 Banach 空间。</p>
<p>假设 $X$ 为 $\mathbb{K}$ 上的线性空间，并且存在两个范数 $\Vert\cdot\Vert_1,\Vert\cdot\Vert_2$，称二者等价，若</p>
<script type="math/tex; mode=display">
\exists \alpha,\beta > 0,\forall x\in X,\quad \alpha\Vert x\Vert_1\le \Vert x\Vert_2\le \beta\Vert x\Vert_2</script><p>此时 $(X,\Vert\cdot\Vert_1),(X,\Vert\cdot\Vert_2)$ 有：</p>
<ul>
<li>相同的收敛列、柯西列；</li>
<li>相同的开集、闭集、闭包、内部；</li>
<li>$(X,\Vert\cdot\Vert_1)$ Banach $\iff (X,\Vert\cdot\Vert_2)$ Banach；</li>
</ul>
<blockquote>
<p><strong>引理</strong>：$(X,\Vert\cdot\Vert)$，$Y\subset X$ 为有限维线性子空间，设 $e_1,…,e_n$ 为 $Y$ 的一组基，则 $\exists c &gt; 0,\forall \lambda_1,…,\lambda_n \in \mathbb{K}$，有</p>
<script type="math/tex; mode=display">
c(|\lambda_1|+\cdots+|\lambda_n|) \le \Vert\lambda_1 e_1+\cdots+\lambda_n e_n\Vert \le \max_i\Vert e_i\Vert (|\lambda_1|+\cdots+|\lambda_n|)</script><p>证明：需要用到 Bolzano-Weierstrass 定理，即<strong>有界列存在收敛子列</strong>。反证法，证明略。</p>
<p><strong>定理</strong>：若 $\text{dim}X&lt;\infty$，则 $X$ 上的<strong>范数互相等价</strong>，且均为 Banach 空间。</p>
<p>证明：只需要证明所有范数与 $\Vert\cdot\Vert_1$ 范数等价，且 $(X,\Vert\cdot\Vert_1)$ 构成 Banach 空间。</p>
</blockquote>
<p><strong>推论</strong>：$(X,\Vert\cdot\Vert)$，若 $Y\subset X$ 为有限维线性子空间，则 $Y$ 为闭集。</p>
<p>有限维赋范空间除了这个良好的性质（一定是 Bananch 空间）之外，还有其他的好性质。下面引入紧集的概念。</p>
<p><strong>定义</strong>：$(X,d)$，$M\subset X$ 为紧集，若 $\forall x_n\in M,\exists n_k\uparrow \infty,\exists x\in M, x_{n_k}\to x(k\to \infty).$</p>
<p>实际上，紧集的概念跟完备性的概念有点像，前者是任意序列一定存在收敛子列，后者说明柯西列一定是收敛列。他们都跟序列的收敛性有关，后面也可以看到，他们都跟闭集有一定的关系。</p>
<p>实际上，紧集一定是有界闭集，但是闭集不一定是紧集。不过对于有限维赋范空间来说，二者等价，后面会有定理专门证明。</p>
<p><strong>定理</strong>：$(X,d)$，若 $M\subset X$ 为<strong>紧集</strong>，则 $M$ 一定<strong>完备</strong>，并且一定是<strong>有界闭集</strong>。</p>
<p><strong>定理</strong>：$(X,d)$，$M\subset X$ 为紧集，$N\subset M$，则 $N$ 为紧集 $\iff N$ 为闭集。</p>
<p>证明：略。</p>
<p><strong>定理</strong>：$(X,\Vert\cdot\Vert)$，$\text{dim}X=n&lt;\infty$，$M\subset X$ 为紧集 $\iff M$ 为有界闭集。</p>
<p><strong>证明</strong>：必要性易证，充分性证明的关键是首先找到 Hamel 基表示，将在 $M$ 中寻找收敛子列的任务分解到对每一个坐标维度上寻找收敛子列，而这可以根据 Bolzano-Weiertrass 定理得到，然后再根据每个坐标为度上的收敛子列得到 $M$ 中的收敛子列。证毕。</p>
<p><strong>引理</strong>：$(X,\Vert\cdot\Vert)$，$M\subsetneq X$ 为闭集线性子空间，对于 $\forall 0 &lt; \theta &lt; 1$，$\exists z\in X,\Vert z\Vert =1,\forall y\in M,\Vert z-y\Vert \ge \theta.$</p>
<p><strong>证明</strong>：固定 $\forall v\in Y^c$，令 $a=\inf_{y\in Y}\Vert v-y\Vert$，那么 $a&gt;0$，否则的话与 $Y$ 是闭集相矛盾。因此存在 $y_0\in Y$ 使得 $a\le \Vert v-y_0\Vert \le a/\varepsilon$。那么我们就可以把这个 $v$ 平移到 $0$ 点附近，同时也把对应的 $y_0$ 平移到原点附近，也就是取 $y=\frac{v-y_0}{\Vert v-y_0\Vert}$，那么 $\Vert y\Vert=1$，并且可以验证 $\forall z\in X$，都有 $\Vert y-z\Vert \ge \varepsilon.$ 证毕。</p>
<p><strong>定理</strong>(F.Riesz)：$(X,\Vert\cdot\Vert)$，则 $\text{dim}X&lt;\infty \iff \bar{B}(0,1)$ 为紧集。</p>
<p><strong>证明</strong>：必要性易证。充分性可以利用反证法，若维度无穷，则可以构造出特殊的序列，使得序列中任意两个元素的距离都大于 $1/2$（这要用到上一个引理），从而不存在收敛子列，即 $\bar{B}(0,1)$ 不是紧集。证毕。</p>
<p><strong>推论</strong>：$(X,\Vert\cdot\Vert)$，则 $\text{dim}X&lt;\infty\iff S(0,1)=\{x:\Vert x\Vert=1\}$ 为紧集。</p>
<p><strong>定义</strong>：$(X,d)$，$M\subset X$ 非空，$x_0\in X$，令</p>
<script type="math/tex; mode=display">
\rho(x_0, M) = \inf_{y\in M} d(x_0, y)</script><p>称为 $x_0$ 到 $M$ 的距离。若 $\exists y_0\in M$，使得 $\rho(x_0,M)=d(x_0,y_0)$，则称 $y_0$ 为 $x_0$ 在 $M$ 中的最佳逼近元。</p>
<p><strong>命题</strong>：设 $M$ 为非空紧集，则 $\forall x_0 \in X$，$x_0$ 在 $M$ 中的最佳逼近元存在。</p>
<p><strong>命题</strong>：$(X,d), M$ 为 $X$ 的有限维线性子空间，$\forall x_0\in X, \rho(x_0,M)=\inf_{y\in M}\{\Vert x_0-y\Vert\}$，则 $x_0$ 在 $M$ 中的最佳逼近元一定存在。</p>
<p>证明：略。</p>
<blockquote>
<p><strong>小结</strong>：本部分主要证明了有限维赋范空间的良好性质：</p>
<ol>
<li>有限维赋范空间<strong>所有范数等价</strong>，且均为 <strong>Banach 空间</strong>；</li>
<li>有限维赋范空间<strong>紧集</strong>（即任意序列存在收敛子列） $\iff$ <strong>有界闭集</strong>；</li>
</ol>
</blockquote>
<h2 id="4-有界线性算子"><a href="#4-有界线性算子" class="headerlink" title="4. 有界线性算子"></a>4. 有界线性算子</h2><h3 id="4-1-线性算子"><a href="#4-1-线性算子" class="headerlink" title="4.1 线性算子"></a>4.1 线性算子</h3><p><strong>定义</strong>：$X,Y$ 为 $\mathbb{K}$ 上的线性子空间，$D(T)\subset X$ 为线性子空间，$T:D(T)\to Y$ 为<strong>线性算子</strong>，若 $\forall x,y\in D(T),\forall \lambda \in \mathbb{K}$，都有</p>
<script type="math/tex; mode=display">
T(x+y)=Tx+Ty,\quad T(\lambda x)=\lambda Tx \\
\iff T(\lambda x+\mu y) = \lambda Tx + \mu Ty.</script><p><strong>定义</strong>：零空间 $N(T)=\text{ker}(T)=\{x\in D(T), Tx=0\}=T^{-1}(0),$ 像空间 $R(T)=\{Tx, x\in D(T)\}.$</p>
<p><strong>命题</strong>：$N(T)$ 为 $D(T)$ 的线性子空间，$R(T)$ 为 $Y$ 的线性子空间。</p>
<p><strong>命题</strong>：$T$ 为单射 $N(T)=\{0\}$。</p>
<p><strong>命题</strong>：$M\subset D(T)$ 为 $n$ 维线性子空间，则 $\text{dim} T(M) \le n$。</p>
<p>证明：略。</p>
<h3 id="4-2-算子范数"><a href="#4-2-算子范数" class="headerlink" title="4.2 算子范数"></a>4.2 算子范数</h3><p><strong>定义</strong>：对于线性算子，若 $\exists c\ge 0,\forall x\in D(T), \Vert Tx\Vert_Y \le c\Vert x\Vert_X$，则称 $T$ 是<strong>有界的</strong>。使该式成立的最小 $c$ 称为 $T$ 的<strong>范数</strong>，等价于</p>
<script type="math/tex; mode=display">
\Vert T\Vert = \sup_{x\in D(T),x\ne 0} \frac{\Vert Tx\Vert}{\Vert x\Vert} = \sup_{x\in D(T),\Vert x\Vert \le 1} \frac{\Vert Tx\Vert}{\Vert x\Vert} = \sup_{x\in D(T),\Vert x\Vert < 1} \frac{\Vert Tx\Vert}{\Vert x\Vert}</script><p><strong><em>例子 1</em></strong>：$X=Y=C[0,1],(Tx)(t)=\int_0^t x(\tau)d\tau$，则 $\Vert T\Vert = 1, x(t)\equiv1$ 时取到。</p>
<p><strong><em>例子 2</em></strong>：$X=C[-1,1],f(x)=\int_{-1}^0x(t)dt - \in_0^1x(t)dt,|f(x)|\le 2\Vert x\Vert_\infty$，但是 $2$ 取不到。</p>
<p><strong><em>例子 3</em></strong>：$T:C^1[0,1]\to C[0,1],T(x)=x’$，那么 $T$ 为线性<strong>无界</strong>算子。令 $x_n(t)=t^n,t\in[0,1]$，则 $(Tx_n)(t)=nt^{n-1}.$</p>
<p><strong><em>例子 4</em></strong>：$A=(a_{ij})_{n\times n},a_{ij}\in\mathbb{K},Tx=Ax.\Vert T\Vert=?$</p>
<p><strong>定理</strong>：$X,Y$ 为 $\mathbb{K}$ 上的赋范空间，假设 $\text{dim}X=n&lt;\infty$，$T:X\to Y$ 是线性算子，那么 $T$ 一定是有界的。</p>
<p>证明：用基来表示 $X$ 中的元素，并利用性质 $\Vert\lambda_1 e_1+\cdots+\lambda_n e_n\Vert \ge c(|\lambda_1|+\cdots+|\lambda_n|)$ 即可得证。证毕。</p>
<p><strong>定理</strong>：$X,Y$ 为 $\mathbb{K}$ 上的赋范空间，$T:X\to Y$ 是线性算子，那么以下命题等价</p>
<ol>
<li>$T$ 有界</li>
<li>$T$ 处处连续</li>
<li>$T$ 在 $X$ 上某一点连续</li>
</ol>
<p><strong>证明</strong>：$(1)\to(2),(2)\to(3)$ 易证；</p>
<p>$(2)\to(1)$：由于 $T$ 连续，在 $x=0$ 点附近，存在 $\delta&gt;0, \forall x \in B(0,\delta)$，有 $\Vert Tx\Vert\le1$，因而 $\Vert Tx\Vert \le \frac{2}{\delta}\Vert x\Vert$；</p>
<p>$(3)\to(2)$：由于 $T$ 为线性算子，那么只要在任一 $x_0$ 点处连续，经过平移可以得到 $T$ 在任意一点处都连续。</p>
<p>证毕。</p>
<p><strong>推论</strong>：$X,Y$ 为 $\mathbb{K}$ 上的赋范空间，$T:X\to Y$ 线性有界，那么 $N(T)$ 为 $X$ 的闭集线性子空间。</p>
<p>用符号 $B(X,Y)$ 来表示 $X\to Y$ 的有界线性算子。</p>
<p><strong>定理</strong>：假设 $X$ 为赋范空间，$Y$ 为 Banach 空间，那么 $B(X,Y)$ 为 Banach 空间。</p>
<p>证明：略。</p>
<blockquote>
<p><strong>小结</strong>：本部分定义了线性算子，以及有界线性算子的范数。由于<strong>有界线性算子等价于连续算子</strong>，今后主要研究的也是有界线性算子。</p>
</blockquote>
<h2 id="5-有界线性泛函"><a href="#5-有界线性泛函" class="headerlink" title="5. 有界线性泛函"></a>5. 有界线性泛函</h2><p>从赋范空间 $X$ 到 $\mathbb{K}$ 的线性算子称为 $X$ 上的<strong>线性泛函</strong>。</p>
<p><strong>定义</strong>：$(X,\Vert\cdot\Vert)$，用 $X’$ 表示 $X$ 上所有有界线性泛函全体，称之为 $X$ 的<strong>拓扑对偶空间</strong>。</p>
<p>若 $X$ 上的一组基为 $\{e_1,e_2,\cdots\}$（有限维或者无限维均可），那么 $f:X\to\mathbb{K}$ 由 $f(e_1),f(e_2),\cdots$ 唯一确定。</p>
<p><strong>定义</strong>：$(X,Y)$ 赋范空间，若存在线性算子 $T:X\to Y$，并且 $\Vert Tx\Vert_Y = \Vert x\Vert_X,\forall x\in X$，则称 $X,Y$ 为<strong>等距同构</strong>的，因此可以视 $X,Y$ 为同一空间。</p>
<p><strong>命题</strong>：$(\mathbb{K}^n,\Vert\cdot\Vert_2)’ = (\mathbb{K}^n,\Vert\cdot\Vert_2)$（此命题的含义为 $(\mathbb{K}^n,\Vert\cdot\Vert_2)$ 的对偶空间等价于 $n$ 维空间，也就是说每一个有界线性泛函都可以映射为 $\mathbb{K}^n$ 上的一个点/向量）</p>
<p>证明：略。</p>
<p>对于形如 $(X,\Vert\cdot\Vert_1)’ = (Y,\Vert\cdot\Vert_2)$ 的命题，证明思路如下。</p>
<p><strong>证明</strong>：要想证明两个空间等价，那么只需要找到一个线性映射  $T:(X,\Vert\cdot\Vert_1)’ \to (Y,\Vert\cdot\Vert_2)$，使得映射前后在两个空间的范数相等。考虑对 $\forall f\in (X,\Vert\cdot\Vert_1)’$，取 $f\mapsto (f(e_1),f(e_2),\cdots,f(e_n))$，那么按照以下步骤证明：</p>
<ol>
<li>证明对于任意 $\forall f\in (X,\Vert\cdot\Vert_1)’$，的确有 $(f(e_1),f(e_2),\cdots,f(e_n))\in Y$，即映射的源空间和像空间的确是 $X,Y$；</li>
<li>证明 $T$ 为单射（即证明若 $Tf=Tg$，那么有 $f=g$）；</li>
<li>证明 $T$ 为满射，此时 $T$ 为双射（即证明 $\forall \alpha \in Y$，存在 $f \in (X,\Vert\cdot\Vert_1)’$ 使得 $Tf=\alpha$，一般需要构造线性算子 $f$）；</li>
<li>证明 $\Vert Tf\Vert = \Vert f\Vert$。</li>
</ol>
<p>证毕。</p>
<p>因此按照上面的方法，还可以证明如下几个命题。</p>
<ul>
<li>$(\mathbb{K}^n,\Vert\cdot\Vert_p)’ = (\mathbb{K}^n,\Vert\cdot\Vert_q),\frac{1}{p}+\frac{1}{q}=1$</li>
<li>$(c_0, \Vert \cdot\Vert_\infty)’=(\ell^1,\Vert \cdot\Vert_1)$</li>
<li>$(\ell^1,\Vert \cdot\Vert_1)’=(\ell^\infty,\Vert \cdot\Vert_\infty)$</li>
<li>$(\ell^p,\Vert \cdot\Vert_p)’=(\ell^q,\Vert \cdot\Vert_q)$</li>
</ul>
<blockquote>
<p><strong>小结</strong>：有界线性泛函可以映射到某个我们熟悉的空间，对于后面的处理很有用。</p>
</blockquote>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>线性空间</tag>
        <tag>赋范空间</tag>
        <tag>Banach空间</tag>
        <tag>Hamel基</tag>
        <tag>Schauder基</tag>
        <tag>线性算子</tag>
        <tag>算子范数</tag>
        <tag>线性泛函</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记1：度量空间</title>
    <url>/2020/10/18/functional-analysis/ch1-metric-space/</url>
    <content><![CDATA[<p>这一章节研究度量空间的基本结构，在开始前，我们需要思考几个问题：</p>
<ol>
<li>什么是度量空间？</li>
<li>我们为什么要首先学习度量空间呢？</li>
</ol>
<p>在这篇笔记的最后再来回答这个问题。</p>
<a id="more"></a>
<h2 id="1-度量空间定义"><a href="#1-度量空间定义" class="headerlink" title="1. 度量空间定义"></a>1. 度量空间定义</h2><p>对于工科生来说，“空间”这个概念还是比较模糊的，但是我本人在学习数学的过程中发现很多数学的分支学科都是对于某个空间进行研究，比如欧氏空间、内积空间、测度空间、拓扑空间、希尔伯特空间，以及这门课要学习的度量空间等等。</p>
<p>个人觉得，定义空间的同时往往伴随着相应的某种运算的定义，也就是说我们研究空间的时候，主要还是为了研究某些运算，为了保证运算前后的结果都有比较好的性质（或者说便于处理），我们才抽象出了对应的空间的概念。回到这门课，泛函分析里面将要遇到的空间都是度量空间，因此我们首先需要知道他的定义。</p>
<p><strong>“度量”实际上指的就是抽象的“距离”</strong>，比如两个点的距离、两个向量的距离、两个无穷长序列的距离、两个连续函数的距离。尽管点、向量、无穷长序列、连续函数的形式差别很大，但是如果我们都把他们抽象成为高维空间中的一个点，我们可以用相似的方式定义距离（也就是度量）的概念。</p>
<p>这样一来，由于<strong>大多数时候我们关心的是两个高维点之间的相对距离，而不是他们本身的绝对坐标</strong>（比如我们研究数列收敛性 $x_n\to x$ 的时候，只需要分析 $|x_n-x|\to 0$ 是否成立，而不太关心 $x$ 具体是什么），因此抽象出距离这个概念之后，我们就可以把点、向量、序列、连续函数都统一的看待，用统一的方法和理论处理相似的问题。</p>
<p>首先，我们可以抽象出一个集合 $X$。$X$ 可以是一个实数点集，比如 $X=\mathbb{R}$；也可以是一个多维实空间 $X=\mathbb{R}^n$，他的元素就是向量；也可以是连续函数集 $X=C[a,b]$，表示区间 $[a,b]$ 上的所有连续函数构成的集合。</p>
<p>现在问题的关键就是<strong>如何定义度量</strong>？当然不能随意定义，我们必须要他满足一定条件，才能称之为度量：</p>
<ol>
<li>$\forall x,y\in X, d(x,y)\ge0$;</li>
<li>$x,y\in X, d(x,y)=0 \iff x=y$</li>
<li>$\forall x,y\in X, d(x,y)=d(y,x)$</li>
<li>$\forall x,y,z\in X, d(x,y)\le d(x,z)+d(z,y)$</li>
</ol>
<p>根据定义可以看出来，度量就是一种距离，称 $(X,d)$ 为<strong>度量空间</strong>（或<strong>距离空间</strong>）。下面给出一些度量空间的例子，要证明是否是度量只需要验证几条定义即可。</p>
<p><strong>注</strong>：后面的内容基本都默认我们是在度量空间 $(X,d)$ 中讨论，为了书写省劲，一般都没有明确写出来“存在度量空间 $(X,d)$”，但是需要注意我们是在度量空间中讨论的。</p>
<p><strong><em>例子 1(离散度量)</em></strong>：定义</p>
<script type="math/tex; mode=display">
d(x,y) = \begin{cases}
0, & x=y \\
1, & x\ne y
\end{cases}</script><p><strong><em>例子 2</em></strong>：记 $X=\mathbb{K}$，其中我们记 $\mathbb{K}=\mathbb{R} \text{ or } \mathbb{C}$（后面的笔记也都保持这个习惯），$d(x,y)=|x-y|$，那么 $(X,d)$ 是一个度量空间。</p>
<p><strong><em>例子 3</em></strong>：$X=\mathbb{K}^n, d_p(\boldsymbol{x,y})=|\boldsymbol{x-y}|_p = (\sum_i |x_i-y_i|^p)^{1/p}$，对于 $1\le p \le \infty$，$(X,d_p)$ 是度量空间。</p>
<p><strong><em>例子 4</em></strong>：$X=\ell^p,d_p(\boldsymbol{x,y})=|\boldsymbol{x-y}|_p$，对于 $1\le p\le \infty$，$(\ell^p,d_p)$ 是度量空间。其中</p>
<script type="math/tex; mode=display">
\ell^p = \left\{(x_n)_{n\ge1},\ x_n\in\mathbb{K},\ \exists c\ge0,\ |x_n|\le c  \right\}</script><p>这个在证明的过程中需要证明 $\forall x,y\in\ell^p$，都有 $x+y \in \ell^p$。</p>
<p>例子3、4的证明过程中还需要用到两个比较重要的不等式：Hölder 不等式和 Minkowski 不等式。</p>
<blockquote>
<p><strong>Hölder不等式</strong>：$\forall 1\le p,q\le\infty, \frac{1}{p}+\frac{1}{q}=1,\forall x,y\in\mathbb{R}^n$，有</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n}|x_iy_i| \le \|x\|_p \|y\|_q</script><p>当 $p=q=2$ 的时候，上面的式子就退化成 Cauchy-Schwarz 不等式。</p>
<p><strong>证明</strong>：可以取 $u(t)=t^{p-1},t\ge0$，反函数就有 $t=u^{\frac{1}{p-1}}=u^{q-1},u\ge0$。考虑可能有下面两种情况：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-holder.png" alt=""></p>
<p>以第一种情况为例，$\int_0^\alpha t^{p-1}dt + \int_0^\beta u^{q-1}du = \frac{\alpha^p}{p}+\frac{\beta^q}{q} \ge \alpha\beta$，第二种情况也有相同的结果。我们可以首先假设 $|x|_p = |y|_q=1$，因而就有</p>
<script type="math/tex; mode=display">
|x_iy_i| \le \frac{|x_i|^p}{p} + \frac{|y_i|^q}{q} \\
\Longrightarrow \sum |x_iy_i| \le 1 = \|x\|_p \|y\|_q</script><p>如果 $|x|_p \ne 1$，那么我们可以取 $x’ = x/|x|_p$，代入上面的情况就能得到 Hölder 不等式了。证毕。</p>
<p><strong>Minkowski 不等式</strong>：$\forall x,y\in\ell^p, p\ge1$，都有</p>
<script type="math/tex; mode=display">
\|x+y\|_p \le \|x\|_p + \|y\|_p .</script><p><strong>证明</strong>：</p>
<p>证毕。</p>
</blockquote>
<p>对于空间 $\ell^p$ 还有如下有趣的<strong>性质</strong>：</p>
<ul>
<li>若 $1\le p\le q$，则 $\ell^p \subset \ell^q$</li>
<li>$\lim_{p\downarrow 1} |x|_p = |x|_1$</li>
<li>$\lim_{p\uparrow 1} |x|_p = |x|_\infty$</li>
</ul>
<p><strong><em>例子 5(无穷维向量)</em></strong>：定义 $S=\{(x_n)_{n\ge1}, x_n\in\mathbb{K}\}$，定义如下度量</p>
<script type="math/tex; mode=display">
d(x,y) = \sum_{n=1}^\infty\frac{1}{2^n}\frac{|x_n-y_n|}{1+|x_n-y_n|} < \infty</script><p>则 $(S,d)$ 是度量空间。</p>
<p><strong><em>例子 6(连续函数)</em></strong>：$X=C[a,b],\ d_p(x,y) = \left(\int_a^b |x(t)-y(t)|^pdt\right)^{1/p},\ 1\le p\le\infty$，则 $(X,d)$ 是度量空间。</p>
<h2 id="2-开集"><a href="#2-开集" class="headerlink" title="2. 开集"></a>2. 开集</h2><p>上面一小节分析了度量空间 $(X,d)$ 中度量 $d$ 的定义，这一部分研究一下集合 $X$ 的性质。我们主要从<strong>开集和闭集</strong>这个角度来分析集合的结构。</p>
<p>需要注意的是在实空间 $\mathbb{R}$ 当中我们对开集（开区间）都比较熟悉了，但是还有很多其他很复杂的集合，比如上面提到的连续函数构成的集合 $C[a,b]$，这种情况下什么是<strong>开集</strong>呢？这个时候我们就需要抽象出“开集”这个概念最为本质的性质了。</p>
<h3 id="2-1-开集的定义"><a href="#2-1-开集的定义" class="headerlink" title="2.1 开集的定义"></a>2.1 开集的定义</h3><p><strong>定义</strong>：开球 $B(x_0,\delta)=\{x\in X,\ d(x,x_0)&lt;\delta\}$，闭球 $\bar{B}(x_0,\delta)=\{x\in X,\ d(x,x_0)\le\delta\}$，球面 $S(x_0,\delta)=\{x\in X,\ d(x,x_0)=\delta\}.$</p>
<p><strong>定义</strong>：$(X,d),M\subset X$，称 $x_0\in M$ 为 $M$ 的<strong>内点</strong>，若 $\exists \delta&gt;0, B(x_0,\delta)\subset M$，$M$ 的所有内点的集合称为 $M$ 的内部，记为 $\mathring{M}$。</p>
<p><strong>定义</strong>：$M$ 为<strong>开集</strong> $\iff M=\mathring{M} \iff \forall x\in M,\ \exists \delta&gt;0,\ B(x,\delta)\subset M$。</p>
<p><strong>定义</strong>：$F\subset X$ 为<strong>闭集</strong> $\iff F^c=X\setminus F$ 为开集。</p>
<p><strong>定义</strong>：<strong>闭包</strong> $\bar{M}=\{x\in X,\ \forall \delta&gt;0,\ B(x,\delta)\cap M\ne \varnothing \}.$</p>
<p><strong>注 1</strong>：有的集合可能既不是开集，也不是闭集！比如实空间 $\mathbb{R}$ 中区间 $[0,1)$。</p>
<p><strong>注 2</strong>：但是假如现在考虑的不是实空间，而是 $X=[0,+\infty)$，那么 $[0,1)$ 就是开集！判断是否是开集还是要根据定义！</p>
<p><strong><em>例子 1(离散度量空间)</em></strong>：$(X,d)$ 为离散度量空间，那么任意的 $M\subset X$ 都是既开又闭的集合。因为我们可以取 $\forall x\in M,\ B(x,1/2)=\{x\}\subset M$。</p>
<h3 id="2-2-开集的性质"><a href="#2-2-开集的性质" class="headerlink" title="2.2 开集的性质"></a>2.2 开集的性质</h3><p><strong>命题</strong>：$\mathring{M}$ 为包含在 $M$ 中的最大开集。</p>
<p><strong>证明</strong>：分为三个过程：1）$\mathring{M}\subset M$；<strong>2）$\mathring{M}$ 为开集；</strong>3）$\mathring{M}$ 最大。注意不要忘了第 2) 部分，细节略。</p>
<p><strong>定理</strong>：$(X,d)$，则</p>
<ul>
<li>$X,\varnothing$ 为开集</li>
<li>$(G_i)_{i\in I}$ 为一族开集，则 $\bigcup_{i\in I}G_i$ 为开集（<strong>开集无限并</strong>仍然是开集）；</li>
<li>$G_1,…,G_n$ 为开集，则 $\bigcap_{i=1}^n G_i$ 为开集（<strong>开集有限交</strong>仍然是开集）；</li>
</ul>
<p>证明：略。</p>
<p><strong>定理</strong>：$(X,d)$，则</p>
<ul>
<li>$X,\varnothing$ 为闭集（注意 $X,\varnothing$ 既开又闭）</li>
<li>$(F_i)_{i\in I}$ 为一族开集，则 $\bigcap_{i\in I}F_i$ 为开集（<strong>闭集无限交</strong>仍然是闭集）；</li>
<li>$F_1,…,F_n$ 为开集，则 $\bigcup_{i=1}^n F_i$ 为开集（<strong>闭集有限并</strong>仍然是闭集）；</li>
</ul>
<p>证明：略。</p>
<p><strong>性质</strong>：闭包 $\bar{M}$ 为闭集，且 $\bar{M}$ 为包含 $M$ 的最小闭集。</p>
<p><strong>推论</strong>：$(X,d), M\subset X$，$M$ 为闭集 $\iff M=\bar{M}.$</p>
<p>证明：略。</p>
<blockquote>
<p><strong>拓扑空间</strong></p>
<p>拓扑的定义是：给集合 $X$ 指定拓扑，就是指定集合 $X$ 中哪些子集是开集，指定的方式需要满足：</p>
<ol>
<li>$R,\varnothing$ 是开集；</li>
<li>开集的有限交仍然是开集；</li>
<li>开集的任意并仍然是开集。</li>
</ol>
<p>$X$ 上的<strong>拓扑</strong> $\mathcal{T}$ 是 $X$ 的子集族，满足上述的条件。定义了拓扑 $\mathcal{T}$ 的集合 $X$ 称为<strong>拓扑空间</strong>。对于拓扑空间 $(X,\mathcal{T})$ 有子集 $\mathcal{O}$，若 $\mathcal{O}\in \mathcal{T}$，则称 $\mathcal{O}$ 为开集。</p>
<p><strong>注1</strong>：先有拓扑 $\mathcal{T}$，然后如果 $X$ 的子集 $\mathcal{O}\in \mathcal{T}$，才有 $\mathcal{O}$ 是开集的说法。</p>
<p><strong>注2</strong>：拓扑空间跟度量空间类似，首先定义了一种运算，比如度量空间是需要定义度量，拓扑空间是需要定义对开集封闭的运算（有限交、任意并），其中的元素对这些运算封闭，然后才有空间的概念.</p>
</blockquote>
<h3 id="2-3-稠密与可分"><a href="#2-3-稠密与可分" class="headerlink" title="2.3 稠密与可分"></a>2.3 稠密与可分</h3><p><strong>定义</strong>：称 $M\subset X$ 是 $X$ 的<strong>稠密子集</strong>，若 $\bar{M}=X$。换一种表述方式，也就是说 $\forall x\in X,\forall \delta &gt; 0, B(x,\delta)\cap M\ne \varnothing$。</p>
<p><strong><em>例子 1</em></strong>：$(\mathbb{R},d)$，其中 $d(x,y)=|x-y|$，则 $\bar{\mathbb{Q}}=\mathbb{R}.$</p>
<p><strong><em>例子 2</em></strong>：$(\mathbb{C},d)$，其中 $d(x,y)=|x-y|$，则 $\overline{\mathbb{Q}+i\mathbb{Q}}=\mathbb{C}.$</p>
<p><strong><em>例子 3</em></strong>：$(\mathbb{R}^n,d_2)$，则 $\overline{\mathbb{Q}^n}=\mathbb{R}^n.$</p>
<p><strong><em>例子 4</em></strong>：对于 $1\le p\le \infty$，定义 $M=\{(x_n)_{n\ge1},\ x_n\in\mathbb{Q},\exists N,\forall n\ge N, x_n=0 \}$，那么对于度量 $d_p$，有 $\bar{M}=\ell^p.$</p>
<p><strong>性质</strong>：对于两个度量 $d_1,d_2$，如果存在 $c_1,c_2&gt;0$，对 $\forall x,y\in X$，都有 $c_1d_1(x,y)\le d_2(x,y)\le c_2 d_1(x,y)$，也即这两个度量相互控制，那么对任意 $M\subset X$，有 $M$ 的<strong>闭包相同</strong>，<strong>内部也相同</strong>。</p>
<p><strong><em>例子 5</em></strong>：对于离散度量空间，$X$ 的稠密子集只有 $X$ 本身。</p>
<p><strong><em>例子 6</em></strong>：$X=C[a,b]$，$d_\infty(x,y)=\max_{t\in[a,b]} |x(t)-y(t)|$，因此 $M=\{多项式 p(t)=a_0+a_1t+\cdots+a_Nt^N,a_i\in\mathbb{Q}\}$，则 $\bar{M}=X.$</p>
<p><strong>定义</strong>：称度量空间 $(X,d)$ 是<strong>可分</strong>的，若 $\exists M$ 为至多可数集（有限集或者可数集），并且 $\bar{M}=X.$</p>
<p>这里先介绍几个关于<strong>可数集</strong>的性质：</p>
<ul>
<li>若 $X_1,…,X_n$ 可数，则 $X_1\times \cdots \times X_n$ 可数</li>
<li><strong>可数个</strong>可数集的<strong>并集</strong>仍然是可数集</li>
</ul>
<p><strong><em>例子 1</em></strong>：$\{0,1\}^{\mathbb{N}}$ <strong>不是可数集</strong>！其中 $\mathbb{N}$ 为自然数集。</p>
<p><strong><em>例子 2</em></strong>：$(\mathbb{K}^n, d_p)$ 可分。</p>
<p><strong><em>例子 3</em></strong>：$(\ell^p,d_p)$ 可分，$M$ 与上面例子 4 的定义相同。</p>
<p><strong><em>例子 4</em></strong>：$(C[a,b],d_\infty)$ 可分，$M$ 与上面例子 6 的定义相同。</p>
<p><strong><em>例子 5</em></strong>：$(\ell^\infty,d_\infty)$ <strong>不可分</strong>。</p>
<p><em>证明</em>：反证法。参考课本 P14，略。</p>
<h2 id="3-收敛性与完备性"><a href="#3-收敛性与完备性" class="headerlink" title="3. 收敛性与完备性"></a>3. 收敛性与完备性</h2><p>这一部分则开始考虑 $X$ 中的元素序列，以及序列的极限是否存在、极限是什么的问题。之所以考虑序列这件事情，是因为我们实际中处理问题的时候往往是用序列去逼近一个元素，序列当中的每个元素可能是简单的，二最后去逼近的这个元素往往是不太显然或者比较复杂的东西。这样我们只需要证明序列中的元素满足某些性质，就能证明最后的极限具有某些特殊性质，更容易处理。</p>
<h3 id="3-1-序列收敛性"><a href="#3-1-序列收敛性" class="headerlink" title="3.1 序列收敛性"></a>3.1 序列收敛性</h3><p>我们对<strong>序列收敛性</strong>的定义是对于 $x_n,x\in\mathbb{R}$，称 $x_n$ 收敛到 $x$，记为 $x_n\to x$ ($\lim_{n\to\infty} x_n=x$)，若 $\forall \varepsilon&gt; 0,\exists N,\ \forall n\ge N$，都有 $|x_n-x|&lt;\varepsilon$。换一种表述方式就是在度量空间 $(X,d)$ 中，$d(x_n,x)\to 0$。</p>
<p><strong>注</strong>：需要注意的是只有 $x\in X$，我们才能说 $x_n\to x$。例如取 $X=(0,1),x_n=\frac{1}{n+1}$，那么 $x_n$ 在 $X$ 中<strong>不收敛</strong>。</p>
<p><strong>命题</strong>：若 $x_n\to x$，则 $\{x_n\}_{n\ge1}$ 为有界集合，且 $x$ 唯一。</p>
<p><strong>定理</strong>：度量空间 $(X,d)$，有 $M\subset X$，那么</p>
<ul>
<li>$x\in \bar{M} \iff \exists x_n\in M,\ x_n\to x$；</li>
<li>$M$ 为<strong>闭集</strong> $\iff \forall x_n\in M$，设 $x_n\to x\in X$，则 $x\in M$（即<strong>闭集对极限封闭</strong>）。</li>
</ul>
<p>证明：第一条应用定义，第二条应用第一条的结论。细节略。</p>
<h3 id="3-2-柯西列与完备性"><a href="#3-2-柯西列与完备性" class="headerlink" title="3.2 柯西列与完备性"></a>3.2 柯西列与完备性</h3><p><strong>定义</strong>：$x_n\in X$，称 $x_n$ 为<strong>柯西列</strong>，若 $\forall \varepsilon &gt; 0,\exists N, \forall m,n\ge N$，则 $d(x_m,x_n)&lt;\varepsilon$。称 $X$ 是<strong>完备</strong>的，若 $\forall x_n\in X$ 为柯西列，则 $\exists x\in X,x_n \to x$。</p>
<p><strong>注</strong>：实际上，收敛列一定是柯西列，即 $\{收敛列\} \subset \{柯西列\}$；而如果 $X$ 又是完备的，那么说明柯西列也一定是收敛列。因此 $X$ 完备 $\iff \{柯西列\}=\{收敛列\}$。</p>
<p><strong>命题</strong>：$x_n$ 为柯西列，则 $\{x_n,n\ge 1\}$ 为有界集。</p>
<p><strong><em>例子 1</em></strong>：$(\mathbb{R},d_1)$ 完备；$(\mathbb{K}^n, d_p)$ 完备；$(\ell^\infty, d_\infty)$ 完备；$(\ell^p,d_p)$ 完备；$(C[a,b],d_\infty)$ 完备。</p>
<p><strong><em>证明</em></strong>：证明完备性的套路：</p>
<ol>
<li>任取柯西列 $x_n\in X$；</li>
<li>找到可能的极限 $x$；</li>
<li>证明 $x\in X$；</li>
<li>证明 $x_n\to x$。</li>
</ol>
<p><strong><em>例子 2</em></strong>：$(C[a,b], d_p)$ <strong>不完备</strong>（反例如下图所示）；$\mathbb{Q}$ 不完备（因为不是闭集）；$c_{00}$ （有限个元素不为零的序列）不完备。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/2-example.jpg" alt=""></p>
<p> <strong>定理</strong>：度量空间 $(X,d)$</p>
<ul>
<li>$\forall Y\subset X$ 为完备的，则 $Y$ 为闭集；</li>
<li>若 $(X,d)$ 完备，则 $\forall Y\subset X$ 完备 $\iff Y$ 为闭集。</li>
</ul>
<p>证明：第一条应用闭集对极限封闭的性质；第二条反向应用 $(X,d)$ 完备的性质。细节略。</p>
<p><strong>定理</strong>：若 $x_n\in C[a,b],x_n\rightrightarrows x$（一致收敛），则 $x\in C[a,b]$。</p>
<h2 id="4-映射与连续性"><a href="#4-映射与连续性" class="headerlink" title="4. 映射与连续性"></a>4. 映射与连续性</h2><p>前面从单个度量空间的角度来考虑元素的性质，现在考虑两个度量空间的对应关系，也就是映射。</p>
<p>对于实空间的映射 $f:(a,b)\to \mathbb{R}$，我们对连续性的定义为：$f$ 在 $t_0\in(a,b)$ 处连续，若 $\lim_{t\to t_0}f(t)=f(t_0)$。由于我们在度量空间中已经定义了距离 ，因此可以将其推广至度量空间。</p>
<p>假设有度量空间 $(X_1,d_1)$ 和 $(X_2,d_2)$，映射 $T:X_1 \to X_2$。</p>
<p><strong>定义</strong>：称映射 $T$ 在 $t=t_0$ 处<strong>连续</strong>，若 $\forall \varepsilon &gt; 0,\ \exists \delta &gt; 0$，使得 $\forall t\in X_1,\ d_1(t,t_0)&lt;\delta$，都有 $d_2(Tt, Tt_0)&lt;\varepsilon$。若 $T$ 在 $\forall t\in X_1$ 处都连续，则称 $T$ 为<strong>连续映射</strong>。</p>
<p><strong><em>例子 1</em></strong>：若 $(X_1,d_1)$ 为离散度量空间，那么任意 $T:X_1 \to X_2$ 一定是连续映射。证明只需要套用定义，取 $\delta=1/2$ 即可。</p>
<p><strong><em>例子 2</em></strong>(Lipschitz 连续)：$\exists c&gt;0,\ \forall s,t\in X_1$ 都有 $d_2(Ts,Tt)&lt; c d_1(s,t)$，那么 $T$ 是连续映射。</p>
<p><strong>定理</strong>：$T$ 为<strong>连续映射</strong> $\iff \forall G\subset X_2$ 为开集，那么 $T^{-1}(G)=\{x\in X_1, Tx\in G\}$ 是 $X_1$ 中的开集。</p>
<p><strong>证明</strong>：”$\Longrightarrow$”：若已知 $T$ 连续，$G$ 为开集</p>
<p>$G$ 为开集，就有 $\forall x_0\in T^{-1}(G),\exists \varepsilon &gt; 0,\ B(Tx_0,\varepsilon)\subset G$</p>
<p>由于 $T$ 连续，则一定 $\exists \delta &gt; 0$，使得 $\forall x\in B(x_0, \delta)$，都有 $Tx\in B(Tx_0,\varepsilon)$，因而 $B(x_0,\varepsilon)\subset X_1$</p>
<p>故 $T^{-1}(G)$ 为开集。</p>
<p>“$\Longleftarrow$”：假设 $\forall G\subset X_2$ 为开集，$T^{-1}(G)$ 在 $X_1$ 中也是开集，那么套用连续映射的定义，就能证明 $T$ 为连续映射。</p>
<p>证毕。</p>
<p><strong>推论</strong>：$T$ 为<strong>连续映射</strong> $\iff \forall F\subset X_2$ 为闭集，那么 $T^{-1}(F)=\{x\in X_1, Tx\in F\}$ 是 $X_1$ 中的闭集。</p>
<p><strong>定理</strong>：$T$ 在 $x_0$ 处连续 $\iff \forall x_n\in X_1, x_n\to x_0$，则 $Tx_n \to Tx_0$。</p>
<p><strong>证明</strong>：”$\Longrightarrow$”：应用定义；</p>
<p>“$\Longleftarrow$”：反证法，假设 $T$ 在 $x_0$ 处不连续，</p>
<p>那么 $\exists \varepsilon_0 &gt; 0,\forall \delta &gt;0, \exists x\in B(x_0, \delta)$，使得 $d(Tx_0, Tx) &gt; \varepsilon_0$</p>
<p>可以取 $\delta = 1/n$，由此构造出一个序列 $x_n \in B(x_0,1/n)$，</p>
<p>可以知道 $x_n\to x_0$，但是却有 $d(x_n,x_0) \ge \varepsilon_0$，与假设矛盾。</p>
<p>证毕。</p>
<p><strong>推论</strong>：$T:X_1\to X_2$ 处处连续 $\iff \forall x_n\to x, Tx_n \to Tx$。</p>
<h2 id="5-Banach-不动点定理"><a href="#5-Banach-不动点定理" class="headerlink" title="5. Banach 不动点定理"></a>5. Banach 不动点定理</h2><p>不动点想必大家在别的地方都或多或少听说过或者用过，应该是解决很多问题的重要工具。在这一部分的内容里面则可以看到，前面讲的序列收敛性、映射在不动点定理当中的应用。</p>
<p><strong>定义</strong>：考虑 $X\ne\varnothing, T:X\to X$，若 $x_0\in X,Tx_0=x_0$，则称 $x_0$ 为 $T$ 的<strong>不动点</strong>。</p>
<p><strong>定义</strong>：$T:X\to X$，假设 $\exists 0\le \alpha &lt; 1$，使得 $\forall x,y\in X$，都有 $d(Tx,Ty) \le \alpha d(x,y)$，则称 $T$ 为<strong>压缩映射</strong>。</p>
<p><strong>定理(Banach不动点定理)</strong>：假设 $(X,d)$ 为<strong>非空、完备</strong>度量空间，$T:X\to X$ 为压缩映射，则 $T$ <strong>存在唯一</strong>的不动点。</p>
<p><strong>证明</strong>：考虑 $x_0\in X,x_1=Tx_0,\cdots,x_n=Tx_{n-1},\cdots$，那么可以首先证明 $x_n$ 为柯西列，进而存在收敛值 $x$。 由于 $d(x_n,x_{n+1})=d(x_n,Tx_n)\to 0$，从而趋向于 $x=Tx$。之后再证明唯一性。证毕。</p>
<p><strong>定理</strong>：假设 $(X,d)$ <strong>非空完备</strong>，$T:X\to X$，设 $\exists m\ge1$，$T^m$ 为压缩映射，则 $\exists! x\in X$ 使得 $Tx=x$。</p>
<p><strong>证明</strong>：只需要证明 $S=T^m$ 的不动点都是 $T$ 的不动点，反之 $T$ 的不动点也都是 $S$ 的不动点即可。</p>
<p>由不动点定理可知，$S$ 存在唯一一个不动点，记为 $y_0$，即 $Sy_0=y_0$，那么 $STy_0=T^{m+1}y_0=TSy_0=Ty_0$，即 $Ty_0$ 也是 $S$ 的不动点，因此一定有 $Ty_0=y_0$，即 $y_0$ 也是 $T$ 的不动点。假设 $z_0$ 是 $T$ 的不动点，那么很容易证明他也是 $S$ 的不动点。因此 $T$ 存在唯一不动点。证毕。</p>
<p><strong><em>例子 1</em></strong>：$c&gt;0$，求 $\sqrt{c}$ 的数值解。可以用数值迭代，取 $f(x)=(x+\frac{c}{x})/2,D=[\sqrt{c},+\infty)$，求不动点即可。</p>
<p><strong><em>例子 2</em></strong>：$(X=\mathbb{K}^n,d_\infty),C\in\mathbb{K}^{n\times n},b\in\mathbb{K}^n$，映射 $Tx=Cx+b$。容易证明若 $\forall i,\sum_j|a_{ij}|&lt;1$，则 $T$ 为压缩映射。</p>
<p><strong><em>例子 3</em></strong>：考虑 $(t_0,x_0)\in \mathbb{R}^2,a,b&gt;0$，考虑矩形 $R=[t_0-a,t_0+a]\times[x_0-b,x_0+b]$，连续函数 $f:R\to\mathbb{R}$，假设存在 $k\ge0,|f(t,u)-f(t,v)|\le k|u-v|,\forall (t,u),(t,v)\in R$。考虑初值问题</p>
<script type="math/tex; mode=display">
(P):\begin{cases}
x'(t)=f(t,x(t)) \\
x(t_0)=x_0
\end{cases}</script><p>求上述初值问题的解 $x(t)\in C[t_0-\beta,t_0+\beta],0&lt;\beta\le a$。</p>
<p><strong><em>解</em></strong>：首先给出结论：如果给定 $c=\max_{(t,x)\in R}|f(t,x)|,0&lt;\beta&lt;\min\{a,\frac{b}{c},\frac{1}{k}\}$，那么存在唯一的 $x\in C^1[t_0-\beta,t_0+\beta]$，使得当 $t\in[t_0-\beta,t_0+\beta]$ 时，有 $x(t)\in[x_0-b,x_0+b]$ 且 $x$ 满足方程 $(P)$。下面给出证明。</p>
<p>由于 $x’(t)=f(t,x(t)) \Longrightarrow \int_{t_0}^t x’(\tau)d\tau=\int_{t_0}^t f(\tau,x(\tau))d\tau \Longrightarrow  x(t)=x_0+\int_{t_0}^t f(\tau,x(\tau))d\tau$，</p>
<p>可以证明 $|x(t)-x_0|\le c\beta &lt;b \Longrightarrow (t,x(t))\in R$。</p>
<p>$(X,d_\infty)$ 完备，取 $M=\bar{B}(x_0,c\beta)$ 为闭集，因此 $M$ 为完备的，</p>
<p>取 $Tx=x_0+\int_{t_0}^t f(\tau,x(\tau))d\tau$，也可以证明 $d_\infty(Tx,x_0)\le c\beta \Longrightarrow Tx\in M$，即 $T:M\to M$，</p>
<p>又容易证明 $T$ 为压缩映射，因而存在唯一的 $x\in M$ 使得 $Tx=x$，只需要迭代即可获得 $x(t)$。</p>
<p><strong><em>例子 4(隐函数存在定理)</em></strong>：略。</p>
<h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><p>这一章当中讲解了度量空间、开集闭集、序列收敛性、柯西列、集合完备性、映射与连续性，以及最后的Banach不动点定理。现在我们已经完成了度量空间中的内容，你能够回答文章开头的问题了吗？对于度量空间这个概念有什么新的理解吗？</p>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
      <tags>
        <tag>度量空间</tag>
        <tag>完备空间</tag>
        <tag>开集</tag>
        <tag>可分性</tag>
        <tag>柯西列</tag>
        <tag>连续映射</tag>
        <tag>Banach不动点定理</tag>
      </tags>
  </entry>
  <entry>
    <title>泛函分析笔记0：绪论</title>
    <url>/2020/10/18/functional-analysis/ch0-intro/</url>
    <content><![CDATA[<p>本系列泛函分析课程笔记的参考教材是清华大学出版社，步尚全老师写的《泛函分析基础》。</p>
<p>笔记的内容基本来自于我的听课记录，以及自己复习过程中的一些想法，由于笔者的数学功底极其不扎实，因此很可能会有不准确甚至错误之处，欢迎大家指正。</p>
]]></content>
      <categories>
        <category>Functional Analysis</category>
      </categories>
  </entry>
  <entry>
    <title>VMWare15 + Ubuntu18.04 踩坑记录</title>
    <url>/2020/08/11/software/vmware/</url>
    <content><![CDATA[<h2 id="1-虚拟机联网问题"><a href="#1-虚拟机联网问题" class="headerlink" title="1. 虚拟机联网问题"></a>1. 虚拟机联网问题</h2><p>VMWare虚拟机大致有 3 种联网方式，以下三种方式联网自由度逐渐递减：</p>
<ol>
<li>桥接：虚拟机就相当于局域网中的另一台主机，有独立的 ip 地址；</li>
<li>NAT：虚拟机需要借助于主机才能联网，虚拟机也是以物理主机的身份与外界通信；</li>
<li>Host-only：虚拟机只能与主机通信，不能联网；</li>
</ol>
<p>有时候设置好 NAT 或者桥接模式后仍然不能联网，可以尝试输入以下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo service network-manager restart</span><br></pre></td></tr></table></figure>
<p>也可以尝试先把虚拟机关机，点击 VMWare <code>编辑 &gt;&gt; 更改设置 &gt;&gt;还原默认设置 &gt;&gt; 确定</code>，然后虚拟机开机就可以了。</p>
<p>找不到网络连接图标</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo service network-manager stop</span><br><span class="line">sudo rm /var/lib/NetworkManager/NetworkManager.state</span><br><span class="line">sudo service network-manager start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将文件里面唯一的<span class="literal">false</span>改成<span class="literal">true</span></span></span><br><span class="line">sudo gedit /etc/NetworkManager/NetworkManager.conf</span><br><span class="line">sudo service network-manager restart</span><br></pre></td></tr></table></figure>
<h2 id="2-vi-输入问题"><a href="#2-vi-输入问题" class="headerlink" title="2. vi 输入问题"></a>2. vi 输入问题</h2><p>vi 输入模式下方向键会出来 A, B, C, D，而且退格键不好使。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法 1"></a>方法 1</h3><p>可以修改文件 <code>/etc/vim/vimrc.tiny</code>，注释掉原来的 <code>set compatible</code>，改成以下内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> nocompatible</span><br><span class="line"><span class="built_in">set</span> backspace=2</span><br></pre></td></tr></table></figure>
<h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法 2"></a>方法 2</h3><p>在用户个人目录下创建文件 <code>.vimrc</code>，写入以下内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> nocompatible     <span class="comment"># 以非兼容模式工作  </span></span><br><span class="line"><span class="built_in">set</span> backspace=2</span><br></pre></td></tr></table></figure>
<h3 id="方法-3"><a href="#方法-3" class="headerlink" title="方法 3"></a>方法 3</h3><p>安装 vim</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>树莓派设置开机自启动程序</title>
    <url>/2020/07/20/hardware/raspberrypi-autostart/</url>
    <content><![CDATA[<p>最近调试树莓派，希望<strong>开机运行两个程序</strong>，其中一个是<strong>可执行文件</strong>，另一个是 <strong>python 脚本</strong>，他们都是无限循环的程序，也就是说不关机不会停止运行。中间还是遇到了很多 bug，现在记录一下自启动程序的设置方法以及debug的整个过程。</p>
<h2 id="1-自启动程序设置方法"><a href="#1-自启动程序设置方法" class="headerlink" title="1. 自启动程序设置方法"></a>1. 自启动程序设置方法</h2><p>网上用的最多的方法就是修改 <code>/etc/rc.local</code> 文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo nano /etc/rc.local</span><br></pre></td></tr></table></figure>
<p>进入之后在 <code>exit 0</code> 这句话上面添加需要运行的程序。比如我想运行 <code>~/test/</code> 文件夹下的可执行文件 <code>runme</code> 和 python 脚本 <code>runhe.py</code>，那么就需要添加下面两个命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sleep 5</span></span><br><span class="line">/home/pi/test/runme &amp;</span><br><span class="line">sudo -H -u pi /usr/bin/python3 /home/pi/test/runhe.py &amp;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>这里有几个<strong>注意事项</strong>：</p>
<ol>
<li>最好都使用<strong>绝对路径</strong>！</li>
<li>如果程序是无限循环（不会终止）的，那么需要在行尾添加 <code>&amp;</code>，如果不是的话可以不加这个 <code>&amp;</code>，这个符号可以理解为允许当前行的程序在后台运行，这样就可以继续启动下一行的程序了。</li>
<li>第一行的 <code>sleep 5</code> 表示先暂停 5s，主要是为了防止有的变量或者环境还没有准备好，可以根据情况决定是否添加。</li>
<li>运行 python 脚本的时候最好前面用 <code>/usr/bin/python3 xxx.py</code> 而不是直接 <code>python3 xxx.py</code>，后者不一定会报错，但是像前面说的，还是尽量用绝对路径。</li>
<li>最重要的一点，也是我的 bug 原因所在：运行 python 脚本的时候，网上大多数教程说的是添加 <code>python3 xxx.py</code> 就行了，但是我 debug 过程中发现必须要使用 <code>sudo -H -u pi /xxx/python3 xxxx.py</code> 来显式的指定用户，否则可能会报错 <code>ModuleNotFoundError: No module named &#39;XXX&#39;</code>，这应该是因为某些包只在某个用户环境中安装了。</li>
</ol>
<h2 id="2-debug技巧"><a href="#2-debug技巧" class="headerlink" title="2. debug技巧"></a>2. debug技巧</h2><p>按照上面的方法修改 <code>rc.local</code> 文件还是有可能失败，这里再记录几个 debug 的方法。</p>
<p>首先是可以利用下面的命令查看是否运行了含有 <code>runme</code> 的程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ps aux|grep runme</span><br></pre></td></tr></table></figure>
<p>另外可以将 <code>rc.local</code> 的第一行 <code>#!/bin/sh</code> 修改为</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/sh -e(或者 -x)</span></span><br></pre></td></tr></table></figure>
<p>这样可以把日志记录到 <code>/var/log/messages</code> 文件中，后续可以查看这个文件看看是哪里报错。</p>
<p>然后是每次修改 <code>rc.local</code> 之后都要重启来检验有没有问题，太麻烦了，其实还有更高效的方法，只需要在命令行运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart rc-local</span><br><span class="line">systemctl status rc-local</span><br></pre></td></tr></table></figure>
<p>前者模仿开机过程，重新执行一遍 <code>rc.local</code> 中的命令，后者查看运行状态。</p>
<p>此外，还可以使用 <code>nohup</code> 命令使程序后台运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup ./calcDynamic &amp;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>raspberry pi</tag>
      </tags>
  </entry>
  <entry>
    <title>JY901串口数据接收与处理(Python)</title>
    <url>/2020/07/09/hardware/jy901/</url>
    <content><![CDATA[<p>最近在用JY901做一些实验，关于JY901网上有很多资料了，也有上位机软件，可以方便的查看输出数据。我想做的是对输出的角速度进行积分，对比积分后的结果与输出的角度，如果数据都比较准确地话，那么他们应该相差不大。</p>
<p>这篇文章里，要完成的事情就是通过串口接收他输出的角速度和角度，然后对角速度进行积分，并实时显示数据结果。下面我首先对各个部分进行分块解释，完整的代码放在最后。</p>
<a id="more"></a>
<h2 id="1-串口通信"><a href="#1-串口通信" class="headerlink" title="1. 串口通信"></a>1. 串口通信</h2><p>python实现串口通信可以用 <code>pySerial</code> 库。我们首先选择串口对应的设备端口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取串口设备对应的端口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_serial_port</span><span class="params">()</span>:</span></span><br><span class="line">    ports = list(serial.tools.list_ports.comports())</span><br><span class="line">    <span class="keyword">for</span> port <span class="keyword">in</span> ports:</span><br><span class="line">        print(ports.index(port), port)</span><br><span class="line">    selected = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> selected &lt; <span class="number">0</span> <span class="keyword">or</span> selected &gt;= len(ports):</span><br><span class="line">        print(<span class="string">"please input serial to use [start from 0]:"</span>)</span><br><span class="line">        selected = int(input())</span><br><span class="line">        print(<span class="string">"selected: "</span>, selected)</span><br><span class="line"></span><br><span class="line">    port = ports[selected]</span><br><span class="line">    print(<span class="string">"use "</span>, port)</span><br><span class="line">    port = list(port)</span><br><span class="line">    <span class="keyword">return</span> port[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>然后打开串口并持续接收数据，在收到的数据中提取IMU数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serial_readers</span><span class="params">(self, port = None, size = <span class="number">22</span>)</span>:</span></span><br><span class="line">    <span class="comment"># port   : 串口端口</span></span><br><span class="line">    <span class="comment"># size   : 默认一次读取数据长度(bytes)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> port:</span><br><span class="line">        port = get_serial_port()</span><br><span class="line">    total = bytearray()</span><br><span class="line">    ser = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> ser:</span><br><span class="line">                ser = serial.Serial(port, <span class="number">115200</span>, timeout=<span class="number">60</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> ser.is_open:</span><br><span class="line">                ser.open()</span><br><span class="line">            tmp = ser.read(size)</span><br><span class="line">            total.extend(tmp)</span><br><span class="line">            total, ext_omega, ext_angle = self.extract_raw_data(total)   <span class="comment">#提取IMU数据</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">"exception "</span>, e)</span><br><span class="line">            <span class="keyword">if</span> ser:</span><br><span class="line">                ser.close()</span><br><span class="line">            time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.record_omega, self.record_angle</span><br></pre></td></tr></table></figure>
<h2 id="2-IMU数据提取"><a href="#2-IMU数据提取" class="headerlink" title="2. IMU数据提取"></a>2. IMU数据提取</h2><p>首先我们来看JY901串口数据的帧结构：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">加速度：0x55 0x51 AxL AxH AyL AyH AzL AzH TL TH SUM </span><br><span class="line">角速度：0x55 0x52 wxL wxH wyL wyH wzL wzH TL TH SUM </span><br><span class="line">角度：  0x55 0x53 RollL RollH PitchL PitchH YawL YawH TL TH SUM</span><br></pre></td></tr></table></figure>
<p>这三种数据帧长度都是 11 Bytes，最后一位为校验位，我们从串口数据中提取IMU数据也是根据这个帧结构，先检测帧头，然后读取整个帧（代码里偷了懒，并没有核对校验位）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_raw_data</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="comment"># 根据报文格式读取角速度数据</span></span><br><span class="line">    <span class="comment"># 0x55 0x52 + data + CRC(8bits)</span></span><br><span class="line">    length = <span class="number">9</span></span><br><span class="line">    ext_omega = <span class="literal">None</span></span><br><span class="line">    ext_angle = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        st = data.index(<span class="string">b'\x55'</span>)                <span class="comment"># 寻找报文引导字</span></span><br><span class="line">        <span class="keyword">if</span> st &gt;= <span class="number">0</span>:</span><br><span class="line">            reserved = data[st+<span class="number">1</span>]               <span class="comment"># 数据类型</span></span><br><span class="line">            <span class="keyword">if</span> reserved == <span class="number">82</span>:                  <span class="comment"># 0x52，提取角速度消息</span></span><br><span class="line">                <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                    <span class="keyword">if</span> self.count_omega==<span class="number">0</span>:</span><br><span class="line">                        self.time_now = self.time_last = time.time()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.time_last = self.time_now</span><br><span class="line">                        self.time_now = time.time()</span><br><span class="line"></span><br><span class="line">                    ext_omega = data[st:st+length+<span class="number">2</span>]</span><br><span class="line">                    temp = int.from_bytes(ext_omega[<span class="number">6</span>:<span class="number">8</span>], byteorder=<span class="string">'little'</span>, signed=<span class="literal">True</span>)</span><br><span class="line">                    self.omega = temp/<span class="number">32768</span>*<span class="number">2000</span></span><br><span class="line">                    self.record_omega.append(self.omega)   <span class="comment">#deg/s</span></span><br><span class="line">                    data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">                    self.count_omega += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                    self.omega_inte += self.omega*(self.time_now - self.time_last)</span><br><span class="line">                    self.omega_inte = (self.omega_inte+<span class="number">180</span>)%<span class="number">360</span> - <span class="number">180</span></span><br><span class="line">                    self.record_inte.append(self.omega_inte)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> reserved == <span class="number">83</span>:                <span class="comment"># 0x53，提取角度消息</span></span><br><span class="line">                <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                    ext_angle = data[st:st+length+<span class="number">2</span>]</span><br><span class="line">                    temp = int.from_bytes(ext_angle[<span class="number">6</span>:<span class="number">8</span>], byteorder=<span class="string">'little'</span>, signed=<span class="literal">True</span>)</span><br><span class="line">                    self.yaw = temp/<span class="number">32768</span>*<span class="number">180</span></span><br><span class="line">                    <span class="keyword">if</span> self.count_yaw==<span class="number">0</span>:                   <span class="comment">#记录初始相位</span></span><br><span class="line">                        self.yaw_init = self.yaw</span><br><span class="line">                    self.record_angle.append(self.yaw - self.yaw_init)      <span class="comment">#deg</span></span><br><span class="line">                    data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">                    self.count_yaw += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> reserved == <span class="number">81</span>:                <span class="comment"># 0x51, 丢弃加速度消息</span></span><br><span class="line">                <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                    ext_data = <span class="literal">None</span></span><br><span class="line">                    data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                data = data[st+<span class="number">2</span>:]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> data, ext_omega, ext_angle</span><br><span class="line">    <span class="keyword">if</span> self.count_yaw%<span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> self.count_yaw&gt;<span class="number">0</span>:</span><br><span class="line">        self.save_data()</span><br><span class="line">        print(len(self.record_omega),<span class="string">' extract omega  '</span>,ext_omega,self.omega_inte)</span><br><span class="line">        print(len(self.record_angle),<span class="string">' extract angle  '</span>,ext_angle,self.yaw - self.yaw_init)</span><br><span class="line">        print(<span class="string">'*'</span>*<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">return</span> data, ext_omega, ext_angle</span><br></pre></td></tr></table></figure>
<h2 id="3-绘图显示"><a href="#3-绘图显示" class="headerlink" title="3. 绘图显示"></a>3. 绘图显示</h2><p>我把历史接收数据都存在一个 <code>list</code> 里面了，每过 0.1s 就重新绘图，看起来就像是在实时更新一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self)</span>:</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> len(self.record_angle)&gt;<span class="number">0</span> <span class="keyword">and</span> len(self.record_inte)&gt;<span class="number">0</span> :</span><br><span class="line">                plt.clf()</span><br><span class="line">                plt.plot(list(range(len(self.record_angle))),self.record_angle,<span class="string">'r'</span>)</span><br><span class="line">                plt.plot(list(range(len(self.record_inte))),self.record_inte,<span class="string">'b'</span>)</span><br><span class="line">                plt.xlim(max(<span class="number">0</span>,self.count_yaw<span class="number">-3000</span>),self.count_yaw+<span class="number">200</span>)</span><br><span class="line">                plt.legend(labels=[<span class="string">'angle'</span>,<span class="string">'integration'</span>])</span><br><span class="line">                plt.pause(<span class="number">0.1</span>)</span><br><span class="line">                plt.ioff()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">"exception "</span>, e)</span><br></pre></td></tr></table></figure>
<h2 id="4-运行效果"><a href="#4-运行效果" class="headerlink" title="4. 运行效果"></a>4. 运行效果</h2><h2 id="5-完整代码"><a href="#5-完整代码" class="headerlink" title="5. 完整代码"></a>5. 完整代码</h2><p>为了便于共享数据，定义了一个类 <code>JY901</code>，将上面的各个函数放在这个类里面，输出的数据也都记录在 <code>JY901</code> 的成员变量里。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> serial</span><br><span class="line"><span class="keyword">import</span> serial.tools.list_ports</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JY901</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.time_last = <span class="number">0</span></span><br><span class="line">        self.time_now = <span class="number">0</span></span><br><span class="line">        self.record_angle = []</span><br><span class="line">        self.record_omega = []</span><br><span class="line">        self.record_inte = []</span><br><span class="line">        self.count_yaw = <span class="number">0</span></span><br><span class="line">        self.count_omega = <span class="number">0</span></span><br><span class="line">        self.omega_inte = <span class="number">0</span>         <span class="comment">#角速度积分</span></span><br><span class="line">        self.yaw_init = <span class="number">0</span></span><br><span class="line">        self.yaw = <span class="number">0</span></span><br><span class="line">        self.omega = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_raw_data</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># 根据报文格式读取角速度数据</span></span><br><span class="line">        <span class="comment"># 0x55 0x52 + data + CRC(8bits)</span></span><br><span class="line">        length = <span class="number">9</span></span><br><span class="line">        ext_omega = <span class="literal">None</span></span><br><span class="line">        ext_angle = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            st = data.index(<span class="string">b'\x55'</span>)                <span class="comment"># 寻找报文引导字</span></span><br><span class="line">            <span class="keyword">if</span> st &gt;= <span class="number">0</span>:</span><br><span class="line">                reserved = data[st+<span class="number">1</span>]               <span class="comment"># 数据类型</span></span><br><span class="line">                <span class="keyword">if</span> reserved == <span class="number">82</span>:                  <span class="comment"># 0x52，提取角速度消息</span></span><br><span class="line">                    <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                        <span class="keyword">if</span> self.count_omega==<span class="number">0</span>:</span><br><span class="line">                            self.time_now = self.time_last = time.time()</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            self.time_last = self.time_now</span><br><span class="line">                            self.time_now = time.time()</span><br><span class="line"></span><br><span class="line">                        ext_omega = data[st:st+length+<span class="number">2</span>]</span><br><span class="line">                        temp = int.from_bytes(ext_omega[<span class="number">6</span>:<span class="number">8</span>], byteorder=<span class="string">'little'</span>, signed=<span class="literal">True</span>)</span><br><span class="line">                        self.omega = temp/<span class="number">32768</span>*<span class="number">2000</span></span><br><span class="line">                        self.record_omega.append(self.omega)   <span class="comment">#deg/s</span></span><br><span class="line">                        data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">                        self.count_omega += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                        self.omega_inte += self.omega*(self.time_now - self.time_last)</span><br><span class="line">                        self.omega_inte = (self.omega_inte+<span class="number">180</span>)%<span class="number">360</span> - <span class="number">180</span></span><br><span class="line">                        self.record_inte.append(self.omega_inte)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> reserved == <span class="number">83</span>:                <span class="comment"># 0x53，提取角度消息</span></span><br><span class="line">                    <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                        ext_angle = data[st:st+length+<span class="number">2</span>]</span><br><span class="line">                        temp = int.from_bytes(ext_angle[<span class="number">6</span>:<span class="number">8</span>], byteorder=<span class="string">'little'</span>, signed=<span class="literal">True</span>)</span><br><span class="line">                        self.yaw = temp/<span class="number">32768</span>*<span class="number">180</span></span><br><span class="line">                        <span class="keyword">if</span> self.count_yaw==<span class="number">0</span>:                   <span class="comment">#记录初始相位</span></span><br><span class="line">                            self.yaw_init = self.yaw</span><br><span class="line">                        self.record_angle.append(self.yaw - self.yaw_init)      <span class="comment">#deg</span></span><br><span class="line">                        data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">                        self.count_yaw += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> reserved == <span class="number">81</span>:                <span class="comment"># 0x51, 丢弃加速度消息</span></span><br><span class="line">                    <span class="keyword">if</span> st+length+<span class="number">1</span> &lt; len(data):</span><br><span class="line">                        ext_data = <span class="literal">None</span></span><br><span class="line">                        data = data[st+length+<span class="number">2</span>:]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    data = data[st+<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> data, ext_omega, ext_angle</span><br><span class="line">        <span class="keyword">if</span> self.count_yaw%<span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> self.count_yaw&gt;<span class="number">0</span>:</span><br><span class="line">            self.save_data()</span><br><span class="line">            print(len(self.record_omega),<span class="string">' extract omega  '</span>,ext_omega,self.omega_inte)</span><br><span class="line">            print(len(self.record_angle),<span class="string">' extract angle  '</span>,ext_angle,self.yaw - self.yaw_init)</span><br><span class="line">            print(<span class="string">'*'</span>*<span class="number">30</span>)</span><br><span class="line">        <span class="keyword">return</span> data, ext_omega, ext_angle</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serial_readers</span><span class="params">(self, port = None, size = <span class="number">22</span>)</span>:</span></span><br><span class="line">        <span class="comment"># port   : 串口端口</span></span><br><span class="line">        <span class="comment"># size   : 默认一次读取数据长度(bytes)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> port:</span><br><span class="line">            port = get_serial_port()</span><br><span class="line">        total = bytearray()</span><br><span class="line">        ser = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ser:</span><br><span class="line">                    ser = serial.Serial(port, <span class="number">115200</span>, timeout=<span class="number">60</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ser.is_open:</span><br><span class="line">                    ser.open()</span><br><span class="line">                tmp = ser.read(size)</span><br><span class="line">                total.extend(tmp)</span><br><span class="line">                total, ext_omega, ext_angle = self.extract_raw_data(total)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(<span class="string">"exception "</span>, e)</span><br><span class="line">                <span class="keyword">if</span> ser:</span><br><span class="line">                    ser.close()</span><br><span class="line">                time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.record_omega, self.record_angle</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        np.savez(<span class="string">'record.npz'</span>,record_omega=self.record_omega, record_angle=self.record_angle)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self)</span>:</span></span><br><span class="line">        plt.ion()</span><br><span class="line">        plt.figure()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">if</span> len(self.record_angle)&gt;<span class="number">0</span> <span class="keyword">and</span> len(self.record_inte)&gt;<span class="number">0</span> :</span><br><span class="line">                    plt.clf()</span><br><span class="line">                    plt.plot(list(range(len(self.record_angle))),self.record_angle,<span class="string">'r'</span>)</span><br><span class="line">                    plt.plot(list(range(len(self.record_inte))),self.record_inte,<span class="string">'b'</span>)</span><br><span class="line">                    plt.xlim(max(<span class="number">0</span>,self.count_yaw<span class="number">-3000</span>),self.count_yaw+<span class="number">200</span>)</span><br><span class="line">                    plt.legend(labels=[<span class="string">'angle'</span>,<span class="string">'integration'</span>])</span><br><span class="line">                    plt.pause(<span class="number">0.1</span>)</span><br><span class="line">                    plt.ioff()</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(<span class="string">"exception "</span>, e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_serial_port</span><span class="params">()</span>:</span></span><br><span class="line">    ports = list(serial.tools.list_ports.comports())</span><br><span class="line">    <span class="keyword">for</span> port <span class="keyword">in</span> ports:</span><br><span class="line">        print(ports.index(port), port)</span><br><span class="line">    selected = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> selected &lt; <span class="number">0</span> <span class="keyword">or</span> selected &gt;= len(ports):</span><br><span class="line">        print(<span class="string">"please input serial to use [start from 0]:"</span>)</span><br><span class="line">        selected = int(input())</span><br><span class="line">        print(<span class="string">"selected: "</span>, selected)</span><br><span class="line"></span><br><span class="line">    port = ports[selected]</span><br><span class="line">    print(<span class="string">"use "</span>, port)</span><br><span class="line">    port = list(port)</span><br><span class="line">    <span class="keyword">return</span> port[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    imu = JY901()</span><br><span class="line">    thread_rcv = threading.Thread(target=imu.serial_readers)</span><br><span class="line">    thread_plt = threading.Thread(target=imu.plot)</span><br><span class="line">    thread_plt.start()</span><br><span class="line">    thread_rcv.start()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>IMU</tag>
      </tags>
  </entry>
  <entry>
    <title>Why Baby Why</title>
    <url>/2020/06/12/music/WhyBabyWhy/</url>
    <content><![CDATA[<p><center><h2>夏天到了🍉</h2></center><br><a id="more"></a></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=509531185&auto=1&height=66"></iframe>

<blockquote>
<p>What do you mean, god gave me two left feet?<br>Shooting me glares from the passenger seat<br>Come on, baby, we got all dressed up<br>Won’t you dance with me?</p>
<p>Why, baby, why do you give me trouble?<br>I’m trying to hide as not to be seen<br>But I know you mean well, step on my feet<br>Come dance with me</p>
<p>Come on, try to have a good time<br>There’s a punch bowl, but you brought red wine<br>Come on, baby, show me what’s a good time<br>Why, baby, why?</p>
<p>Why must you play me like I play the guitar?<br>She made me jump into the reservoir<br>And go running ‘til we fade to black<br>Like the movies do</p>
<p>Why, baby, why do you give me trouble?<br>I’m trying to hide as not to be seen<br>But I know you mean well, step on my feet<br>Come dance with me</p>
<p>Come on, try to have a good time<br>There’s a punch bowl, but you brought red wine<br>Come on, baby, show me what’s a good time<br>Why, baby, why?</p>
<p>Come on, try to have a good time<br>There’s a punch bowl, but you brought red wine<br>Come on, baby, show me what’s a good time<br>Why baby why?<br>(Oh-oh)<br>Come on, baby, show me what’s a good time<br>Why baby why?</p>
</blockquote>
]]></content>
      <categories>
        <category>Music</category>
      </categories>
  </entry>
  <entry>
    <title>树莓派使用</title>
    <url>/2020/06/08/hardware/raspberrypi/</url>
    <content><![CDATA[<h2 id="1-系统安装"><a href="#1-系统安装" class="headerlink" title="1. 系统安装"></a>1. 系统安装</h2><p><a href="https://zhuanlan.zhihu.com/p/59027897" target="_blank" rel="noopener">参考链接</a></p>
<h2 id="2-连接树莓派"><a href="#2-连接树莓派" class="headerlink" title="2. 连接树莓派"></a>2. 连接树莓派</h2><p>如果有显示器和键盘鼠标，可以直接利用HMDI线连接树莓派和显示器，像操作普通电脑一样。</p>
<p>如果没有显示器，就需要通过ssh连接。首先需要知道树莓派的地址。</p>
<a id="more"></a>
<h3 id="2-1-方法一：wifi"><a href="#2-1-方法一：wifi" class="headerlink" title="2.1 方法一：wifi"></a>2.1 方法一：wifi</h3><ol>
<li><p>在树莓派的 SD 卡里面 <code>boot</code> 分区下面创建文件 <code>wpa_supplicant.conf</code>，写入以下内容</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="attribute">country</span>=CN</span><br><span class="line"><span class="attribute">ctrl_interface</span>=DIR=/var/run/wpa_supplicant <span class="attribute">GROUP</span>=netdev</span><br><span class="line"><span class="attribute">update_config</span>=1</span><br><span class="line"> </span><br><span class="line">network=&#123;</span><br><span class="line">    <span class="attribute">ssid</span>=<span class="string">"wifi名"</span></span><br><span class="line">    <span class="attribute">psk</span>=<span class="string">"密码"</span></span><br><span class="line">    <span class="attribute">key_mgmt</span>=WPA-PSK</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同样在 <code>boot</code> 分区里面创建一个空文件，重命名为 <code>ssh</code>，注意不是 <code>ssh.txt</code>，没有后缀！！！</p>
</li>
<li><p>树莓派上电，可以在浏览器输入 <code>192.168.0.1</code> 查看树莓派地址，一般为 <code>192.168.0.x</code></p>
</li>
<li><p>用 putty 或者其他软件 ssh 远程连接，默认用户名 <code>pi</code>，密码 <code>raspberry</code></p>
</li>
<li><p>可以在命令行输入 <code>sudo raspi-config</code> 打开 VNC、修改分辨率</p>
</li>
</ol>
<h3 id="2-2-方法二：网线"><a href="#2-2-方法二：网线" class="headerlink" title="2.2 方法二：网线"></a>2.2 方法二：网线</h3><p>可以用网线直接把树莓派和笔记本电脑连接在一块。之后需要打开 <code>控制面板 -&gt; 网络和 Internet -&gt; 网络和共享中心</code>，不出意外的话可以看到下面的东西</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/net.PNG" alt=""></p>
<p>点击 <code>WLAN -&gt; 属性 -&gt; 共享</code>，  如下图勾选对号并且点击 <code>设置</code> 勾选 <code>1703</code> ，点击 <code>确定</code>。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/net2.PNG" alt=""></p>
<p>再点击 <code>以太网 -&gt; 详细信息</code>，看到那个 <code>IPv4地址</code>，记住他 <code>192.168.137.1</code>。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/net4.PNG" alt=""></p>
<p>然后打开命令行 <code>win+R -&gt; cmd</code>，输入 <code>arp -a</code>，就可以看到下面的东西，找到那个接口 <code>192.168.137.1</code>（就是上面记住的那个地址），看下面的 IP 地址（我这里是因为连接了两次，所以分配了两次地址 <code>192.168.137.43 or 79</code>，如果是第一次连接的话应该只有一个地址）就是树莓派的地址。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/net5.PNG" alt=""></p>
<p>获得这个 IP 地址以后就能跟方法一中一样，用 ssh 连接树莓派了。</p>
<h2 id="3-修改系统镜像源"><a href="#3-修改系统镜像源" class="headerlink" title="3. 修改系统镜像源"></a>3. 修改系统镜像源</h2><ol>
<li><p>备份原文件</p>
<figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">sudo <span class="keyword">cp</span> /etc/apt/sources<span class="meta">.list</span> /etc/apt/sources<span class="meta">.list</span>.bak 	</span><br><span class="line">sudo <span class="keyword">cp</span> /etc/apt/sources<span class="meta">.list</span>.d/raspi<span class="meta">.list</span> /etc/apt/sources<span class="meta">.list</span>.d/raspi<span class="meta">.list</span>.bak</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改软件更新源</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> /etc/apt/sources.<span class="keyword">list</span></span><br></pre></td></tr></table></figure>
<p>输入以下内容</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">deb <span class="symbol">http:</span>/<span class="regexp">/mirrors.tuna.tsinghua.edu.cn/raspbian</span><span class="regexp">/raspbian/</span> buster main contrib non-free rpi</span><br><span class="line">deb-src <span class="symbol">http:</span>/<span class="regexp">/mirrors.tuna.tsinghua.edu.cn/raspbian</span><span class="regexp">/raspbian/</span> buster main contrib non-free rpi</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改系统更新源</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> /etc/apt/sources.<span class="keyword">list</span>.d/raspi.<span class="keyword">list</span></span><br></pre></td></tr></table></figure>
<p>输入以下内容</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">deb http:<span class="regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="regexp">/raspberrypi/</span> buster main ui</span><br></pre></td></tr></table></figure>
</li>
<li><p>同步更新源及更新软件包</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> update</span><br><span class="line">sudo apt-<span class="builtin-name">get</span> upgrade</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="4-修改-pip-镜像源"><a href="#4-修改-pip-镜像源" class="headerlink" title="4. 修改 pip 镜像源"></a>4. 修改 pip 镜像源</h2><p><strong>临时更换源</strong>只需要在命令后面加一个参数：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install django -i http://pypi.douban.com/simple</span><br></pre></td></tr></table></figure>
<p><strong>永久更换源</strong>可以创建文件 <code>~/.pip/pip.conf</code> 并写入以下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure>
<p>上面是选择的清华源，还有以下国内镜像源可以选择：</p>
<ul>
<li>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></li>
<li>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a></li>
<li>华中理工大学：<a href="http://pypi.hustunique.com/" target="_blank" rel="noopener">http://pypi.hustunique.com/</a></li>
<li>山东理工大学：<a href="http://pypi.sdutlinux.org/" target="_blank" rel="noopener">http://pypi.sdutlinux.org/</a></li>
<li>豆瓣：<a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></li>
</ul>
<h2 id="5-串口通信"><a href="#5-串口通信" class="headerlink" title="5. 串口通信"></a>5. 串口通信</h2><p>树莓派需要手动打开串口，可以在命令行输入 <code>sudo raspi-config</code> 选择 <code>interface options</code> 打开 <code>Serial</code>。树莓派 4B 的针脚如图所示，对于 USB 转串口模块只需要接树莓派的 6(GND),8(TXD),10(RXD) 口即可，注意 USB 转串口的 RXD 要与树莓派的 TXD 连接。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/pin-raspi.png" alt=""></p>
]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>raspberry pi</tag>
      </tags>
  </entry>
  <entry>
    <title>强大数定律与弱大数定律</title>
    <url>/2020/05/30/statistic/law%20of%20large%20numbers/</url>
    <content><![CDATA[<p>最近学习的时候遇到了<strong>强大数定律</strong>与<strong>弱大数定律</strong>，两者的区分提到了“<strong>依概率收敛</strong>”和“<strong>几乎处处收敛</strong>”，由于本人的数学基础太差，一直很难理解这个地方，在网上查阅了一些资料有了一些个人的理解，不知道对不对，不过还是想记录下来。提前说明，这里给出的解释非常的不严格，<del>甚至有点搞笑</del>，不过个人觉得很容易理解。</p>
<p>首先给出来强弱大数定律的表述。</p>
<p>对于独立同分布的无穷随机序列 $\{X_i\}$，期望为 $\mathbb{E}(X_i)=\mu$。大数定律要研究的就是样本均值的极限</p>
<script type="math/tex; mode=display">
\bar{X}_n=\frac{1}{n}(X_1+\cdots+X_n) \\
\bar{X}_n\to \mu \quad\text{ for }\quad n\to \infty</script><p>他们的主要不同点在于收敛性的强弱不同。</p>
<p><strong>弱大数定律</strong>：样本均值<strong>依概率收敛(converges in probability)</strong>于期望</p>
<script type="math/tex; mode=display">
\lim_{n\to\infty}\text{Pr}\left(\vert \bar{X}_n-\mu \vert > \varepsilon\right)=0</script><p><strong>强大数定律</strong>：样本均值<strong>几乎处处收敛(converges almost surely)</strong>于期望</p>
<script type="math/tex; mode=display">
\text{Pr}(\lim_{n\to\infty}\vert \bar{X}_n-\mu \vert > \varepsilon)=0 \\
\iff \text{Pr}(\lim_{n\to\infty} \bar{X}_n =\mu )=1</script><a id="more"></a>
<p>从形式上来看似乎只是把极限和概率交换了一下位置，但是这个交换就导致了本质的区别，后面会解释。这里首先参考<a href="https://www.zhihu.com/question/21110761/answer/23815273" target="_blank" rel="noopener">强大数定律和弱大数定律的本质区别？ - runze Zheng的回答 - 知乎</a>，这个回答以及网上很多的解释主要是从“依概率收敛”与“几乎处处收敛”的角度出发，我觉得这个对理解很有帮助。下面简单复述这篇回答。</p>
<blockquote>
<p><strong>注</strong>：这部分内容摘自<a href="https://www.zhihu.com/question/21110761/answer/23815273" target="_blank" rel="noopener">知乎回答</a></p>
<h4 id="1-依概率收敛的例子"><a href="#1-依概率收敛的例子" class="headerlink" title="1. 依概率收敛的例子"></a>1. 依概率收敛的例子</h4><p>考虑下图，图中的每条线都代表一个数列，虚线表示一个非常小的区间。总的来说每个数列都越来越趋近0，且大部分时候不会超过虚线所表示的小边界，但是，偶尔会有一两条线超过虚线、然后再回到虚线之内。而且我们<strong>不能保证，有没有哪一个数列会在未来再次超出虚线的范围然后再回来——虽然概率很小</strong>。注意虚线的范围可以是任意小的实数，此图中大约是$\pm 0.04$，可以把这个边界缩小到$\pm 0.004$，甚至$\pm 4*10^{-10}$，随你喜欢，这个性质始终存在。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/weak-lln.jpg" alt=""></p>
<h4 id="2-几乎处处收敛的例子"><a href="#2-几乎处处收敛的例子" class="headerlink" title="2. 几乎处处收敛的例子"></a>2. 几乎处处收敛的例子</h4><p>图中的黑线表示一个随机数列，这个数列在大约 $n=200$ 之后进入了一个我们定的小边界（用虚线表示），之后我们可以确定，它再也不会超出虚线所表示的边界（超出这个边界的概率是0）。跟上面的例子一样，虚线所表示的边界可以定得任意小，而一定会有一个n值，当这个数列超过了n值之后，超出这个边界的概率就是0了。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/strong-lln.jpg" alt=""></p>
</blockquote>
<p>注意上面依概率收敛中有一句“不能保证，有没有哪一个数列会在未来再次超出虚线的范围然后再回来——虽然概率很小”，这个很重要，因为完全有可能有一个序列是这样的（我后面把这种序列叫做“刺头序列”吧）：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/series-not-cvg.PNG" alt=""></p>
<p>这个序列大趋势是 $X_n=1/n$，但是每过一段时间就有一个刺头跳出来。我们不能说 $\lim_{n\to\infty}X_n=0$，这是因为序列极限的定义为：若若对于任意 $\varepsilon &gt; 0$，都存在一个 $N$ 使得 $|X_m-\mu| &lt; \varepsilon,\forall m &gt; N$，则 $\lim_{n\to\infty} X_n=\mu$。但是对于上面这个例子呢？如果 $\varepsilon &lt; 0.2$，我们就找不到这样一个 $N$ 满足条件。</p>
<p>弱大数定律中允许存在这样的“刺头序列”，甚至全部是这种刺头序列也没关系，只要保证你们的“刺头”别挤在一块，偶尔出来跳一下，这样计算一个概率以后，有刺头出现的概率会随着 $n\to\infty$ 而趋向于 0。</p>
<p>强大数定律中则不允许这样的刺头序列（应该说刺头序列出现的概率是 0），他要求每个序列的极限都一定是 $\mu$。</p>
<p>那么为什么把概率和极限交换一个位置就会出现强弱大数定理的这种差别呢？问题就出在<strong>极限 $\lim$ 的定义</strong>了，当 $\lim$ 在内部的时候，我们是盯着每一个序列看到了无穷远处，然后发现这个序列并不会有刺头跳出来，也就是说每个序列都有很好的性质。而当 $\lim$ 在外部的时候，我们是先求了一个概率，然后再看到无穷远处，那么我们只知道求概率以后几乎没有刺头，谁知道中间有没有滥竽充数的呢？甚至可能所有序列都是滥竽充数的，只不过 $n$ 的时候是序列 $a$ 跳出来，$n+1$ 的时候是序列 $b$，$n+2$ 的时候是序列 $c$ ……</p>
]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>大数定律</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记26：不动点迭代</title>
    <url>/2020/05/27/optimization/ch26-fixed-point/</url>
    <content><![CDATA[<p>前面讲了很多具体的算法，比如梯度、次梯度、近似点梯度、加速近似点梯度、PPA、DR方法、ADMM、ALM等，对这些方法的迭代过程有了一些了解。这一节则主要是针对算法的<strong>收敛性</strong>进行分析，试图从一个更加抽象的层面，利用<strong>不动点迭代</strong>的思想，把上面的算法综合起来，给一个比较 general 的收敛性分析方法。</p>
<h2 id="1-什么是不动点？"><a href="#1-什么是不动点？" class="headerlink" title="1. 什么是不动点？"></a>1. 什么是不动点？</h2><p>对于希尔伯特空间(Hilbert space) $\mathcal{H}$，定义了内积 $\left&lt;\cdot,\cdot\right&gt;$ 和 范数 $|\cdot|$（可以借助于 $\mathbb{R}^2$ 来想象）。算子 $T:\mathcal{H}\to\mathcal{H}$（或者是 $C\to C$，$C$ 为 $\mathcal{H}$ 的闭子集）。那么算子 $T$ 的不动点集合就定为</p>
<script type="math/tex; mode=display">
\mathrm{Fix} T:=\{x \in \mathcal{H}: x=T(x)\}</script><p>如果不动点集合非空，想要研究的是不动点迭代 $x^{k+1}\leftarrow T(x^k)$ 的收敛性。为了简便，常把 $T(x)$ 写为 $Tx$。</p>
<p>为什么要研究不动点迭代呢？因为前面我们讲的算法里面很多都可以表示为这种形式。</p>
<a id="more"></a>
<p><strong><em>例子 1(GD)</em></strong>：对于无约束优化 $\min f(x)$，不假设 $f$ 一定是凸的。如果有 $\nabla f(x^\star)=0$，那么 $x^\star$ 被称为驻点(stationary point)。梯度下降做的什么事呢？$x^{k+1}=x^k-\gamma \nabla f(x^k)$，所以实际上算子 $T$ 为</p>
<script type="math/tex; mode=display">
T:=I-\gamma \nabla f</script><p>我们要找的最优解 $x^\star$ 满足 $\nabla f(x^\star)=0\Longrightarrow x^\star=T(x^\star)$，因此我们要找的就是 $T$ 的不动点。</p>
<p><strong><em>例子 2(PG1)</em></strong>：对于有约束优化 $\min f(x),\text{ s.t. }x\in C$，假设 $f$ 为正常的闭凸函数，$C$ 为非空闭凸集。对于这个带约束的优化问题，我们可以做完一步梯度下降以后再做个投影 $x^{k+1}\leftarrow \operatorname{proj}_{C}(x^k-\gamma \nabla f(x^k))$，所以有</p>
<script type="math/tex; mode=display">
T:=\operatorname{proj}_{C}(I-\gamma \nabla f)</script><p>而我们要找的最优解需要满足 $\left\langle\nabla f\left(x^{\star}\right), x-x^{\star}\right\rangle \geq 0 \quad \forall x \in C \iff 0\in \nabla f(x^\star)+\partial \delta_C(x^\star)$，这实际上还是在找 $T$ 的不动点。</p>
<p><strong><em>例子 3(PG2)</em></strong>：上面向 $C$ 的投影实际上也是在算 $\text{prox}$ 算子。对于优化问题 $\min f(x)+g(x)$ 我们要解的方程是 </p>
<script type="math/tex; mode=display">
\begin{array}{c}0\in \nabla f(x)+\partial g(x) \iff 0\in x+\nabla f(x)-x+\partial g(x) \\\iff (I-\nabla f)(x)\in (I+\partial g)(x) \\\iff x=(I+\partial g)^{-1}(I-\nabla f)(x)\end{array}</script><p>上一节讲到了 $(I+\partial g)^{-1}$ 就是 $\text{prox}$ 算子，所以这个不动点迭代就等价于近似点梯度方法。</p>
<p><strong><em>例子 4(KKT)</em></strong>：对于优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}\min\quad& f_0(x) \\\text{s.t.}\quad& g(x)\le0 \\& h(x)=0\end{aligned}</script><p>拉格朗日函数为 $L(x,\lambda,\nu)=f_0(x)+\lambda^T g(x)+\nu^T h(x),\lambda\ge0$，KKT 条件就是要求解方程</p>
<script type="math/tex; mode=display">
T(x,\lambda,\nu)=\left[\begin{array}{c}\partial_x L(x,\lambda,\nu) \\-f(x)+\partial \delta_{\{\lambda\ge0\}}(x) \\h(x)\end{array}\right]=0</script><p>也就是要找到 $\tilde{T}=I+T$ 的不动点。</p>
<p>上面几个例子主要为了说明很多优化算法都可以写成不动点迭代的形式，那么要想分析他们的收敛性，只需要分析不动点迭代的收敛性就可以了，下面要讲的就是这件事情。</p>
<h2 id="2-收敛性分析"><a href="#2-收敛性分析" class="headerlink" title="2. 收敛性分析"></a>2. 收敛性分析</h2><p>要想分析收敛速度，必须要引入的一个性质就是 Lipschitz 连续。</p>
<p><strong>定义</strong>：算子 $T$ 是 $L$-Lipschitz 的，$L\in[0,+\infty)$，如果它满足</p>
<script type="math/tex; mode=display">
\|T x-T y\| \leq L\|x-y\|, \quad \forall x, y \in \mathcal{H}</script><p><strong>定义</strong>：算子 $T$ 是 $L$-<strong>quasi</strong>-Lipschitz 的，$L\in[0,+\infty)$，如果对任意 $x^\star\in \text{Fix}T$ 它满足</p>
<script type="math/tex; mode=display">
\|T x-x^\star\| \leq L\|x-x^\star\|, \quad \forall x \in \mathcal{H}</script><h3 id="2-1-收缩算子收敛性"><a href="#2-1-收缩算子收敛性" class="headerlink" title="2.1 收缩算子收敛性"></a>2.1 收缩算子收敛性</h3><p><strong>定义</strong>：算子 $T$ 是<strong>收缩算子(contractive)</strong>，如果它满足 $L$-Lipschitz，$L\in[0,1)$</p>
<p><strong>定义</strong>：算子 $T$ 是<strong>准收缩算子(quasi-contractive)</strong>，如果它满足 $L$-quasi-Lipschitz，$L\in[0,1)$</p>
<p>准收缩算子很形象，它说明每次迭代之后，我们都会距离不动点 $x^\star$ 更近一点。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/26-contractive.PNG" alt="contractive"></p>
<p>下面是一个证明收敛性的重要定理！</p>
<blockquote>
<p><strong>定理(Banach fixed-point theorem)</strong>：如果 $T$ 是收缩算子，那么</p>
<ul>
<li>$T$ 有唯一的不动点 $x^\star$（存在且唯一）</li>
<li>$x^k\to x^\star$（强收敛）</li>
<li>$| x^k-x^\star|\le L^k|x^0-x^\star|$（线性收敛速度）</li>
</ul>
<p><strong>证明</strong>：略。</p>
</blockquote>
<p><strong><em>例子 1(GD)</em></strong>：对于一般的 $\min f(x)$，我们假设 $f$ 为 Lipschitz-diﬀerentiable 并且是 strongly-convex 的（回忆GD的收敛速度证明）那么就有（假设 $f\in C^2$）</p>
<script type="math/tex; mode=display">
mI\le \nabla^2 f\le LI</script><p>梯度下降的算子为 $T=I-\gamma \nabla f$，我们需要计算这个算子的 Lipschitz 常数</p>
<script type="math/tex; mode=display">
\begin{array}{c}(1-\alpha L)I\le D(I-\alpha\nabla f)=I-\alpha \nabla^2 f(x)\le(I-\alpha m)I \\\| D(I-\alpha\nabla f)\| \le \max(|1-\alpha m|,|1-\alpha L|)\end{array}</script><p>注：$T=I-\gamma \nabla f$ 里面的 $I$ 表示算子，$I-\alpha \nabla^2 f(x)$ 里边的 $I$ 就表示对角元素等于 1 的矩阵，虽然形式一样，但意义不太一样。</p>
<p>如果要想 $T$ 是收缩算子，则需要 $\alpha\in(0,2/L)$，这也是为什么我们前面章节证明 GD 收敛性的时候需要步长 $t\le 2/L$。</p>
<p><strong><em>例子 2</em></strong>：如果是对于一般的算子，我们想要求解 $0\in F(x)$，类比梯度下降方法，可以有 $x^{k+1}=x^k-\alpha Fx^k$，$T=I-\alpha F$。类似的，我们也假设 $F$ 为 $m$ strongly monotone，$L$-Lipschitz 的，那么有</p>
<script type="math/tex; mode=display">
\begin{aligned}\| Tx-Ty\|^2_2 &=\| (I-\alpha F)(x)-(I-\alpha F)(y)\|^2_2 \\&= \|x-y\|_2^2 - 2\alpha(Fx-Fy)^T(x-y)+\alpha^2\|Fx-Fy\|^2_2 \\&\le (1-2\alpha m+\alpha^2L^2)\|x-y\|^2\end{aligned}</script><p>所以当 $\alpha\in\left(0,\frac{2m}{L^2}\right)$ 的时候 $T$ 是收缩算子，可以证明收敛性。</p>
<p>跟前面梯度下降对比，前面只要求 $\alpha &lt;2/L$，所以梯度下降的要求更宽松。即使不满足强凸性质，梯度下降也能保证收敛，但是这里就必须要有 $m$ strongly monotone，这是因为梯度算子提供了更多的信息。</p>
<h3 id="2-2-非扩张算子收敛性"><a href="#2-2-非扩张算子收敛性" class="headerlink" title="2.2 非扩张算子收敛性"></a>2.2 非扩张算子收敛性</h3><p>前面的收缩算子要求 $L&lt;1$，这个条件还是比较强的，很多时候只能得到 $L=1$，这个时候被称为 <strong>nonexpansive</strong>，也就是非扩张的。这个时候</p>
<ul>
<li>$T$ 可能不存在不动点 $x^\star$</li>
<li>如果 $x^\star$ 存在，不动点迭代 $x^{k+1}=Tx^k$ 有界</li>
<li>可能会发散（这个发散我个人不太理解，或许不收敛也被认为是发散）</li>
</ul>
<p>比如说旋转、反射操作，如图所示</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/26-rotation.PNG" alt="rotation"></p>
<p>那么对于这种算子怎么证明收敛性呢？如果得不到收缩性质，那么我们可以放松一点要求，比如得到下面的</p>
<script type="math/tex; mode=display">
\|T x-T y\|^{2} \leq\|x-y\|^{2}-\eta\|R x-R y\|^{2}, \quad \forall x, y \in \mathcal{H}</script><p>这个算子 $R$ 一会再说，虽然我们还是只能有 $L=1$，但是每次迭代之后都可以减少一点小尾巴，累积起来就能收敛到不动点了。那么就让我们再引入一些定义吧。</p>
<p>定义不动点残差算子 $R:=I-T$，有 $Rx^\star=0\iff x^\star=Tx^\star$。</p>
<p><strong>定义</strong>：若 $T$ 对于某些 $\eta&gt;0$ 满足下式，则称其为平均算子(<strong>averaged operator</strong>)</p>
<script type="math/tex; mode=display">
\|T x-T y\|^{2} \leq\|x-y\|^{2}-\eta\|R x-R y\|^{2}, \quad \forall x, y \in \mathcal{H}</script><p><strong>定义</strong>：若 $T$ 对于某些 $\eta&gt;0$ 满足下式，则称其为准平均算子(<strong>quasi-averaged operator</strong>)</p>
<script type="math/tex; mode=display">
\left\|T x-x^{*}\right\|^{2} \leq\left\|x-x^{*}\right\|^{2}-\eta\|R x\|^{2}, \quad \forall x \in \mathcal{H}</script><p>我们也可以表示为 $\eta:=\frac{1-\alpha}{\alpha}$，从而将上面平均算子称为 $\alpha$-<strong>averaged operator</strong>。如果 $\alpha=1/2$，称 $T$ 为 <strong>firmly nonexpansive</strong>；如果 $\alpha=1$，称 $T$ 为 <strong>nonexpansive</strong>。</p>
<p><strong>引理</strong>：$T$ 是 $\alpha$-averaged operator，当且仅当存在一个 nonexpansive $T’$ 使得</p>
<script type="math/tex; mode=display">
T=(1-\alpha)I+\alpha T'</script><p>或者等价的有算子</p>
<script type="math/tex; mode=display">
T^{\prime}:=\left(\left(1-\frac{1}{\alpha}\right) I+\frac{1}{\alpha} T\right)</script><p>是 nonexpansive。</p>
<p><strong>证明</strong>：通过代数运算可以得到</p>
<script type="math/tex; mode=display">
\alpha\left(\|x-y\|^{2}-\left\|T^{\prime} x-T^{\prime} y\right\|^{2}\right)=\|x-y\|^{2}-\|T x-T y\|^{2}-\frac{1-\alpha}{\alpha}\|R x-R y\|^{2}</script><p>因此 $T’$ nonexpansive $\iff$ T $\alpha$-averaged。证毕。</p>
<p>有了 $\alpha$-averaged 性质，我们也可以得到一个收敛性的定理。</p>
<blockquote>
<p><strong>定理(Krasnosel’skii)</strong>：$T$ 为 <strong>$\alpha$-averaged</strong> 算子，且不动点存在，则迭代方法</p>
<script type="math/tex; mode=display">
x^{k+1}\leftarrow Tx^k</script><p>弱收敛至 $T$ 的不动点。</p>
<p><strong>证明</strong>：略。</p>
<p><strong>定理(Mann’s version)</strong>：$T$ 为 <strong>nonexpansive</strong> 算子，且不动点存在，则迭代方法</p>
<script type="math/tex; mode=display">
x^{k+1} \leftarrow\left(1-\lambda_{k}\right) x^{k}+\lambda_{k} T x^{k}</script><p>弱收敛至 $T$ 的不动点，只要满足</p>
<script type="math/tex; mode=display">
\lambda_{k}>0, \quad \sum_{k} \lambda_{k}\left(1-\lambda_{k}\right)=\infty</script><p><strong>证明</strong>：略。</p>
</blockquote>
<p>Mann’s version 相当于是对 Krasnosel’skii 的一个推广，每一步都取一个不同的 $\alpha$。</p>
<p><strong><em>例子 1(PPA)</em></strong>：对于 $\min f(x)$，近似点算子 </p>
<script type="math/tex; mode=display">
T:=\operatorname{prox}_{\lambda f}</script><p>就是 <strong>ﬁrmly-nonexpansive</strong>，因此可以弱收敛。</p>
<h3 id="2-3-复合算子"><a href="#2-3-复合算子" class="headerlink" title="2.3 复合算子"></a>2.3 复合算子</h3><p>对于多个算子复合，有以下性质：</p>
<ul>
<li>$T_{1}, \ldots, T_{m}: \mathcal{H} \rightarrow \mathcal{H}$ contractive $\Longrightarrow T_{1} \circ \cdots \circ T_{m}$ contractive</li>
<li>$T_{1}, \ldots, T_{m}: \mathcal{H} \rightarrow \mathcal{H}$ nonexpansive $\Longrightarrow T_{1} \circ \cdots \circ T_{m}$ nonexpansive</li>
<li>$T_{1}, \ldots, T_{m}: \mathcal{H} \rightarrow \mathcal{H}$ averaged $\Longrightarrow T_{1} \circ \cdots \circ T_{m}$ averaged</li>
<li>$T_i$ 是 $\alpha_i$-averaged(允许 $\alpha_i=1$) $\Longrightarrow T_{1} \circ \cdots \circ T_{m}$ 是 $\alpha$-averaged，其中</li>
</ul>
<script type="math/tex; mode=display">
\alpha=\frac{m}{m-1+\frac{1}{\max _{i} \alpha_{i}}}</script><p><strong>例子</strong>：如第一小节的投影梯度、PG。</p>
<h2 id="3-更一般的情况"><a href="#3-更一般的情况" class="headerlink" title="3. 更一般的情况"></a>3. 更一般的情况</h2><p>我们的优化问题可以概括为求解方程</p>
<script type="math/tex; mode=display">
0\in(A+B)(x)</script><p>其中 $A,B$ 为算子，那么对这个式子做变形就能得到很多方法。</p>
<p><strong>Forward-backward</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}0\in(A+B)(x) &\iff 0\in(I+\alpha B)(x)-(I-\alpha A)(x) \\&\iff (I-\alpha A)(x)\in(I+\alpha B)(x) \\&\iff x=(I+\alpha B)^{-1}(I-\alpha A)(x)\end{aligned}</script><p><strong>Forward-backward-forward</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}0\in(A+B)(x) &\iff x=(I+\alpha B)^{-1}(I-\alpha A)(x) \\&\iff (I-\alpha A)(x)= (I-\alpha A)(I+\alpha B)^{-1}(I-\alpha A)(x) \\&\iff x=\left((I-\alpha A)(I+\alpha B)^{-1}(I-\alpha A)+\alpha A\right)(x)\end{aligned}</script><p><strong>Combetts-Pesquest</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}0\in(A+B+C)(x) &\iff \begin{cases}0\in Ax+u+Cx \\ u\in Bx \end{cases} \\&\iff 0\in \left[\begin{array}{c}Ax \\ B^{-1}u\end{array}\right] + \left[\begin{array}{c}u+Cx \\ -x\end{array}\right]\end{aligned}</script><p><strong>DR splitting</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}0\in(A+B)(x) \iff (\frac{1}{2}I+\frac{1}{2}C_A R_B)(z)=z,\quad x=R_B(z)\\\end{aligned}</script><p>其中</p>
<p>$C_A$：Cayley operator，$C_A=2R_A-I=2\text{prox}_f-I$</p>
<p>$R_B$：resolvent operator，$R_B=(z)=(I+B)^{-1}(z)$</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>PG 算法</tag>
        <tag>利普希兹连续</tag>
        <tag>强凸函数</tag>
        <tag>近似点算子</tag>
        <tag>PPA</tag>
        <tag>算子分裂法</tag>
        <tag>不动点迭代</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记25：原始对偶问题 PDHG</title>
    <url>/2020/05/24/optimization/ch25-PDHG/</url>
    <content><![CDATA[<p>前面的章节要么从原始问题出发，要么从对偶问题出发，通过求解近似点或者一个子优化问题进行迭代，而且推导过程中我们发现根据问题的参数特征，比如矩阵 $A$ 是瘦高型的还是矮胖型的，采用对偶和原始问题的复杂度会不一样，可以选择一个更简单的。而这一节，我们将要从<strong>原始对偶问题</strong>出发来优化，什么是原始对偶问题呢？就是原始优化变量和对偶优化变量（原始函数和共轭函数）混合在一块，看下面的原理就知道了。</p>
<a id="more"></a>
<h2 id="1-原始对偶问题"><a href="#1-原始对偶问题" class="headerlink" title="1. 原始对偶问题"></a>1. 原始对偶问题</h2><p>现在考虑原始优化问题，其中 $f,g$ 为闭凸函数</p>
<script type="math/tex; mode=display">
\min \quad f(x)+g(Ax)</script><p>这个问题我们前面遇到好多次了，一般都是取 $y=Ax$ 加一个约束条件然后计算拉格朗日函数（自己拿小本本写一下），再求解 KKT 条件对吧。好，让我们列出来 KKT 条件：</p>
<ol>
<li>原始可行性：$x\in\text{dom}f,Ax=y\in\text{dom}g$</li>
<li>$x,z$ 是拉格朗日函数的最小值点，因此 $-A^Tz\in\partial f(x),z\in\partial g(y)$</li>
</ol>
<p>其中 $z\in\partial g(y)\iff Ax=y\in\partial g^\star(z)$。也就是说，要想求解 KKT 条件，我们需要的实际上是求解下面一个“方程”</p>
<script type="math/tex; mode=display">
0 \in\left[\begin{array}{cc}0 & A^{T} \\-A & 0\end{array}\right]\left[\begin{array}{l}x \\z\end{array}\right]+\left[\begin{array}{c}\partial f(x) \\\partial g^{\star}(z)\end{array}\right]</script><blockquote>
<p><strong>Remarks</strong>：这个式子可重要啦，后面还会用到！而且他从集合的角度揭示了我们求解最优值问题的本质，那就是找一个<strong>包含关系</strong>。</p>
<p>比如上面的这个式子我们用一个算子来表示为 $T(x,z)$，我们求解最优值实际上要就是找满足 $0\in T(x,z)$ 的解 $(x^\star,z^\star)$。而对一个简单的优化问题 $\min f(x)$，我们实际上就是在找满足 $0\in\partial f(x)$ 的 $x^\star$，这个时候我们可以把次梯度看作是一个算子。</p>
<p>在这一章的后面几个小节，我们将从算子的角度重新来看待优化问题，看完之后可以再回到这里细细品味。</p>
</blockquote>
<p>好我们先把这个东西放一放，再来看看另一个跟拉格朗日函数有关的函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
h(x,z)&=\inf_{y}L(x,y,z)\\
&=f(x)-g^\star(z)+z^TAx
\end{aligned}</script><p>如果计算 $0\in\partial h(x,z)$ 是不是就是上面那个方程？！也就是说上面很重要的那个方程实际上就是在求解 $h(x,z)$ 的鞍点！很容易理解，因为 KKT 条件本质上就是在求拉格朗日函数的鞍点（当然，如果存在不等式约束就不一定是鞍点了）。大家注意，你看这个 $h$ <del>他又长又宽</del>，这个 $h$ 同时包含了原始变量 $x$ 和对偶变量 $z$，同时还既有原始函数 $f$ 又有对偶函数 $g^\star$，所以我们叫他原始对偶优化问题，$h$ 也是部分拉格朗日函数(partial Lagrangian)。</p>
<h2 id="2-PDHG"><a href="#2-PDHG" class="headerlink" title="2. PDHG"></a>2. PDHG</h2><p>前面说了我们要求解的问题是</p>
<script type="math/tex; mode=display">
0 \in\left[\begin{array}{cc}
0 & A^{T} \\
-A & 0
\end{array}\right]\left[\begin{array}{l}
x \\
z
\end{array}\right]+\left[\begin{array}{c}
\partial f(x) \\
\partial g^{\star}(z)
\end{array}\right]</script><blockquote>
<p><strong>PDHG(Primal-dual hybrid gradient method)</strong>的迭代格式是这样的</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{\tau f}\left(x_{k}-\tau A^{T} z_{k}\right) \\
z_{k+1}=\operatorname{prox}_{\sigma g^{*}}\left(z_{k}+\sigma A\left(2 x_{k+1}-x_{k}\right)\right)
\end{array}</script><p>步长需要满足 $\sigma\tau|A|_2^2\le1$。</p>
</blockquote>
<p>是不是看起来跟 DR 方法很像呢？事实上他们两个是等价的，后面会证明。回忆 ADMM，我们每次需要求解的优化问题是</p>
<script type="math/tex; mode=display">
x^{k+1}=\arg\min_x f(x)+\frac{t}{2}\| Ax-y^k+\frac{z^k}{t}\|^2</script><p>要求解这个优化问题，我们往往会得到一个线性方程，还需要计算 $(A^TA)^{-1}$，这就很麻烦了。但是观察 PDHG 的迭代格式，我们只需要求解 $f,g^\star$ 的 $\text{prox}$ 算子，我们只需要求解 $A,A^T$ 之间的乘法而不需要求逆了，这就方便很多了。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/25-example.png" alt="example"></p>
<p>看上面这个例子，我们前面说过 ADMM 等价于 dual DR，不过这个例子里边 PDHG 是最慢的。</p>
<p>下面我们就来证明一下如何从 PDHG 导出 DR 方法。</p>
<p>对于优化问题 $\min f(x)+g(x)$，实际上相当于 $\min f(x)+g(Ax),A=I$，另外我们再取 PDHG 中的 $\sigma=\tau=1$，那么就可以得到</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{f}\left(x_{k}-z_{k}\right) \\
z_{k+1}=\operatorname{prox}_{g^{*}}\left(z_{k}+2 x_{k+1}-x_{k}\right)
\end{array}</script><p>这实际上就是 DR Splitting 那一节讲的原始对偶形式。</p>
<p>另外也可以从 DR 方法导出 PDHG。我们可以将原问题 $\min f(x)+g(A x)$ 改写为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{minimize} &\quad f(x)+g(A x+B y) \\
\text{subject to} &\quad y=0
\end{aligned}</script><p>这里边我们可以选择 $B$ 使 $AA^T+BB^T=(1/\alpha)I$，其中 $1/\alpha\ge|A|_2^2$。为什么这么选呢？令 </p>
<script type="math/tex; mode=display">
\tilde{g}(x,y)=g(Ax+By)=g\left(\tilde{A}\left(\begin{array}{c}x\\ y\end{array}\right)\right)</script><p>那么就有 $\tilde{A}\tilde{A}^T=(1/\alpha)I$，而前面讲 $\text{prox}$ 算子的时候我们讲了一个性质，满足这个条件的时候 $\text{prox}_{\tilde{g}}$ 可以用 $\text{prox}_g$ 来表示。</p>
<blockquote>
<p>复习：$\text{prox}$ 算子的性质</p>
<p>$f(x)=g(Ax+b)$，对于一般的 $A$ 并不能得到比较好的性质，但如果 $AA^T=(1/\alpha)I$，则有 </p>
<script type="math/tex; mode=display">
\begin{aligned}\operatorname{prox}_{f}(x) &=\left(I-\alpha A^{T} A\right) x+\alpha A^{T}\left(\operatorname{prox}_{\alpha^{-1} g}(A x+b)-b\right) \\&=x-\alpha A^{T}\left(A x+b-\operatorname{prox}_{\alpha^{-1} g}(A x+b)\right)\end{aligned}</script></blockquote>
<p>我们还可以取 $\tilde{f}(x,y)=f(x)+\delta_{0}(y)$，那么优化问题就变成了 $\min \tilde{f}(x,y)+\tilde{g}(x,y)$，应用 DR 方法迭代格式为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{\left[\begin{array}{c}
x_{k+1} \\
y_{k+1}
\end{array}\right]=\operatorname{prox}_{\tau \tilde{f}} \left(\left[\begin{array}{c}
x_{k}-p_{k} \\
y_{k}-q_{k}
\end{array}\right]\right)} \\
{\left[\begin{array}{c}
p_{k+1} \\
q_{k+1}
\end{array}\right]=\operatorname{prox}_{(\tau \tilde{g})^{*}}\left(\left[\begin{array}{c}
p_{k}+2 x_{k+1}-x_{k} \\
q_{k}+2 y_{k+1}-y_{k}
\end{array}\right]\right)}
\end{array}</script><p>我们需要计算 $\text{prox}_{\tilde{f}}$ 和 $\text{prox}_{\tilde{g}}$</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{\tau \tilde{f}}(x, y)=\left[\begin{array}{c}
\operatorname{prox}_{\tau f}(x) \\
0
\end{array}\right]</script><script type="math/tex; mode=display">
\begin{aligned}
\operatorname{prox}_{\tau \tilde{g}}(x, y) &=\left[\begin{array}{c}
x \\
y
\end{array}\right]-\alpha\left[\begin{array}{c}
A^{T} \\
B^{T}
\end{array}\right]\left(A x+B y-\operatorname{prox}_{(\tau / \alpha) g}(A x+B y)\right.\\
&=\left[\begin{array}{c}
x \\
y
\end{array}\right]-\tau\left[\begin{array}{c}
A^{T} \\
B^{T}
\end{array}\right] \operatorname{prox}_{\sigma g^{\star}}(\sigma(A x+B y)) \\
&=\left[\begin{array}{c}
x \\
y
\end{array}\right]-\operatorname{prox}_{(\tau \tilde{g})^\star}(x, y)
\end{aligned}</script><p>其中 $\sigma=\alpha/\tau$。代入到 DR 方法的迭代方程</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{\left[\begin{array}{c}
x_{k+1} \\
y_{k+1}
\end{array}\right]=\left[\begin{array}{c}
\operatorname{prox}_{\tau f}\left(x_{k}-p_{k}\right) \\
0
\end{array}\right]} \\
{\left[\begin{array}{c}
p_{k+1} \\
q_{k+1}
\end{array}\right]=\tau\left[\begin{array}{c}
A^{T} \\
B^{T}
\end{array}\right] \operatorname{prox}_{\sigma g^{*}}\left(\sigma\left[\begin{array}{cc}
A & B
\end{array}\right]\left[\begin{array}{c}
p_{k}+2 x_{k+1}-x_{k} \\
q_{k}+2 y_{k+1}-y_{k}
\end{array}\right]\right)}
\end{array}</script><p>根据第二个式子应该有 $\left[\begin{array}{c} p_{k} \\ q_{k} \end{array}\right] \in \text { range }\left[\begin{array}{c} A^{T} \\ B^{T} \end{array}\right]$，因此存在 $z_k$ 满足 $\left[\begin{array}{c}p_{k+1} \\<br>q_{k+1}\end{array}\right]=\tau\left[\begin{array}{c}A^{T} \\ B^{T}\end{array}\right]z_k$。同时因为 $AA^T+BB^T=(1/\alpha)I$，所以能找到唯一的 $z_k$ 同时满足 $z_k=\sigma(Ap_k+Bq_k)$。那么把 $z_k$ 代入到上面的迭代方程，同时消掉 $y_k=0$，就可以得到</p>
<script type="math/tex; mode=display">
x_{k+1}=\operatorname{prox}_{\tau f}\left(x_{k}-\tau A^{T} z_{k}\right)\\ z_{k+1}=\operatorname{prox}_{\sigma g^{*}}\left(z_{k}+\sigma A\left(2 x_{k+1}-x_{k}\right)\right)</script><p>这就是 PDHG 算法，其中 $\sigma\tau=\alpha\le 1/|A|^2$。</p>
<p>当然，我们还可以对 PDHG 算法进行改进，比如：</p>
<p><strong>PDHG withover relaxation</strong>：$\rho_k\in(0,2)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\bar{x}_{k+1} &=\operatorname{prox}_{\tau f}\left(x_{k}-\tau A^{T} z_{k}\right) \\
\bar{z}_{k+1} &=\operatorname{prox}_{\sigma g^{*}}\left(z_{k}+\sigma A\left(2 \bar{x}_{k+1}-x_{k}\right)\right) \\
\left[\begin{array}{c}
x_{k+1} \\
z_{k+1}
\end{array}\right] &=\left[\begin{array}{c}
x_{k} \\
z_{k}
\end{array}\right]+\rho_{k}\left[\begin{array}{c}
\bar{x}_{k+1}-x_{k} \\
\bar{z}_{k+1}-z_{k}
\end{array}\right]
\end{aligned}</script><p>其收敛性与 DR 方法相同。</p>
<p><strong>PDHG with acceleration</strong>：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{\tau_{k} f}\left(x_{k}-\tau_{k} A^{T} z_{k}\right) \\
z_{k+1}=\operatorname{prox}_{\sigma_{k} g^{*}}\left(z_{k}+\sigma_{k} A\left(\left(1+\theta_{k}\right) x_{k+1}-\theta_{k} x_{k}\right)\right)
\end{array}</script><p>对于强凸函数 $f$，以及适当的选择 $\sigma_k,\tau_k,\theta_k$，收敛速度可以达到 $1/k^2$。</p>
<h2 id="3-单调算子"><a href="#3-单调算子" class="headerlink" title="3. 单调算子"></a>3. 单调算子</h2><p>单调算子(monotone operator)我们在讲次梯度的时候提到过，这次我们从算子的角度理解一下 PDHG 方法。</p>
<h3 id="3-1-集值算子"><a href="#3-1-集值算子" class="headerlink" title="3.1 集值算子"></a>3.1 集值算子</h3><p>集值算子(Multivalued/set-valued operator)，就是说映射得到的不是单个的值，而是一个集合。比如算子 $F$ 把向量 $x\in R^n$ 映射到集合 $F(x)\subseteq R^n$。有两个定义</p>
<ul>
<li>定义域 $\operatorname{dom} F =\left\{x \in \mathbf{R}^{n} | F(x) \neq \emptyset\right\}$</li>
<li>图 $\operatorname{gr}(F) =\left\{(x, y) \in \mathbf{R}^{n} \times \mathbf{R}^{n} | x \in \operatorname{dom} F, y \in F(x)\right\}$</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/25-set-valued.png" alt="set-valued"></p>
<p>对算子放缩、求逆等操作都可以表示为对“<strong>图</strong>”的<strong>线性变换</strong>。</p>
<p><strong>求逆</strong>：$F^{-1}(x)=\{y| x\in F(y)\}$</p>
<script type="math/tex; mode=display">
\operatorname{gr}\left(F^{-1}\right)=\left[\begin{array}{cc}
0 & I \\
I & 0
\end{array}\right] \operatorname{gr}(F)</script><p><strong>放缩</strong>：$(\lambda F)(x)=\lambda F(x)$ and $(F\lambda)(x)=F(\lambda x)$</p>
<script type="math/tex; mode=display">
\operatorname{gr}(\lambda F)=\left[\begin{array}{cc}
I & 0 \\
0 & \lambda I
\end{array}\right] \operatorname{gr}(F), \quad \operatorname{gr}(F \lambda)=\left[\begin{array}{cc}
(1 / \lambda) I & 0 \\
0 & I
\end{array}\right] \operatorname{gr}(F)</script><p><strong>相加</strong>：$(I+\lambda F)(x)=\{x+\lambda y | y \in F(x)\}$</p>
<script type="math/tex; mode=display">
\operatorname{gr}(I+\lambda F)=\left[\begin{array}{cc}
I & 0 \\
I & \lambda I
\end{array}\right] \operatorname{gr}(F)</script><p>注意 $(I+\lambda F)$ 这个形式很特别，如果我们取 $F(x)=\partial f(x)$，那么 $(I+\lambda F)^{-1}$ 实际上就是 $\text{prox}$ 算子（$\lambda&gt;0$），不过我们给他取了另一个名字 <strong>Resolvent</strong>，$y\in(I+\lambda F)^{-1}(x)\iff x-y\in\partial f(y)$，用图来表示就是</p>
<script type="math/tex; mode=display">
\operatorname{gr}\left((I+\lambda F)^{-1}\right)=\left[\begin{array}{cc}
I & \lambda I \\
I & 0
\end{array}\right] \operatorname{gr}(F)</script><p><strong><em>例子 1</em></strong>：$(I+\lambda \partial f(x))^{-1}=\text{prox}_{\lambda f}(x)$</p>
<p><strong><em>例子 2</em></strong>：$F(x)=Ax+b$，$(I+\lambda F)^{-1}(x)=(I+\lambda A)^{-1}(x-\lambda b)$，后面这个求逆完全就是矩阵求逆。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/25-transformation.PNG" alt="transformation"></p>
<h3 id="3-2-单调算子"><a href="#3-2-单调算子" class="headerlink" title="3.2 单调算子"></a>3.2 单调算子</h3><p><strong>定义</strong>：$F$ 是单调算子，若</p>
<script type="math/tex; mode=display">
(y-\hat{y})^{T}(x-\hat{x}) \geq 0 \quad \text { for all } x, \hat{x} \in \operatorname{dom} F, y \in F(x), \hat{y} \in F(\hat{x})</script><p>如果用图表示，就应该有</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}x-\hat{x} \\y-\hat{y}\end{array}\right]^{T}\left[\begin{array}{cc}0 & I \\I & 0\end{array}\right]\left[\begin{array}{c}x-\hat{x} \\y-\hat{y}\end{array}\right] \geq 0 \quad \text { for all }(x, y),(\hat{x}, \hat{y}) \in \operatorname{gr}(F) \quad (\bigstar)</script><p>上面这个式子很重要！！！后面会多次用到。</p>
<p><strong><em>例子</em></strong>：我们需要用到的单调算子有：</p>
<ol>
<li>凸函数次梯度 $\partial f(x)$</li>
<li>仿射变换 $F(x)=Cx+d$，并且需要满足 $C+C^T\succeq 0$</li>
<li>他们的组合，比如</li>
</ol>
<script type="math/tex; mode=display">
F(x, z)=\left[\begin{array}{cc}
0 & A^{T} \\
-A & 0
\end{array}\right]\left[\begin{array}{c}
x \\
z
\end{array}\right]+\left[\begin{array}{c}
\partial f(x) \\
\partial g^{*}(z)
\end{array}\right]</script><p>除了单调算子，还有个<strong>最大单调算子(Maximal monotone operator)</strong>，也就是说它的图不能是其他任意单调算子的真子集，举个栗子就明白了，参考下面的图。我们可以知道b闭凸函数的偏导数、单调仿射变换是最大单调算子，除此之外，还有定理。</p>
<p><strong>Minty’s Theorem</strong>：单调算子 $F$ 是最大单调算子当且仅当</p>
<script type="math/tex; mode=display">
\operatorname{im}(I+F)=\bigcup_{x \in \operatorname{dom} F}(x+F(x))=\mathbf{R}^{n}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/25-maximal-monotone.PNG" alt="maximal-monotone"></p>
<p>除了单调性质，我们在证明收敛新的时候往往还要用到 Lipschitz 连续、强凸性质等等，实际上我们前面已经介绍过很多次了，而且用了一堆名词 coercivity、expansive、firmly nonexpansive，我实在是晕了……这里我们就再总结一下。假设算子 $F$ 有 $y=F(x),\hat{y}=F(\hat{x})$</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$(y-\hat{y})^T(x-\hat{x})\ge \mu \Vert x-\hat{x}\Vert^2,\mu&gt;0$</th>
<th>$(y-\hat{y})^T(x-\hat{x})\ge \gamma \Vert y-\hat{y}\Vert^2,\gamma&gt;0$</th>
<th>$(y-\hat{y})^T(x-\hat{x})\le L\Vert x-\hat{x}\Vert^2 $</th>
</tr>
</thead>
<tbody>
<tr>
<td>coercive</td>
<td>co-coercive</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ﬁrmly nonexpansive($\gamma=1$)</td>
<td>nonexpansive($L\le 1$)</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Lipschitz continuous</td>
</tr>
</tbody>
</table>
</div>
<p><strong>它们之间的关系</strong></p>
<ol>
<li>如果满足 co-coercive 并且有 $\gamma=1$，则其为 firmly nonexpansive</li>
<li>如果满足 Lipschitz continuous 并且有 $L\le 1$，则其为 nonexpansive</li>
<li>co-coercivity 可以导出 Lipschitz continuous($L=1/\gamma$)，但反之不一定。不过对于闭凸函数他们是等价的。</li>
</ol>
<p><strong>它们各自的性质</strong></p>
<p><strong>Coercivity</strong>等价于 $F-\mu I$ 是一个单调算子，也等价于</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
x-\hat{x} \\
y-\hat{y}
\end{array}\right]^{T}\left[\begin{array}{cc}
-2 \mu I & I \\
I & 0
\end{array}\right]\left[\begin{array}{c}
x-\hat{x} \\
y-\hat{y}
\end{array}\right] \geq 0 \quad \text { for all }(x, y),(\hat{x}, \hat{y}) \in \operatorname{gr}(F)</script><p><strong>Co-coercivity</strong>等价于</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
x-\hat{x} \\
y-\hat{y}
\end{array}\right]^{T}\left[\begin{array}{cc}
0 & I \\
I & -2 \gamma I
\end{array}\right]\left[\begin{array}{c}
x-\hat{x} \\
y-\hat{y}
\end{array}\right] \geq 0 \quad \text { for all }(x, y),(\hat{x}, \hat{y}) \in \operatorname{gr}(F)</script><p>对前面提到的 resolvent 来说，算子单调性有以下重要性质：</p>
<blockquote>
<p>重要性质：<strong>算子是单调的，当且仅当他的 resolvant 是 firmly nonexpansive</strong></p>
<p>证明只需要根据矩阵等式 $\lambda\left[\begin{array}{ll}0 &amp; I \\ I &amp; 0\end{array}\right]=\left[\begin{array}{cc}I &amp; I \\ \lambda I &amp; 0 \end{array}\right]\left[\begin{array}{cc}0 &amp; I \\ I &amp; -2 I \end{array}\right]\left[\begin{array}{cc}I &amp; \lambda I \\ I &amp; 0 \end{array}\right]$ 就可以得到（结合 $(\bigstar)$ 式）。</p>
</blockquote>
<p>另外单调算子 $F$ 是最大单调算子，当且仅当</p>
<script type="math/tex; mode=display">
\text{dom}(I+\lambda F)^{-1}=R^n</script><p>这可以由 Minty’s theorem 得到。</p>
<h2 id="4-近似点算法"><a href="#4-近似点算法" class="headerlink" title="4. 近似点算法"></a>4. 近似点算法</h2><h3 id="4-1-回望-PPA"><a href="#4-1-回望-PPA" class="headerlink" title="4.1 回望 PPA"></a>4.1 回望 PPA</h3><p>前面讲到了 Resolvant  $(I+\lambda F)^{-1}$ 实际上就是近似点算子，而 PPA 就是在计算近似点，回忆 PPA 的迭代格式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{k+1} &= \text{prox}_{t_k f}(x_k) \\
&= (1+t_k F)^{-1}(x_k)
\end{aligned}</script><p>我们实际上就是在找 Resolvant 算子 $R_t=(I+t F)^{-1}$ 的<strong>不动点</strong></p>
<script type="math/tex; mode=display">
x=R_t(x)\iff x\in(1+tF)(x)\iff 0\in F(x)</script><p>加入松弛后的 PPA 可以写成下面的形式，其中 $\rho_k\in(0,2)$ 为松弛参数</p>
<script type="math/tex; mode=display">
x_{k+1}=x_k+\rho_k(R_{t_k}(x_k)-x_k)</script><p>那么<strong>收敛性</strong>是怎么样呢？假如 $F^{-1}(0)\neq \varnothing$，在满足以下条件时 PPA 可以收敛</p>
<ul>
<li>$t_k=t&gt;0,\rho_k=\rho\in(0,2)$ 都选择常数值；或者</li>
<li>$t_k,\rho_k$ 随迭代次数变化，但是需要满足 $t_k\ge t_{\min}&gt; 0,0&lt;\rho_{\min}\le \rho_k\le \rho_{\max}&lt; 2,\forall k$</li>
</ul>
<p>这个收敛性的证明可以通过证明 Resolvant 的 firmly nonexpansiveness 性质来完成（可以去复习 DR 方法的收敛性证明，那里实际上也是一个不动点迭代）。</p>
<h3 id="4-2-再看-PPA"><a href="#4-2-再看-PPA" class="headerlink" title="4.2 再看 PPA"></a>4.2 再看 PPA</h3><p>先打个预防针，这一部分很重要！！！看完以后也许会对 PPA 以及其他优化算法有更多的理解！！！</p>
<p>首先我们回忆 PPA 是什么。对于优化问题 $\min f(x)$，迭代格式为 $x^+=\text{prox}_{tf}(x)$。如果我们把 $\partial f(x)$ 用算子 $F$ 来表示，那么优化问题实际上就是在找满足 $0\in F(x)$ 的解，PPA 实际上就是在找不动点 $x=(1+tF)^{-1}(x)$。</p>
<p>假如我们现在引入一个<strong>非奇异矩阵</strong> $A$，令 $x=Ay$ 代入到原方程（为什么要这么做？如果合适地选择 $A$ 的话，有时候可以使问题简化，跟着推导的思路看到最后就能理解了，来吧！）</p>
<p>记 $g(y)=f(Ay)$，那么优化问题变为了 $\min g(y)$，注意由于 $A$ 是非奇异的，所以这个问题跟原问题 $\min f(x)$ 是等价的。我们需要找满足 $0\in\partial g(y)=A^T\partial f(Ay)$ 的解，于是可以定义算子 $G(y)=\partial g(y)=A^TF(Ay)$，这个时候 $G$ 的图就是做一个线性变换</p>
<script type="math/tex; mode=display">
\operatorname{gr}(G)=\left[\begin{array}{cc}A^{-1} & 0 \\0 & A^{T}\end{array}\right] \operatorname{gr}(F)</script><p>如果 $F$ 是一个单调算子的话，那么 $G$ 也是一个<strong>单调算子</strong>，这是因为（结合 $(\bigstar)$ 式）</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cc}A^{-1} & 0 \\0 & A^{T}\end{array}\right]^{T}\left[\begin{array}{cc}0 & I \\I & 0\end{array}\right]\left[\begin{array}{cc}A^{-1} & 0 \\0 & A^{T}\end{array}\right]=\left[\begin{array}{cc}0 & I \\I & 0\end{array}\right]</script><p>然后对 $\min g(y)$ 应用 PPA 迭代格式为</p>
<script type="math/tex; mode=display">
y_{k+1}=(I+t_kG)^{-1}(y_k)</script><p>我们把 $x_k=Ay_k,G=A^TF(Ay)$ 都代入进去，就能把上面的式子等价表示为</p>
<script type="math/tex; mode=display">
\frac{1}{t_k}H(x_k-x)\in F(x)</script><p>其中 $H=(AA^T)^{-1}\succ 0$。这个式子又可以表示为</p>
<script type="math/tex; mode=display">
x_{k+1}=(H+t_kF)^{-1}(Hx_k)</script><p>因为 $\min g(y)\iff \min f(x)$，所以上面这个迭代格式也完全适用于原问题，如果取 $H=I$ 那就是原始形式的 PPA，如果取别的形式，那么就获得了推广形式的 PPA！</p>
<p>引入 $A$ 有什么作用呢？我们看</p>
<script type="math/tex; mode=display">
\frac{1}{t_k}H(x_k-x)\in F(x) \iff x_{k+1}=\arg\min_x\left(f(x)+\frac{1}{2t_k}\|x-x_k\|_H^2 \right)</script><p>其中 $|x|_H^2=x^THx$，如果说 $f(x)=(1/2)|Bx-b|^2$，那么我们就可以选择 $A$ 使 $H=(1/\alpha) I-B^TB$，这样迭代求解 $x_{k+1}$ 就简单了。当然这个作用范围很有限，下面的例子更能显现他的威力。</p>
<p>我们再回到原始对偶问题，记算子 $F$ 为</p>
<script type="math/tex; mode=display">
F(x,z)=\left[\begin{array}{cc}0 & A^{T} \\-A & 0\end{array}\right]\left[\begin{array}{l}x \\z\end{array}\right]+\left[\begin{array}{c}\partial f(x) \\\partial g^{\star}(z)\end{array}\right]</script><p>优化问题就是要找到 $0\in F(x,z)$。如果用原始的 PPA 算法，迭代方程为 $(x_{k+1},z_{k+1})=(I+tF)^{-1}(x_k,z_k)$，$(x_{k+1},z_{k+1})$ 是下面方程的解</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/25-ppa.PNG" alt="ppa"></p>
<p>注意到 $x,z$ 纠缠在一起了，我们想把他们拆开来分别求解 $x,z$，问题就能更简单。怎么做呢，引入一个 $H$</p>
<script type="math/tex; mode=display">
H=\left[\begin{array}{cc}I & -\tau A^{T} \\-\tau A & (\tau / \sigma) I\end{array}\right]</script><p>其中若 $\sigma\tau|A|_2^2&lt; 1$ 则 $H$ 为正定矩阵。这个时候 $(x_{k+1},z_{k+1})$ 就是下面方程的解</p>
<script type="math/tex; mode=display">
\begin{array}{c}
{\frac{1}{\tau}\left[\begin{array}{cc}
I & -\tau A^{T} \\
-\tau A & (\tau / \sigma) I
\end{array}\right]\left[\begin{array}{c}
x_{k}-x \\
z_{k}-z
\end{array}\right] \in\left[\begin{array}{cc}
0 & A^{T} \\
-A & 0
\end{array}\right]\left[\begin{array}{c}
x \\
z
\end{array}\right]+\left[\begin{array}{c}
\partial f(x) \\
\partial g^{*}(z)
\end{array}\right] }\\
\Updownarrow \\
{\begin{array}{l}
0 \in \partial f(x)+\frac{1}{\tau}\left(x-x_{k}+\tau A^{T} z_{k}\right) \\
0 \in \partial g^{*}(z)+\frac{1}{\sigma}\left(z-z_{k}-\sigma A\left(2 x-x_{k}\right)\right)
\end{array} }\\
\Updownarrow \\
{\begin{aligned}
x_{k+1} &=(I+\tau \partial f)^{-1}\left(x_{k}-\tau A^{T} z_{k}\right) \\
z_{k+1} &=\left(I+\sigma \partial g^{*}\right)^{-1}\left(z_{k}+\sigma A\left(2 x_{k+1}-x_{k}\right)\right)
\end{aligned} }
\end{array}</script><p>对于化简后的式子，我们就可以先单独求解 $x_{k+1}$，然后再求解 $z_{k+1}$。这实际上也是 PDHG 的迭代方程</p>
<script type="math/tex; mode=display">
\begin{array}{l}x_{k+1}=\operatorname{prox}_{\tau f}\left(x_{k}-\tau A^{T} z_{k}\right) \\z_{k+1}=\operatorname{prox}_{\sigma g^{*}}\left(z_{k}+\sigma A\left(2 x_{k+1}-x_{k}\right)\right)\end{array}</script>]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>近似点算子</tag>
        <tag>PPA</tag>
        <tag>算子分裂法</tag>
        <tag>原始对偶问题</tag>
        <tag>PDHG</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记24：ADMM</title>
    <url>/2020/05/20/optimization/ch24-ADMM/</url>
    <content><![CDATA[<p>上一节讲了对偶问题上的 DR-splitting 就等价于原问题的 ADMM，这一节在详细的讲一下 ADMM 及其变种。</p>
<a id="more"></a>
<h2 id="1-标准-ADMM-形式"><a href="#1-标准-ADMM-形式" class="headerlink" title="1. 标准 ADMM 形式"></a>1. 标准 ADMM 形式</h2><p>首先还是给出 ADMM 要求解的问题的格式，也就是约束存在耦合：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_{x,z} \quad& f(x)+g(z) \\
\text{s.t.} \quad& Ax+Bz=b
\end{align}</script><p>这个问题的增广拉格朗日函数为</p>
<script type="math/tex; mode=display">
L_{\beta}(\mathbf{x}, \mathbf{z}, \mathbf{w})=f(\mathbf{x})+g(\mathbf{z})-\mathbf{w}^{\top}(\mathbf{A} \mathbf{x}+\mathbf{B z}-\mathbf{b})+\frac{\beta}{2}\|\mathbf{A} \mathbf{x}+\mathbf{B z}-\mathbf{b}\|_{2}^{2}</script><p>ADMM 的迭代方程为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{x}^{k+1}=\operatorname{argmin}_{\mathbf{x}} L_{\beta}\left(\mathbf{x}, \mathbf{z}^{\mathbf{k}}, \mathbf{w}^{k}\right) \\
\mathbf{z}^{k+1}=\operatorname{argmin}_{\mathbf{z}} L_{\beta}\left(\mathbf{x}^{k+1}, \mathbf{z}, \mathbf{w}^{k}\right) \\
\mathbf{w}^{k+1}=\mathbf{w}^{k}-\beta\left(\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}^{k+1}-\mathbf{b}\right)
\end{array}</script><p>这实际上就是把 ALM 中关于 $x,z$ 的联合优化给分开了，分别进行优化。如果取 $\mathbf{y}^{k}=\mathbf{w}^{k}/\beta$，就可以转化为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{x}^{k+1}=\operatorname{argmin}_{\mathbf{x}} f(\mathbf{x})+g\left(\mathbf{z}^{k}\right)+\frac{\beta}{2}\left\|\mathbf{A} \mathbf{x}+\mathbf{B} \mathbf{z}^{k}-\mathbf{b}-\mathbf{y}^{k}\right\|_{2}^{2} \\
\mathbf{z}^{k+1}=\operatorname{argmin}_{\mathbf{z}} f\left(\mathbf{x}^{k+1}\right)+g(\mathbf{z})+\frac{\beta}{2}\left\|\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}-\mathbf{b}-\mathbf{y}^{k}\right\|_{2}^{2} \\
\mathbf{y}^{k+1}=\mathbf{y}^{k}-\left(\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}^{k+1}-\mathbf{b}\right)
\end{array}</script><p>最后一步也可以加一个步长系数</p>
<script type="math/tex; mode=display">
\mathbf{y}^{k+1}=\mathbf{y}^{k}-\gamma\left(\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}^{k+1}-\mathbf{b}\right)</script><h2 id="2-收敛性分析"><a href="#2-收敛性分析" class="headerlink" title="2. 收敛性分析"></a>2. 收敛性分析</h2><p>假如 $x^\star,z^\star,y^\star$ 是该问题的最优解，那么对拉格朗日函数求导可以得到 KKT 条件</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
(\text {primal feasibility}) & \mathbf{A x}^{\star}+\mathbf{B z}^{\star}=\mathbf{b} \\
(\text {dual feasibility } I) & 0 \in \partial f\left(\mathbf{x}^{\star}\right)+\mathbf{A}^{T} \mathbf{y}^{\star} \\
(\text {dual feasibility } I I) & 0 \in \partial g\left(\mathbf{z}^{\star}\right)+\mathbf{B}^{T} \mathbf{y}^{\star}
\end{array}</script><p>由于 $\mathbf{z}^{k+1}=\operatorname{argmin}_{\mathbf{z}} g(\mathbf{z})+\frac{\beta}{2}\left|\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}-\mathbf{b}-\mathbf{y}^{k}\right|_{2}^{2}$，求导就可以得到</p>
<script type="math/tex; mode=display">
\Rightarrow 0 \in \partial g\left(\mathbf{z}^{k+1}\right)+\mathbf{B}^{T}\left(\mathbf{A} \mathbf{x}^{k+1}+\mathbf{B} \mathbf{z}^{k+1}-\mathbf{b}-\mathbf{y}^{k}\right)=\partial g\left(\mathbf{z}^{k+1}\right)+\mathbf{B}^{T} \mathbf{y}^{k+1}</script><p>也就是说对偶可行性 $II$ 在每次迭代过程中都能满足，但是对偶可行性 $I$ 则不能满足，因为有</p>
<script type="math/tex; mode=display">
0 \in \partial f\left(\mathbf{x}^{k+1}\right)+\mathbf{A}^{T}\left(\mathbf{y}^{k+1}+\mathbf{B}\left(\mathbf{z}^{k}-\mathbf{z}^{k+1}\right)\right)</script><p>当 $k\to\infty$ 的时候 $I$ 还是可以渐近逼近的。</p>
<p><strong>收敛性</strong>：如果假设 $f,g$ 是闭凸函数，并且 KKT 条件的解存在，那么 $\mathbf{A} \mathbf{x}^{k}+\mathbf{B z}^{k} \rightarrow \mathbf{b}$，$f\left(\mathbf{x}^{k}\right)+g\left(\mathbf{z}^{k}\right) \rightarrow p^{*}$，$\mathbf{y}^{k}$ 收敛。并且如果 $(x^k,y^k)$ 有界，他们也收敛。</p>
<p><strong>收敛速度</strong>：ADMM 算法的收敛速度没有一个 general 的分析和结论。在不同的假设条件下有不同的结论。</p>
<ul>
<li>如果每步更新都有关于 $x^k,y^k,z^k$ 的准确解，并且 $f$ 光滑，$\nabla f$ 利普希茨连续，那么收敛速度为 $O(1/k),O(1/k^2)$（应该是针对不同情况可能有不同速度，课上也没怎么讲，了解一下就够了）</li>
<li>……</li>
</ul>
<h2 id="3-ADMM-变种"><a href="#3-ADMM-变种" class="headerlink" title="3. ADMM 变种"></a>3. ADMM 变种</h2><p>在 ADMM 的标准形式里比较关键的实际上就是要求一个如下形式的子问题（极小化问题）</p>
<script type="math/tex; mode=display">
\min _{\mathbf{x}} f(\mathbf{x})+\frac{\beta}{2}\|\mathbf{A} \mathbf{x}-\mathbf{v}\|_{2}^{2}</script><p>其中 $\mathbf{v}=\mathbf{b}-\mathbf{B} \mathbf{z}^{k}+\mathbf{y}^{k}$。这个问题对于不同的 $f$ 求解复杂度也不一样，而且有的时候并不能得到准确的解，只能近似。实际上这也是一个优化问题，可以采用的方法有</p>
<ol>
<li>迭代方法，比如 CG，L-BFGS；</li>
<li>如果 $f(x)=1/2 |Cx-d|^2$，那么子问题就是求解方程 $(C^TC+\beta A^TA)x^{k+1}=\cdots$，由于 $C$ 是固定的参数，因此可以在一开始做一次 Cholesky 分解或者 $LDL^T$ 分解，之后求解就很简单了。另外如果 $(C^TC+\beta A^TA)$ 的结构是简单矩阵 + 低秩矩阵，就可以用 Woodbury 公式矩阵求逆；</li>
<li>单次梯度下降法 $\mathbf{x}^{k+1}=\mathbf{x}^{k}-c^{k}\left(\nabla f\left(\mathbf{x}^{k}\right)+\beta \mathbf{A}^{T}\left(\mathbf{A} \mathbf{x}+\mathbf{B} \mathbf{z}^{k}-\mathbf{b}-\mathbf{y}^{k}\right)\right)$</li>
<li>如果 $f$ 非光滑，也可以把上面的梯度下降换成 proximal 梯度下降；</li>
<li>可以在后面加一个正则项</li>
</ol>
<script type="math/tex; mode=display">
\mathbf{x}^{k+1}=\operatorname{argmin} f(\mathbf{x})+\frac{\beta}{2}\left\|\mathbf{A} \mathbf{x}+\mathbf{B y}^{k}-\mathbf{b}-\mathbf{z}^{k}\right\|_{2}^{2}+\frac{\beta}{2}\left(\mathbf{x}-\mathbf{x}^{k}\right)^{T}\left(\mathbf{D}-\mathbf{A}^{T} \mathbf{A}\right)\left(\mathbf{x}-\mathbf{x}^{k}\right)</script><p>这个时候优化问题就变成了 $\min f(x)+(\beta/2)(x-x^k)^TD(x-x^k)$，如果取一个简单的 $D$ 比如 $D=I$，那么问题就可能得到简化。</p>
<h2 id="4-分布式-ADMM"><a href="#4-分布式-ADMM" class="headerlink" title="4. 分布式 ADMM"></a>4. 分布式 ADMM</h2><p>回想我们之前在计算近似点以及近似点梯度下降的时候，如果函数 $f,g$ 有特殊结构是不是可以分布式并行计算，而 ADMM 的子问题实际上跟近似点算子很像，所以如果有一定的特殊结构也可以并行处理。</p>
<p>首先回忆一下 ADMM 子问题的形式</p>
<script type="math/tex; mode=display">
\min _{\mathbf{x}} f(\mathbf{x})+\frac{\beta}{2}\|\mathbf{A} \mathbf{x}-\mathbf{v}\|_{2}^{2}</script><p>现在这个优化变量 $\mathbf{x}$ 是一个向量，我们的思想就是 $\mathbf{x}$ 分成多个子块 $x_1,…,x_n$，如果函数有特殊的形式，就能把上面的问题解耦成多个子项的求和，然后针对 $x_1,…,x_n$ 就能并行求解了。下面看几种特殊形式。</p>
<h3 id="4-1-Distributed-ADMM-Ⅰ"><a href="#4-1-Distributed-ADMM-Ⅰ" class="headerlink" title="4.1 Distributed ADMM Ⅰ"></a>4.1 Distributed ADMM Ⅰ</h3><p>函数 $f$ 需要是可分的</p>
<script type="math/tex; mode=display">
f(\mathbf{x})=f_{1}\left(\mathbf{x}_{1}\right)+f_{2}\left(\mathbf{x}_{2}\right)+\cdots+f_{N}\left(\mathbf{x}_{N}\right)</script><p>约束条件 $A\mathbf{x}+B\mathbf{z}=\mathbf{b}$ 也需要是可分的</p>
<script type="math/tex; mode=display">
\mathbf{A}=\left[\begin{array}{cccc}\mathbf{A}_{1} & & & \mathbf{0} \\& \mathbf{A}_{2} & & \\& & \ddots & \\\mathbf{0} & & & \mathbf{A}_{N}\end{array}\right]</script><p>如果满足上面的两个性质，那么原本的更新过程</p>
<script type="math/tex; mode=display">
\mathbf{x}^{k+1} \leftarrow \min f(\mathbf{x})+\frac{\beta}{2}\left\|\mathbf{A} \mathbf{x}+\mathbf{B y}^{k}-\mathbf{b}-\mathbf{z}^{k}\right\|_{2}^{2}</script><p>就可以分成并行的 $N$ 个优化问题</p>
<script type="math/tex; mode=display">
\begin{array}{c}\mathbf{x}_{1}^{k+1} \leftarrow \min f_{1}\left(\mathbf{x}_{1}\right)+\frac{\beta}{2}\left\|\mathbf{A}_{1} \mathbf{x}_{1}+\left(\mathbf{B} \mathbf{y}^{k}-\mathbf{b}-\mathbf{z}^{k}\right)_{1}\right\|_{2}^{2} \\\vdots \\\mathbf{x}_{N}^{k+1} \leftarrow \min f_{N}\left(\mathbf{x}_{N}\right)+\frac{\beta}{2}\left\|\mathbf{A}_{N} \mathbf{x}_{N}+\left(\mathbf{B} \mathbf{y}^{k}-\mathbf{b}-\mathbf{z}^{k}\right)_{N}\right\|_{2}^{2}\end{array}</script><p><strong>例子 1</strong>(consensus)：假如我们的 $f$ 并不像上面那样可分，而是 $\min\sum_{i=1}^N f_i(\mathbf{x})$，注意上面要求 $f_i,f_j$ 的自变量分别是 $x_i,x_j$，而这里的 $f_i,f_j$ 的自变量都是 $\mathbf{x}$。可以怎么办呢？引入 $\mathbf{x}$ 的 $N$ 个 copies，把优化问题写成</p>
<script type="math/tex; mode=display">
\begin{align}\min_{\{\mathbf{x}_i\},\mathbf{z}} \quad& \sum_i f_i(\mathbf{x}_i) \\\text{s.t.} \quad& \mathbf{x}_i-\mathbf{z}=0,\forall i\end{align}</script><p><strong>例子 2</strong>(exchange)：优化问题的形式为</p>
<script type="math/tex; mode=display">
\begin{align}\min_{\{\mathbf{x}_i\},\mathbf{z}} \quad& \sum_i f_i(\mathbf{x}_i) \\\text{s.t.} \quad& \sum_i\mathbf{x}_i=0\end{align}</script><p>这个问题的满足 $f$ 可分了，但是却不满足上面要求的 $A$ 的形式。可以怎么做呢？再次引入变量</p>
<script type="math/tex; mode=display">
\begin{align}\min_{\{\mathbf{x}_i\},\mathbf{z}} \quad& \sum_i f_i(\mathbf{x}_i) \\\text{s.t.} \quad& \mathbf{x}_i-\mathbf{x}_i'=0,\forall i \\\quad& \sum_i\mathbf{x}_i'=0\end{align}</script><p>这个时候 $\mathbf{x}_i$ 可以并行计算了，但是 $\mathbf{x}_i’$ 还需要处理</p>
<script type="math/tex; mode=display">
(\mathbf{x}_i')^{k+1}=\arg\min_{\{\mathbf{x}_i'\}} \sum_i \frac{\beta}{2}\|\mathbf{x}_i^{k+1}-\mathbf{x}_i' + \mathbf{y}_i^k/\beta\| \\\text{s.t.} \sum_i \mathbf{x}_i'=0</script><p>这个问题可以得到闭式解，代入关于 $\mathbf{x}_i$ 的迭代方程里就可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}\mathbf{x}_{i}^{k+1} &=\underset{\mathbf{x}_{i}}{\operatorname{argmin}} f_{i}\left(\mathbf{x}_{i}\right)+\frac{\beta}{2}\left\|\mathbf{x}_{i}-\left(\mathbf{x}_{i}^{k}-\operatorname{mean}\left\{\mathbf{x}_{i}^{k}\right\}-\mathbf{u}^{k}\right)\right\|_{2}^{2} \\\mathbf{u}^{k+1} &=\mathbf{u}^{k}+\operatorname{mean}\left\{\mathbf{x}_{i}^{k+1}\right\}\end{aligned}</script><p>实际上，这个 exchange 问题还是 consensus 问题的对偶形式，只需要写出来拉格朗日函数和 KKT 条件就可以了。</p>
<h3 id="4-2-Distributed-ADMM-Ⅱ"><a href="#4-2-Distributed-ADMM-Ⅱ" class="headerlink" title="4.2 Distributed ADMM Ⅱ"></a>4.2 Distributed ADMM Ⅱ</h3><p>前面是对 $\mathbf{x}$ 进行分解，其实我们还可以对 $\mathbf{z}$ 进行分解。对于约束 $A\mathbf{x}+\mathbf{z}=\mathbf{b}$，可以按行分解</p>
<script type="math/tex; mode=display">
\mathbf{A}=\left[\begin{array}{c}\mathbf{A}_{1} \\\vdots \\\mathbf{A}_{L}\end{array}\right], \mathbf{z}=\left[\begin{array}{c}\mathbf{z}_{1} \\\vdots \\\mathbf{z}_{L}\end{array}\right], \mathbf{b}=\left[\begin{array}{c}\mathbf{b}_{1} \\\vdots \\\mathbf{b}_{L}\end{array}\right]</script><p>这个时候假如优化函数的形式为 $\min_{\mathbf{x},\mathbf{z}}\sum_l (f_l(\mathbf{x})+g_l(\mathbf{z}_l)),\text{ s.t.}A\mathbf{x}+\mathbf{z}=\mathbf{b}$，注意到这个时候虽然关于 $\mathbf{z}$ 是可分的，但是关于 $\mathbf{x}$ 却不是，跟前面的方法类似，我们把 $\mathbf{z}$ copy 很多份，就可以转化为</p>
<script type="math/tex; mode=display">
\begin{align}\min_{\mathbf{x},\{\mathbf{x}_l\},\mathbf{z}} \quad& \sum_l (f_l(\mathbf{x}_l)+g_l(\mathbf{z}_l)) \\\text{s.t.} \quad& A_l\mathbf{x}_l+\mathbf{z}_l=\mathbf{b}_l,\forall i \\\quad& \mathbf{x}_l-\mathbf{x}=0\end{align}</script><p>ADMM 方法中第一步我们更新 $\{\mathbf{x}_l\}$ 这可以并行处理，第二步我们更新 $\mathbf{x},\mathbf{z}$，巧妙的是我们也可以把他们两个解耦合开再并行处理。</p>
<h3 id="4-3-Distributed-ADMM-Ⅲ"><a href="#4-3-Distributed-ADMM-Ⅲ" class="headerlink" title="4.3 Distributed ADMM Ⅲ"></a>4.3 Distributed ADMM Ⅲ</h3><p>实际上前面分别是对矩阵 $A$ 按列分解和按行分解，那也很容易想到我们可以既对列分解也对行分解。</p>
<p>对优化问题</p>
<script type="math/tex; mode=display">
\begin{align}\min \quad& \sum_j f_j(\mathbf{x}_j)+\sum_i g_i(\mathbf{z}_i) \\\text{ s.t.}\quad& A\mathbf{x}+\mathbf{z}=\mathbf{b}\end{align}</script><p>那么就可以分解为</p>
<script type="math/tex; mode=display">
\mathbf{A}=\left[\begin{array}{cccc}\mathbf{A}_{11} & \mathbf{A}_{12} & \cdots & \mathbf{A}_{1 N} \\\mathbf{A}_{21} & \mathbf{A}_{22} & \cdots & \mathbf{A}_{2 N} \\& & \ldots & \\\mathbf{A}_{M 1} & \mathbf{A}_{M 2} & \cdots & \mathbf{A}_{M N}\end{array}\right], \text { also } \mathbf{b}=\left[\begin{array}{c}\mathbf{b}_{1} \\\mathbf{b}_{2} \\\vdots \\\mathbf{b}_{M}\end{array}\right]</script><p>优化问题可以转化为</p>
<script type="math/tex; mode=display">
\begin{align}\min \quad& \sum_j f_j(\mathbf{x}_j)+\sum_i g_i(\mathbf{z}_i) \\\text{ s.t.}\quad& \sum_j A_{ij}\mathbf{x}_j+\mathbf{z}_i=\mathbf{b}_i,i=1,...,M\end{align}</script><p>但是注意到这个时候 $\mathbf{x}_j$ 之间还是相互耦合的，类比前面的方法，要想解耦合，我们就找一个“替身”，这次是 $\mathbf{p}_{ij}=A_{ij}\mathbf{x}_j$，那么新的问题就是</p>
<script type="math/tex; mode=display">
\begin{align}\min \quad& \sum_j f_j(\mathbf{x}_j)+\sum_i g_i(\mathbf{z}_i) \\\text{ s.t.}\quad& \sum_j \mathbf{p}_{ij}+\mathbf{z}_i=\mathbf{b}_i,\forall i \\\quad& \mathbf{p}_{ij}=A_{ij}\mathbf{x}_j,\forall i,j\end{align}</script><p>ADMM 中可以交替更新 $\{\mathbf{p}_{ij}\}$ 和 $(\{\mathbf{x}_j\},\{\mathbf{z}_i\})$。关于 $\{\mathbf{p}_{ij}\}$ 的求解有闭式解，关于 $(\{\mathbf{x}_j\},\{\mathbf{z}_i\})$ 也是可以分解为分别更新 $\{\mathbf{x}_j\},\{\mathbf{z}_i\}$，但是需要注意的是更新 $\mathbf{x}_j$ 的时候 $f_j,A_{1j}^TA_{1j},…,A_{Mj}^TA_{Mj}$ 都耦合在一起了，实际当中计算应该还是比较麻烦的。</p>
<h3 id="4-3-Distributed-ADMM-Ⅳ"><a href="#4-3-Distributed-ADMM-Ⅳ" class="headerlink" title="4.3 Distributed ADMM Ⅳ"></a>4.3 Distributed ADMM Ⅳ</h3><p>既然上面第三类方法中还是有耦合，那我们就可以再引入“替身变量”来解耦合，对每个 $\mathbf{x}_j$ 都 copy 出来 $\mathbf{x}_{1j},…,\mathbf{x}_{Mj}$，之后可以得到</p>
<script type="math/tex; mode=display">
\begin{align}\min \quad& \sum_j f_j(\mathbf{x}_j)+\sum_i g_i(\mathbf{z}_i) \\\text{ s.t.}\quad& \sum_j \mathbf{p}_{ij}+\mathbf{z}_i=\mathbf{b}_i,\forall i \\\quad& \mathbf{p}_{ij}=A_{ij}\mathbf{x}_{ij},\forall i,j \\\quad& \mathbf{x}_{j}=\mathbf{x}_{ij},\forall i,j\end{align}</script><p>ADMM 中可以交替更新 $(\{\mathbf{x}_j\},\{\mathbf{p}_{ij}\})$ 和 $(\{\mathbf{x}_{ij}\},\{\mathbf{z}_i\})$</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>ADMM</tag>
        <tag>parallel</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记23：算子分裂法 &amp; ADMM</title>
    <url>/2020/05/10/optimization/ch23-DR-splitting-admm/</url>
    <content><![CDATA[<p>前面章节中，针对 $\min f(x)+g(Ax)$ 形式的优化问题，我们介绍了如 PG、dual PG、ALM、PPA 等方法。但是比如 PG 方法为 </p>
<script type="math/tex; mode=display">
x_{k+1}=\text{prox}_{th}(x_k-t_k\nabla g(x_k))</script><p>ALM 的第一步要解一个联合优化问题 </p>
<script type="math/tex; mode=display">
(x^{k+1},y^{k+1}) = \arg\min_{x,y} L_t(x,y,z^k)</script><p>他们都把 $f,g$ 耦合在一起了。如果我们看原始问题 $\min f(x)+g(Ax)$ 实际上就是要找 $x^\star$ 使得 $0\in\partial f(x^\star)+A^T\partial g(x^\star)$，这一节要介绍的 Douglas-Rachford splitting method 实际上就是要 decoupling。</p>
<a id="more"></a>
<h2 id="1-Douglas-Rachford-splitting-Algorithm"><a href="#1-Douglas-Rachford-splitting-Algorithm" class="headerlink" title="1.Douglas-Rachford splitting Algorithm"></a>1.Douglas-Rachford splitting Algorithm</h2><p>针对如下优化问题，其中 $f,g$ 都是闭凸函数</p>
<script type="math/tex; mode=display">
\min f(x)+g(x)</script><blockquote>
<p>先给出 DR-splitting 方法的迭代方程</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{f}\left(y_{k}\right) \\
y_{k+1}=y_{k}+\operatorname{prox}_{g}\left(2 x_{k+1}-y_{k}\right)-x_{k+1}
\end{array}</script></blockquote>
<p>为什么叫做 splitting 呢？回忆 PPA 是不是需要求解 $x^+ = \text{prox}_{t(f+g)}(x)$，而这里则可以分开依次求 $\text{prox}_f$ 和 $\text{prox}_g$，所以被称为 splitting。这个迭代方程看起来没有规律，那么他能不能收敛呢？答案当然是可以的，$x_k$ 最终会收敛到 $0\in \partial f(x)+\partial g(x)$，这个证明放到后面，先来从别的方面认识一下这个方法。</p>
<p>首先 $f,g$ 并没有区分，因此可以交换两者的位置，那么迭代方程也可以写为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{g}\left(y_{k}\right) \\
y_{k+1}=y_{k}+\operatorname{prox}_{f}\left(2 x_{k+1}-y_{k}\right)-x_{k+1}
\end{array}</script><p>但需要注意的是这两种不同的迭代方程产生的序列是不一样的，也可能会影响收敛的速度，因此这个方法关于 $f,g$ 是不对称的。</p>
<p>如果把 $x_{k+1}$ 带入到第二步，整个过程实际上可以用一个迭代方程表示</p>
<script type="math/tex; mode=display">
y_{k+1} = F(y) \notag\\
F(y)=y+\operatorname{prox}_{g}\left(2 \operatorname{prox}_{f}(y)-y\right)-\operatorname{prox}_{f}(y)</script><p>这是个什么式子呢？<strong>不动点迭代</strong>(fixed-point iteration)！就是在找函数 $F(y)$ 的不动点。这个函数 $F(y)$ 是连续的吗？是的，这是因为上一节中我们证明了 $\text{prox}_{h}(x)$ 满足firmly nonexpansive(co-coercivite) 性质</p>
<script type="math/tex; mode=display">
\left(\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right)^{T}(x-y) \geq\left\|\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right\|_{2}^{2}</script><p>因此近似点算子是 Lipschitz continuous 的，所以 $F(y)$ 也是连续的。那么假如最终找到了不动点 $y$，他有什么性质呢？</p>
<script type="math/tex; mode=display">
y=F(y) \notag\\
\iff 0 \in \partial f\left(\operatorname{prox}_{f}(y)\right)+\partial g\left(\operatorname{prox}_{f}(y)\right)</script><p><strong>证明</strong>：对于不动点 $y=F(y)$，取 $x=\text{prox}_f(y)$，我们有</p>
<script type="math/tex; mode=display">
\begin{aligned}
x=\text{prox}_f(y),&\quad F(y)=y \notag\\
\iff x=\text{prox}_f(y),&\quad x=\text{prox}_g(2x-y) \\
\iff y-x\in \partial f(x),&\quad x-y\in\partial g(x)
\end{aligned}</script><p>其中第一个等价性只需要把 $x$ 带入到 $F(y)$ 中，由此我们就可以得到</p>
<script type="math/tex; mode=display">
0=(y-x)+(x-y)\in\partial f(x)+\partial g(x)</script><p>自然而然地我们证明了一开始提到的 $x_{k}$ 的收敛性。</p>
<p><strong>等价形式</strong>：下面这部分则主要是对原始形式做了一些<strong>变量代换</strong>，使其看起来更简洁，并没有新的内容。首先交换 $x,y$ 的迭代次序</p>
<script type="math/tex; mode=display">
\begin{array}{l}
y_{k+1}=y_{k}+\operatorname{prox}_{g}\left(2 x_{k}-y_{k}\right)-x_{k} \\
x_{k+1}=\operatorname{prox}_{f}\left(y_{k+1}\right)
\end{array}</script><p>引入新变量 $u_{k+1}=\text{prox}_g(2x_k-y_k),w_k=x_k-y_k$</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k+1} &=\operatorname{prox}_{g}\left(x_{k}+w_{k}\right) \\
x_{k+1} &=\operatorname{prox}_{f}\left(u_{k+1}-w_{k}\right) \\
w_{k+1} &=w_{k}+x_{k+1}-u_{k+1}
\end{aligned}</script><p><strong>放缩</strong>：除此之外，我们还可以对原始问题做一个放缩变为 $\min tf(x)+tg(x)$，那么迭代方程就变为如下形式，并没有本质的变化</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k+1} &=\operatorname{prox}_{tg}\left(x_{k}+w_{k}\right) \\
x_{k+1} &=\operatorname{prox}_{tf}\left(u_{k+1}-w_{k}\right) \\
w_{k+1} &=w_{k}+x_{k+1}-u_{k+1}
\end{aligned}</script><p><strong>松弛</strong>：前面降到了实际上是在对 $y$ 做不动点迭代，那么我们可以改为</p>
<script type="math/tex; mode=display">
y_{k+1}=y_{k}+\rho_{k}\left(F\left(y_{k}\right)-y_{k}\right)</script><p>如果 $1&lt;\rho_k&lt;2$ 就是超松弛，如果 $0&lt;\rho_k&lt;1$ 就是低松弛。这个时候迭代方程稍微复杂了一点点</p>
<script type="math/tex; mode=display">
\begin{aligned}
u_{k+1} &=\operatorname{prox}_{g}\left(x_{k}+w_{k}\right) \\
x_{k+1} &=\operatorname{prox}_{f}\left(x_{k}+\rho_{k}\left(u_{k+1}-x_{k}\right)-w_{k}\right) \\
w_{k+1} &=w_{k}+x_{k+1}-x_{k}+\rho_{k}\left(x_{k}-u_{k+1}\right)
\end{aligned}</script><p><strong>共轭函数</strong>：根据 Moreau decomposition $\text{prox}_g(x)+\text{prox}_{g^\star}(x)=x$，如果 $\text{prox}_g$ 比较难计算，我们就可以换到共轭函数上去计算</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{f}\left(y_{k}\right) \\
y_{k+1}=x_{k+1}-\operatorname{prox}_{g^{*}}\left(2 x_{k+1}-y_{k}\right)
\end{array}</script><p>下面举几个例子，主要就是练习近似点算子的计算，因为 DR-splitting 方法主要就是在计算 $f,g$ 的近似点。</p>
<p><strong><em>例子 1</em></strong>：变量 $X\in S^n$，参数 $C\in S_+^n,\gamma&gt;0$</p>
<script type="math/tex; mode=display">
\text { minimize } \quad \operatorname{tr}(C X)-\log \operatorname{det} X+\gamma \sum_{i>j}\left|X_{i j}\right|</script><p>我们取 $f(X)=\operatorname{tr}(C X)-\log \operatorname{det} X,\quad g(X)=\gamma \sum_{i&gt;j}\left|X_{i j}\right|$</p>
<p>$X=\text{prox}_{tf}(\hat{X}) \iff C-X^{-1}+(1/t)(X-\hat{X})$，这个方程可以通过对 $\hat{X}-tC$ 进行特征值分解求解</p>
<p>$X=\text{prox}_{tg}(\hat{X})$ 可以通过软阈值(soft-thresholding)求解</p>
<p><strong><em>例子 2</em></strong>：考虑等式约束的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min \quad& f(x)\\
\text{s.t.} \quad& x\in V
\end{aligned}</script><p>等价于 $g=\delta_V$</p>
<script type="math/tex; mode=display">
\begin{array}{l}
x_{k+1}=\operatorname{prox}_{g}\left(y_{k}\right) \\
y_{k+1}=y_{k}+P_V\left(2 x_{k+1}-y_{k}\right)-x_{k+1}
\end{array}</script><p><strong><em>例子 3</em></strong>：考虑这种复合形式 $\min f_1(x)+f_2(Ax)$，可以引入等式约束</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min \quad& f_1(x)+f_2(y) \\
\text{s.t.} \quad& Ax=y
\end{aligned}</script><p>取 $f(x_1,x_2)=f_1(x_1)+f_2(x_2)$，他的近似点算子是可分的</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{t f}\left(x_{1}, x_{2}\right)=\left(\operatorname{prox}_{t f_{1}}\left(x_{1}\right), \operatorname{prox}_{t f_{2}}\left(x_{2}\right)\right)</script><p>然后像例子 2 一样，向超平面 $[A,-I][x_1,x_2]^T=0$ 做个投影。</p>
<h2 id="2-ADMM"><a href="#2-ADMM" class="headerlink" title="2. ADMM"></a>2. ADMM</h2><p>交替方向乘子法(Alternating Direction Method of Multipliers)也是一个很重要而且很受欢迎的算法，下一节还会详细讲，这里主要是看看他与 DR-splitting 的联系。</p>
<p>这里还是先给出结论：<strong>DR-splitting 中取 $\rho_k=1$，应用在对偶问题上，就等价于原问题的 ADMM 算法</strong>。我们先推导对偶问题上的 DR-splitting 迭代形式，然后再引出 ADMM 方法。</p>
<p>对可分离的凸优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\min \quad& f_1(x_1)+f_2(x_2) \\
\text{s.t.} \quad& A_1x_1+A_2x_2=b \\
(D)\max \quad& -b^{T} z-f_{1}^{*}\left(-A_{1}^{T} z\right)-f_{2}^{*}\left(-A_{2}^{T} z\right)
\end{aligned}</script><p>取 $g(z)=b^{T} z+f_{1}^{\star}\left(-A_{1}^{T} z\right), f(z)=f_{2}^{\star}\left(-A_{2}^{T} z\right)$，DR 方法为</p>
<script type="math/tex; mode=display">
u^{+}=\operatorname{prox}_{t g}(z+w), \quad z^{+}=\operatorname{prox}_{t f}\left(u^{+}-w\right), \quad w^{+}=w+z^{+}-u^{+}</script><p><strong>第一步</strong>：他等价于计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x}_{1} &=\underset{x_{1}}{\operatorname{argmin}}\left(f_{1}\left(x_{1}\right)+z^{T}\left(A_{1} x_{1}-b\right)+\frac{t}{2}\left\|A_{1} x_{1}-b+w / t\right\|_{2}^{2}\right) \\
u^{+} &=z+w+t\left(A_{1} \hat{x}_{1}-b\right)
\end{aligned}</script><p>这个证明很不直观，上一节分析 PPA 与 ALM 的关系的时候，证明了一个很不直观的结论：对 $h(z)=g^{\star}(z)+f^{\star}\left(-A^{T} z\right)$，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^+&=\text{prox}_{th}(z) = z+t(A\hat{x}-\hat{y}) \\
(\hat{x}, \hat{y})&=\underset{x, y}{\operatorname{argmin}}\left(f(x)+g(y)+z^{T}(A x-y)+\frac{t}{2}\|A x-y\|_{2}^{2}\right)
\end{aligned}</script><p><strong>第二步</strong>：与第一个式子是类似的，等价于</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\hat{x}_{2}=\underset{x_{2}}{\operatorname{argmin}}\left(f_{2}\left(x_{2}\right)+z^{T} A_{2} x_{2}+\frac{t}{2}\left\|A_{1} \hat{x}_{1}+A_{2} x_{2}-b\right\|_{2}^{2}\right. \\
z^{+}=z+t\left(A_{1} \hat{x}_{1}+A_{2} \hat{x}_{2}-b\right)
\end{array}</script><p><strong>第三步</strong>：$w^+=tA_2\hat{x}_2$</p>
<p>现在我们就可以引出 ADMM 方法了，他包括三个步骤</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{k+1,1}&=\underset{\tilde{x}_{1}}{\operatorname{argmin}}\left(f_{1}\left(\tilde{x}_{1}\right)+z_{k}^{T} A_{1} \tilde{x}_{1}+\frac{t}{2}\left\|A_{1} \tilde{x}_{1}+A_{2} x_{k, 2}-b\right\|_{2}^{2}\right) \\
x_{k+1,2}&=\underset{\tilde{x}_{2}}{\operatorname{argmin}}\left(f_{2}\left(\tilde{x}_{2}\right)+z_{k}^{T} A_{2} \tilde{x}_{2}+\frac{t}{2}\left\|A_{1} x_{k+1,1}+A_{2} \tilde{x}_{2}-b\right\|_{2}^{2}\right) \\
z_{k+1}&=z_{k}+t\left(A_{1} x_{k+1,1}+A_{2} x_{k+1,2}-b\right)
\end{aligned}</script><p>前两步分别对应了增广拉格朗日函数的两部分，分别对 $x_1,x_2$ 进行优化。与原本的 ALM 算法相比，ALM 是每次对 $(x_1,x_2)$ 进行联合优化，即 </p>
<script type="math/tex; mode=display">
\begin{aligned}
(x_{k+1,1},x_{k+1,2}) = \arg\min_{x_1,x_2} L_t(x_1,x_2,z_k) \\
z_{k+1} = z_k + t\left(A_{1} x_{k+1,1}+A_{2} x_{k+1,2}-b\right)
\end{aligned}</script><p>另外我们前面还讲到了 dual PG 方法跟 ALM 也很像，也是增广拉格朗日函数先对 $x_1$ 优化再对 $x_2$ 优化，但注意他跟 ADMM 不同的地方在于：前者对 $x_1$ 优化的时候不包含后面的二次正则项，而 ADMM 则包含，写出来对比一下就知道了</p>
<script type="math/tex; mode=display">
\begin{aligned}
(dual\ PG)\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+z^{T} A x\right) \\
\hat{y} &=\underset{y}{\operatorname{argmin}}\left(g(y)-z^{T} y+\frac{t}{2}\|A \hat{x}-y\|_{2}^{2}\right) \\
(ADMM) x_{k+1,1}&=\underset{\tilde{x}_{1}}{\operatorname{argmin}}\left(f_{1}\left(\tilde{x}_{1}\right)+z_{k}^{T} A_{1} \tilde{x}_{1}+\frac{t}{2}\left\|A_{1} \tilde{x}_{1}+A_{2} x_{k, 2}-b\right\|_{2}^{2}\right) \\
x_{k+1,2}&=\underset{\tilde{x}_{2}}{\operatorname{argmin}}\left(f_{2}\left(\tilde{x}_{2}\right)+z_{k}^{T} A_{2} \tilde{x}_{2}+\frac{t}{2}\left\|A_{1} x_{k+1,1}+A_{2} \tilde{x}_{2}-b\right\|_{2}^{2}\right)
\end{aligned}</script><h2 id="3-收敛性分析"><a href="#3-收敛性分析" class="headerlink" title="3. 收敛性分析"></a>3. 收敛性分析</h2><p>DR 方法可以看成是一个不动点迭代，因此要证明收敛性，我们需要证明以下两个结论：</p>
<ol>
<li>$y_k$ 收敛到 $F(y)$ 的不动点 $y^\star$</li>
<li>$x_{k+1}=\text{prox}_f(y_k)$ 收敛到 $x^\star=\text{prox}_f(y^\star)$</li>
</ol>
<p>在证明收敛性之前，需要先定义两个函数</p>
<script type="math/tex; mode=display">
\begin{aligned}F(y) &=y+\operatorname{prox}_{g}\left(2 \operatorname{prox}_{f}(y)-y\right)-\operatorname{prox}_{f}(y) \\G(y) &=y-F(y) \\&=\operatorname{prox}_{f}(y)-\operatorname{prox}_{g}\left(2 \operatorname{prox}_{f}(y)-y\right)\end{aligned}</script><p>需要用到的是这两个函数的 firmly nonexpansive(co-coercive with parameter 1) 的性质</p>
<script type="math/tex; mode=display">
\begin{aligned}(F(y)-F(\hat{y}))^{T}(y-\hat{y}) &\geq\|F(y)-F(\hat{y})\|_{2}^{2} \quad \text { for all } y, \hat{y} \\(G(y)-G(\hat{y}))^{T}(y-\hat{y}) &\geq\|G(y)-G(\hat{y})\|_{2}^{2}\end{aligned}</script><p><strong>证明</strong>：令 $x=\text{prox}_f(y),\hat{x}=\text{prox}_f(\hat{y})$，$v=\operatorname{prox}_{g}(2 x-y), \quad \hat{v}=\operatorname{prox}_{g}(2 \hat{x}-\hat{y})$</p>
<p>根据 $F(y)=y+v-x,F(\hat{y})=\hat{y}+\hat{v}-\hat{x}$ 有</p>
<script type="math/tex; mode=display">
\begin{array}{l}(F(y)-F(\hat{y}))^{T}(y-\hat{y}) \\\quad \geq \quad(y+v-x-\hat{y}-\hat{v}+\hat{x})^{T}(y-\hat{y})-(x-\hat{x})^{T}(y-\hat{y})+\|x-\hat{x}\|_{2}^{2} \\\quad=(v-\hat{v})^{T}(y-\hat{y})+\|y-x-\hat{y}+\hat{x}\|_{2}^{2} \\\quad=(v-\hat{v})^{T}(2 x-y-2 \hat{x}+\hat{y})-\|v-\hat{v}\|_{2}^{2}+\|F(y)-F(\hat{y})\|_{2}^{2} \\\quad \geq\|F(y)-F(\hat{y})\|_{2}^{2}\end{array}</script><p>其中用到了 $\text{prox}$ 算子的firm nonexpansiveness 性质</p>
<script type="math/tex; mode=display">
(x-\hat{x})^{T}(y-\hat{y}) \geq\|x-\hat{x}\|_{2}^{2}, \quad(2 x-y-2 \hat{x}+\hat{y})^{T}(v-\hat{v}) \geq\|v-\hat{v}\|_{2}^{2}</script><p>证毕。</p>
<p>然后我们就可以根据以下的不动点迭代方程证明前面提到的收敛性</p>
<script type="math/tex; mode=display">
\begin{aligned}y_{k+1} &=\left(1-\rho_{k}\right) y_{k}+\rho_{k} F\left(y_{k}\right) \\&=y_{k}-\rho_{k} G\left(y_{k}\right)\end{aligned}</script><p>其中需要假设 $F$ 的不动点存在，且满足 $0\in\partial f(x)+\partial g(x)$，以及松弛变量 $\rho_k\in [\rho_{\min},\rho_{\max}],0&lt;\rho_{\min}&lt;\rho_{\max}&lt;2$。</p>
<p><strong>证明</strong>：设 $y^\star$ 为 $F(y)$ 的不动点（也即 $G(y)$ 的零点），考虑第 $k$ 步迭代</p>
<script type="math/tex; mode=display">
\begin{aligned}\left\|y^{+}-y^{\star}\right\|_{2}^{2}-\left\|y-y^{\star}\right\|_{2}^{2} &=2\left(y^{+}-y\right)^{T}\left(y-y^{\star}\right)+\left\|y^{+}-y\right\|_{2}^{2} \\&=-2 \rho G(y)^{T}\left(y-y^{\star}\right)+\rho^{2}\|G(y)\|_{2}^{2} \\&\leq-\rho(2-\rho) \| G(y)) \|_{2}^{2} \\&\leq-M \| G(y)) \|_{2}^{2}\end{aligned}</script><p>其中 $M=\rho_{\min}(2-\rho_{\max})$。上式表明</p>
<script type="math/tex; mode=display">
M \sum_{k=0}^{\infty}\left\|G\left(y_{k}\right)\right\|_{2}^{2} \leq\left\|y_{0}-y^{\star}\right\|_{2}^{2}, \quad \| G(y)\|_2\to 0</script><p>还可以得到 $| y_k-y^\star|_2$ 是单调不增的，因此 $y_k$ 有界。</p>
<p>由于 $| y_k-y^\star|_2$ 单调不增，故极限 $\lim_{k\to \infty} | y_k-y^\star|_2$ 存在；又由于 $y_k$ 有界，故存在收敛子序列。</p>
<p>记 $\bar{y}_k$ 为一个收敛子序列，收敛值为 $\bar{y}$，根据 $G$ 的连续性有 $0=\lim _{k \rightarrow \infty} G\left(\bar{y}_{k}\right)=G(\bar{y})$，因此 $\bar{y}$ 是 $G$ 的l零点，且极限 $\lim_{k\to \infty} | y_k-\bar{y}|_2$ 存在。</p>
<p>接着需要证明唯一性，假设 $\bar{u},\bar{v}$ 是两个不同的极限点，收敛极限 $\lim_{k\to \infty} | y_k-\bar{u}|_2,\lim_{k\to \infty} | y_k-\bar{v}|_2$ 存在，因此</p>
<script type="math/tex; mode=display">
\|\bar{u}-\bar{v}\|_{2}=\lim _{k \rightarrow \infty}\left\|y_{k}-\bar{u}\right\|_{2}=\lim _{k \rightarrow \infty}\left\|y_{k}-\bar{v}\right\|_{2}=0</script><p>证毕。</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>近似点算子</tag>
        <tag>算子分裂法</tag>
        <tag>ADMM</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记22：近似点算法</title>
    <url>/2020/05/09/optimization/ch22-ppa/</url>
    <content><![CDATA[<p>在进入具体的优化算法后，我们首先讲了基于梯度的，比如梯度下降(GD)、次梯度下降(SD)；然后又讲了近似点算子，之后讲了基于近似点算子的方法，比如近似点梯度下降(PG)、对偶问题的近似点梯度下降(DPG)、加速近似点梯度下降(APG)。而这一节讲的，还是基于近似点的！他叫<strong>近似点方法(Proximal Point Algorithm, PPA)</strong>，除此之外还会介绍<strong>增广拉格朗日方法(Augmentted Larangian Method, ALM)</strong>。我们就开始吧！</p>
<a id="more"></a>
<h2 id="1-近似点方法"><a href="#1-近似点方法" class="headerlink" title="1. 近似点方法"></a>1. 近似点方法</h2><p>近似点方法跟近似点梯度下降很像，在此之外我们先简单回顾一下 PG 方法。对优化问题</p>
<script type="math/tex; mode=display">
\text{minimize } f(x)=g(x)+h(x)</script><p>其中 $g$ 为光滑凸函数，而且为了保证收敛性需要满足 Lipschitz 光滑性质，$h$ 为非光滑函数，只要 $h$ 为闭凸函数，对于近似点算子 $\text{prox}_{h}(x)$ 自然满足firmly nonexpansive(co-coercivite) 性质，这个也等价于 Lipschitz continuous 性质</p>
<script type="math/tex; mode=display">
\left(\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right)^{T}(x-y) \geq\left\|\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right\|_{2}^{2}</script><p>迭代格式为</p>
<script type="math/tex; mode=display">
x_{k+1}=\text{prox}_{th}(x_k-t_k\nabla g(x_k))</script><p>这个表达式实际上可以等价表示为</p>
<script type="math/tex; mode=display">
x^+ = x-tG_t(x), \qquad G_t(x):=\frac{x-x^+}{t}\in \partial h(x^+)+\nabla g(x)</script><p>然后我们再回顾一下 APG 方法，实际上就是在 PG 的基础上引入了一个外差，直观理解就是加入了动量</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{k+1} &= \text{prox}_{th}(y_k-t_k\nabla g(y_k)) \\
y_k &= x_k + w_k(x_k-x_{k-1})
\end{aligned}</script><p>好了复习结束！那么近似点方法 PPA 针对的优化问题是 $\min f$，其中 $f$ 为闭凸函数</p>
<blockquote>
<p><strong>迭代格式</strong>为</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{k+1} &= \text{prox}_{t_k f}(x_k) \\
&= \arg\min_u \left( f(u)+\frac{1}{2t_k}\Vert u-x_k\Vert_2^2 \right)
\end{aligned}</script></blockquote>
<p>这实际上可以看作是 PG 方法中取函数 $g=0$，因此所有适用于 PG 的收敛性分析也都适用于 PPA 方法，而且由于 $g=0$，因此也不需要对 $f$ 做 Lipschitz 光滑的假设，因此<strong>步长 $t_k$ 可以是任意正实数，而不需要 $0&lt; t_k &lt; 1/L$</strong>。类比 PG 中的收敛性分析可以得到</p>
<script type="math/tex; mode=display">
t_{i}\left(f\left(x_{i+1}\right)-f^{\star}\right) \leq \frac{1}{2}\left(\left\|x_{i}-x^{\star}\right\|_{2}^{2}-\left\|x_{i+1}-x^{\star}\right\|_{2}^{2}\right) \\

\Longrightarrow f\left(x_{k}\right)-f^{\star} \leq \frac{\left\|x_{0}-x^{\star}\right\|_{2}^{2}}{2 \sum_{i=0}^{k-1} t_{i}} \quad \text { for } k \geq 1</script><p>同样得，我们也可以引入外差进行加速</p>
<script type="math/tex; mode=display">
x_{k+1}=\operatorname{prox}_{t_{k} f}\left(x_{k}+\theta_{k}\left(\frac{1}{\theta_{k-1}}-1\right)\left(x_{k}-x_{k-1}\right)\right) \quad \text { for } k \geq 1</script><p>其中可以是任意 $t_k&gt; 0$，$\theta_k$ 由以下方程解得</p>
<script type="math/tex; mode=display">
\frac{\theta_{k}^{2}}{t_{k}}=\left(1-\theta_{k}\right) \frac{\theta_{k-1}^{2}}{t_{k-1}}</script><p>并且可以证明加速后的方法收敛速度可以达到 $O(1/k^2)$。</p>
<p>PPA 的基本原理就没有了，这里简单总结一下， 实际上核心的地方只有一个迭代格式</p>
<script type="math/tex; mode=display">
x_{k+1} = \text{prox}_{t_k f}(x_k)</script><p>其他的收敛性分析以及加速算法都可以类比 PG 得到。</p>
<h2 id="2-增广拉格朗日方法"><a href="#2-增广拉格朗日方法" class="headerlink" title="2. 增广拉格朗日方法"></a>2. 增广拉格朗日方法</h2><p>增广拉格朗日方法(也叫乘子法)一般是为了解决有约束优化问题，并且我们通常考虑等式约束，对于非等式约束可以通过引入松弛变量将其转化为等式约束。这里我们首先介绍一下基本的 ALM 形式。对于优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min\quad& f(x) \\
\text{s.t.}\quad& C(x)=0
\end{aligned}</script><p><strong>增广拉格朗日函数(重要)</strong>为</p>
<script type="math/tex; mode=display">
L_\sigma(x,\nu) = f(x)+\nu^TC(x) + \frac{\sigma}{2}\Vert C(x)\Vert_2^2,\quad \sigma>0</script><p>就是在初始的拉格朗日函数后面加了一个等式约束的二次正则项</p>
<blockquote>
<p><strong>ALM 的迭代格式</strong>则为</p>
<script type="math/tex; mode=display">
\begin{aligned}
x^{k+1} &= \arg\min_{x} L_\sigma(x,\nu^k) \\
\nu^{k+1} &= \nu^k + \sigma C(x^{k+1})
\end{aligned}</script></blockquote>
<p>一般会将增广拉格朗日函数化简成另一种形式(<strong>重要</strong>)</p>
<script type="math/tex; mode=display">
L_\sigma(x,\nu) = f(x) + \frac{\sigma}{2}\Vert C(x)+\frac{\nu}{\sigma}\Vert_2^2</script><p>就是做了一个配方，但化简前后的两个函数并不完全等价，因为丢掉了 $\nu$ 的二次项，不过对于迭代算法没有影响，因为迭代的第一步仅仅是针对 $x$ 求最小。</p>
<p>如果是不等式约束呢？比如优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min_x\quad& f(x) \\
\text{s.t.}\quad& C(x)\ge0
\end{aligned}
\iff
\begin{aligned}
\min_{x,s}\quad& f(x) \\
\text{s.t.}\quad& C(x)-s=0,\quad s\ge0
\end{aligned}</script><p>此时增广拉格朗日函数为</p>
<script type="math/tex; mode=display">
L_\sigma(x,s,\nu) = f(x)-\nu^T(C(x)-s)+\frac{\sigma}{2}\Vert C(x)-s\Vert^2,\quad s\ge0</script><p>迭代方程为</p>
<script type="math/tex; mode=display">
\begin{aligned}
(x^{k+1},s^{k+1}) &= \arg\min_{x,s\ge0} L_\sigma(x,s,\nu^k) \\
\nu^{k+1} &= \nu^k - \sigma (C(x^{k+1})-s^{k+1})
\end{aligned}</script><p>第一步求极小要怎么计算呢？先把增广拉格朗日函数化为</p>
<script type="math/tex; mode=display">
\min_x\left\{f(x)+\min_{s\ge0}\frac{\sigma}{2}\left\Vert C(x)-s-\frac{\nu}{\sigma}\right\Vert^2 \right\} \\
= \min_x\left\{f(x)+\frac{\sigma}{2}\left\Vert C(x)-\frac{\nu}{\sigma}-\Pi_+(C(x)-\frac{\nu}{\sigma})\right\Vert^2 \right\}</script><p>其中 $\Pi_+$ 表示向 $R_+^n$ 空间的投影。</p>
<p><strong><em>例子</em></strong>：这是一个应用 ALM 的例子，考虑优化问题 $\min f(x),\text{ s.t. }Ax\in C$，用 ALM 的迭代步骤为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &= \arg\min_{x} f(x)+\frac{t}{2}d( Ax+z/t)^2 \\
z :&= z + t(A\hat{x}-P_C(A\hat{x}+z/t))
\end{aligned}</script><p>其中 $P_C$ 是向集合 $C$ 的投影，$d(u)$ 是 $u$ 到集合 $C$ 的欧氏距离。</p>
<h2 id="3-PPA-与-ALM-的关系"><a href="#3-PPA-与-ALM-的关系" class="headerlink" title="3. PPA 与 ALM 的关系"></a>3. PPA 与 ALM 的关系</h2><p>这里先给出一个结论：<strong>对原始问题应用 ALM 等价于对对偶问题应用 PPA</strong>。</p>
<p>下面看分析，考虑优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\text { minimize }\quad& f(x)+g(A x)\\
(D)\text { maximize } \quad& -g^{\star}(z)-f^{\star}\left(-A^{T} z\right)
\end{aligned}</script><p>我们就先来看看原始问题应用 ALM 会得到什么。原始问题等价于</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min\quad& f(x)+g(y) \\
\text{s.t.}\quad& Ax=y
\end{aligned}</script><p>拉格朗日函数为</p>
<script type="math/tex; mode=display">
L_t(x,y,z) = f(x)+g(y)+z^T(Ax-y)+\frac{t}{2}\Vert Ax-y\Vert^2</script><p>ALM 迭代方程为</p>
<script type="math/tex; mode=display">
\begin{aligned}
(x^{k+1},y^{k+1}) &= \arg\min_{x,y} L_t(x,y,z^k) \\
z^{k+1} &= z^k + t(Ax^{k+1}-y^{k+1})
\end{aligned}</script><p>对偶问题应用 PPA 的迭代方程是什么呢？首先我们令 $h(z)=g^{\star}(z)+f^{\star}\left(-A^{T} z\right)$，那么就需要求解</p>
<script type="math/tex; mode=display">
z^{+} = \text{prox}_{th}(z) = \underset{u}{\operatorname{argmin}}\left(f^{\star}\left(-A^{T} u\right)+g^{\star}(u)+\frac{1}{2 t}\|u-z\|_{2}^{2}\right) \\

\iff z-z^+ \in t\partial h(z^+)=t\left(-A\partial f^\star(-A^Tz^+)+\partial g^\star(z^+\right)</script><p>这个 $z^+$ 乍一看跟 ALM 的 $z^{k+1}$ 没有一点关系啊，为什么说他们俩等价呢？这就要引出下面一个等式了（先打个预防针，这个等式以及他的推导很不直观，我也没有想到一个很好的解释，但是这个等式以及推导又很重要！在后面章节中也会用到）</p>
<blockquote>
<p>很重要的式子：</p>
<script type="math/tex; mode=display">
z^+=\text{prox}_{th}(z) = z+t(A\hat{x}-\hat{y})</script><p>其中 $\hat{x},\hat{y}$ 为</p>
<script type="math/tex; mode=display">
(\hat{x}, \hat{y})=\underset{x, y}{\operatorname{argmin}}\left(f(x)+g(y)+z^{T}(A x-y)+\frac{t}{2}\|A x-y\|_{2}^{2}\right)</script></blockquote>
<p>先不管推导，这样看来对偶问题的 PPA 是不是就跟原始问题的 ALM 完全等价了呢？！然后我们来看一下证明（更多的是验证上面这两个等式成立，至于怎么推导出来我也不知道……）</p>
<p>增广拉格朗日函数可以转化为</p>
<script type="math/tex; mode=display">
(\hat{x},\hat{y}) = \underset{x, y}{\operatorname{argmin}}\left(f(x)+g(y)+\frac{t}{2}\|A x-y+z/t\|_{2}^{2}\right)</script><p>我们把它表示成一个优化问题，并且引入等式约束</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize}_{x, y, w} \quad& f(x)+g(y)+\frac{t}{2}\|w\|_{2}^{2} \\
\text{subject to}\quad& A x-y+z / t=w
\end{aligned}</script><p>他的 KKT 条件就是</p>
<script type="math/tex; mode=display">
A x-y+\frac{1}{t} z=w, \quad-A^{T} u \in \partial f(x), \quad u \in \partial g(y), \quad t w=u</script><p>我们把 $x,y,w$ 消掉就得到了 $u = z+t(Ax-y)$，并且有</p>
<script type="math/tex; mode=display">
0 \in-A \partial f^{*}\left(-A^{T} u\right)+\partial g^{*}(u)+\frac{1}{t}(u-z)</script><p>上面这个式子就等价于 $u=\text{prox}_{th}(z)$。因此就有 $\text{prox}_{th}(z) = z+t(A\hat{x}-\hat{y})$。</p>
<h2 id="4-Moreau–Yosida-smoothing"><a href="#4-Moreau–Yosida-smoothing" class="headerlink" title="4. Moreau–Yosida smoothing"></a>4. Moreau–Yosida smoothing</h2><p>这一部分是从另一个角度看待 PPA 算法。我们知道如果 $f$ 是光滑函数就可以直接用梯度下降了，如果是非光滑函数则可以用次梯度或者近似点算法，前面复习 PG 方法的时候提到了 PG 也可以看成是一种梯度下降，梯度为 $G_t(x)$</p>
<script type="math/tex; mode=display">
x_{k+1}=\text{prox}_{th}(x_k-t_k\nabla g(x_k)) \iff x^+ = x-tG_t(x)</script><p>这一部分要讲的就是从梯度下降的角度认识 PPA 方法。</p>
<p>这里再次先抛出结论：<strong>PPA 实际上就是对 $f$ 的某个光滑近似函数 $\tilde{f}$ 做梯度下降</strong>。</p>
<p>这个光滑近似函数是什么呢？对于闭凸函数 $f$，我们定义</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_{(t)}(x) &=\inf _{u}\left(f(u)+\frac{1}{2 t}\|u-x\|_{2}^{2}\right) \quad(\text { with } t>0) \\
&=f\left(\operatorname{prox}_{t f}(x)\right)+\frac{1}{2 t}\left\|\operatorname{prox}_{t f}(x)-x\right\|_{2}^{2}
\end{aligned}</script><p>为函数 $f$ 的 <strong>Moreau Envelop</strong> 。这里是将 $\text{prox}_{tf}(x)$ 代回到了原函数中。在此之前我们需要首先研究一下这个函数的性质。</p>
<ol>
<li>$f_{(t)}$ 为<strong>凸函数</strong>。取 $G(x,u) = f(u)+\frac{1}{2t}\Vert u-x\Vert^2_2$ 是关于 $(x,u)$ 的联合凸函数，因此 $f_{(t)}(x)=\inf_u G(x,u)$ 是凸的；</li>
<li>$\text{dom}f_{(t)}=R^n$。这是因为 $\text{prox}_{tf}(x)$ 对任意的 $x$ 都有唯一的定义；</li>
<li>$f_{(t)}\in C^1$ <strong>连续</strong>；</li>
</ol>
<p>另外可以验证共轭函数为</p>
<script type="math/tex; mode=display">
\left(f_{(t)}\right)^{\star}(y)=f^{\star}(y)+\frac{t}{2}\|y\|_{2}^{2}</script><p>因此还有性质</p>
<ol>
<li>$\left(f_{(t)}\right)^{\star}(y)$ 为 <strong>t-强凸函数</strong>，等价的有 $f_{(t)}(x)$ 为 <strong>1/t-smooth</strong>；</li>
</ol>
<p>既然这个 $f_{(t)}$ 为 $C^1$ 连续的，那么他的梯度是什么呢？</p>
<script type="math/tex; mode=display">
f_{(t)}(x) = \left(f_{(t)}(x)\right)^{\star\star}=\sup _{y}\left(x^{T} y-f^{\star}(y)-\frac{t}{2}\|y\|_{2}^{2}\right)</script><p>根据 Legendre transform 有 $y^\star\in\partial f_{(t)}(x)$，令上面式子关于 $y$ 的次梯度等于 0 可以得到</p>
<script type="math/tex; mode=display">
x-ty^\star \in \partial f^\star(y^\star) \iff y^\star\in\partial f(x-ty^\star) \\\Longrightarrow x-ty^\star=\text{prox}_{tf}(x)</script><p>因此我们就有(<strong>重要</strong>)</p>
<script type="math/tex; mode=display">
y^\star=\nabla f_{(t)}(x) = \frac{1}{t}\left(x-\text{prox}_{tf}(x)\right)</script><p>变换一下就是 $\text{prox}_{tf}(x) = x-t\nabla f_{(t)}(x)$，注意这个式子左边就是 PPA 的迭代方程，右边就是光滑函数函数 $f_{(t)}(x)$ 应用梯度下降法的迭代方程，并且由于这个函数是 $L=1/t$-smooth 的，因此我们取的步长为 $t$ 满足要求 $0&lt;t\le 1/L$。也就是我们这一小节刚开始说的，PPA 等价于对一个光滑近似函数 $f_{(t)}(x)$ 的梯度下降方法。</p>
<p><strong><em>例子 1</em></strong>：举个例子算一下 Moreau Envelop，假如函数 $f(x)=\delta_C(x)$，则 $f_{(t)}(x)=\frac{1}{2t}d(x)^2$，这里 $d(x)$ 是 $x$ 到集合 $C$ 的欧氏距离。</p>
<p><strong><em>例子 2</em></strong>：若 $f(x)=\Vert x\Vert_1$，函数 $f_{(t)}(x)=\sum_k \phi_t(x_k)$ 被称为 <strong>Huber penalty</strong>，其中</p>
<script type="math/tex; mode=display">
\phi_{t}(z)=\left\{\begin{array}{ll}z^{2} /(2 t) & |z| \leq t \\|z|-t / 2 & |z| \geq t\end{array}\right.</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/22-huber.PNG" alt=""></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>近似点算子</tag>
        <tag>ALM</tag>
        <tag>增广拉格朗日函数</tag>
        <tag>PPA</tag>
      </tags>
  </entry>
  <entry>
    <title>Trouble I&#39;m In</title>
    <url>/2020/05/04/music/TroubleImIn/</url>
    <content><![CDATA[<center><h2>Trouble I'm In</h2></center>

<a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=29758362&auto=1&height=66"></iframe>


<blockquote>
<p><strong>Trouble I’m In</strong></p>
<p>I wanna feel your touch<br>It’s burning me like an ember<br>Pretending is not enough<br>I wanna feel lost together<br>So I’m giving in<br>So I’m giving in<br>To the trouble I’m in<br>So I’m giving in<br>To the trouble I’m in<br>To the trouble I’m in</p>
<p>You are you are<br>my favourite medicine<br>You are you are<br>you’re where the edge began<br>You are you are<br>just one last time again<br>You are you are<br>You are the trouble I’m in<br>You are the trouble I’m in<br>You are the trouble I’m in</p>
<p>You are you are<br>my favourite medicine<br>You are you are<br>you’re where the edge began<br>You are you are<br>just one last time again<br>You are you are<br>You are the trouble I’m in<br>You are the trouble I’m in</p>
</blockquote>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="http://music.163.com/song/media/outer/url?id=29797698.mp3"></iframe>
]]></content>
      <categories>
        <category>Music</category>
      </categories>
  </entry>
  <entry>
    <title>凸优化笔记21：加速近似点梯度下降</title>
    <url>/2020/04/24/optimization/ch21-apg-fista/</url>
    <content><![CDATA[<p>我们证明了梯度方法最快的收敛速度只能是 $O(1/k^2)$（没有强凸假设的话），但是前面的方法最多只能达到 $O(1/k)$ 的收敛速度，那么有没有方法能达到这一极限呢？有！这一节要讲的<strong>加速近似梯度方法(APG)</strong>就是。这个方法的构造非常的巧妙，证明过程中会发现每一项都恰到好处的抵消了！真不知道作者是怎么想出来这么巧妙地方法，各位可以看看证明过程自行体会。</p>
<a id="more"></a>
<h2 id="1-加速近似梯度方法"><a href="#1-加速近似梯度方法" class="headerlink" title="1. 加速近似梯度方法"></a>1. 加速近似梯度方法</h2><p>首先说我们要考虑的优化问题形式还是</p>
<script type="math/tex; mode=display">
\text{minimize }\quad f(x)=g(x)+h(x)</script><p>其中 $g$ 为光滑项，$\text{dom }g=R^n$，$h$ 为不光滑项，且为闭的凸函数，另外为了证明梯度方法的收敛性，跟前面类似，我们需要引入 Lipschitz-smooth 条件与强凸性质：</p>
<script type="math/tex; mode=display">
\frac{L}{2}x^Tx-g(x),\quad g(x)-\frac{m}{2}x^Tx \quad \text{convex}</script><p>其中 $L&gt;0,m\ge0$，$m$ 可以等于 0，此时就相当于没有强凸性质。</p>
<p>然后我们就来看看 <strong>APG(Accelerated Proximal Gradient Methods)</strong> 方法到底是怎么下降的。首先取 $x_0=v_0,\theta_0\in(0,1]$，对于每次迭代过程，包括以下几个步骤：</p>
<blockquote>
<ol>
<li>求 $\theta_k$：$\frac{\theta_{k}^{2}}{t_{k}}=\left(1-\theta_{k}\right) \gamma_{k}+m \theta_{k} \quad \text { where } \gamma_{k}=\frac{\theta_{k-1}^{2}}{t_{k-1}}$</li>
<li>更新 $x_k,v_k$：</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
y &=x_{k}+\frac{\theta_{k} \gamma_{k}}{\gamma_{k}+m \theta_{k}}\left(v_{k}-x_{k}\right) \quad\left(y=x_{0} \text { if } k=0\right) \\
x_{k+1} &=\operatorname{prox}_{t_{k} h}\left(y-t_{k} \nabla g(y)\right) \quad(\bigstar)\\
v_{k+1} &=x_{k}+\frac{1}{\theta_{k}}\left(x_{k+1}-x_{k}\right)
\end{aligned}</script></blockquote>
<p>这里面的关键就是上面的 $(\bigstar)$ 式，对比前面讲过的近似梯度下降法实际上是 </p>
<script type="math/tex; mode=display">
x_{k+1} =\operatorname{prox}_{t_{k} h}\left(x_k-t_{k} \nabla g(x_k)\right)</script><p>所以这里实际上主要的变化就是将 $x_k$ 换成了 $y$，那么 $y$ 跟 $x_k$ 又有什么不同呢？</p>
<script type="math/tex; mode=display">
y=x_{k}+\frac{\theta_{k} \gamma_{k}}{\gamma_{k}+m \theta_{k}}\left(v_{k}-x_{k}\right)=x_{k}+\beta_{k}\left(x_{k}-x_{k-1}\right) \\
\beta_{k}=\frac{\theta_{k} \gamma_{k}}{\gamma_{k}+m \theta_{k}}\left(\frac{1}{\theta_{k-1}}-1\right)=\frac{t_{k} \theta_{k-1}\left(1-\theta_{k-1}\right)}{t_{k-1} \theta_{k}+t_{k} \theta_{k-1}^{2}}</script><p>可以看到 $y=x_{k}+\beta_{k}\left(x_{k}-x_{k-1}\right)$ 实际上就是在 $x_k$ 的基础上增加了一个<strong>“动量(Momentum)”</strong>，如下图所示</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/21-momentum.PNG" alt="momentum"></p>
<p>我们自然的要关注 $\beta_k,\theta_k$ 的大小以及有什么性质。首先对于参数 $\theta_k$ 它是根据二次方程一步步迭代出来的</p>
<script type="math/tex; mode=display">
\frac{\theta_{k}^{2}}{t_{k}}=\left(1-\theta_{k}\right) \frac{\theta_{k-1}^{2}}{t_{k-1}}+m \theta_{k}</script><p>可以有几个主要结论：</p>
<ol>
<li>如果 $m&gt;0$ 且 $\theta_0=\sqrt{mt_0}$，那么有 $\theta_k=\sqrt{mt_k},\forall k$</li>
<li>如果 $m&gt;0$ 且 $\theta_0\ge\sqrt{mt_0}$，那么有 $\theta_k\ge\sqrt{mt_k},\forall k$</li>
<li>如果 $mt_k&lt;,$，那么有 $\theta_k&lt;1$</li>
</ol>
<p>下面可以看几个关于 $\theta_k,\beta_k$ 随着迭代次数 $k$ 的变化：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/21-beta-theta.PNG" alt="example"></p>
<p>如果我们取前面的 APG 方法中的 $m=0$，然后消掉中间变量 $v_k$，就可以得到 <strong>FISTA(Fast Iterative Shrinkage-Thresholding Algorithm)</strong> 算法</p>
<script type="math/tex; mode=display">
\begin{aligned}
y &=x_{k}+\theta_{k}\left(\frac{1}{\theta_{k-1}}-1\right)\left(x_{k}-x_{k-1}\right) \quad\left(y=x_{0} \text { if } k=0\right) \\
x_{k+1} &=\operatorname{prox}_{t_{k} h}\left(y-t_{k} \nabla g(y)\right)
\end{aligned}</script><h2 id="2-收敛性分析"><a href="#2-收敛性分析" class="headerlink" title="2. 收敛性分析"></a>2. 收敛性分析</h2><p>前面已经了解了基本原理，下面需要证明一下为什么他可以达到 $O(1/k^2)$ 的收敛速度。作为类比，我们先回忆一下之前是怎么证明梯度方法/近似梯度方法的收敛性的？</p>
<script type="math/tex; mode=display">
\begin{aligned}(GD)\quad&  f\left(x^{+}\right)-f^{\star} \leq \nabla f(x)^{T}\left(x-x^{\star}\right)-\frac{t}{2}\|\nabla f(x)\|_{2}^{2}\\\Longrightarrow &f\left(x^{+}\right)-f^{\star} \leq\frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}\right) \\(SD)\quad& 2 t\left(f(x)-f^{\star}\right) \leq\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}+t^{2}\|g\|_{2}^{2} \\(PD)\quad& f\left(x^+\right) \leq f(z)+G_{t}(x)^{T}(x-z)-\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}-\frac{m}{2}\|x-z\|_{2}^{2}\\\Longrightarrow &f\left(x^{+}\right)-f^{\star} \leq \frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}\right)\end{aligned}</script><p>对于这一节的 APG 方法，证明思路是首先证明下面的迭代式子成立</p>
<script type="math/tex; mode=display">
f\left(x_{i+1}\right)-f^{\star}+\frac{\gamma_{i+1}}{2}\left\|v_{i+1}-x^{\star}\right\|_{2}^{2} \\\quad \leq \left(1-\theta_{i}\right)\left(f\left(x_{i}\right)-f^{\star}+\frac{\gamma_{i}}{2}\left\|v_{i}-x^{\star}\right\|_{2}^{2}\right) \quad \text { if } i\ge1</script><p>对比后发现实际上之前我们考虑的是 $f(x^+)-f^\star$ 的迭代式子，而这里我们加了一个小尾巴，考虑 $f(x^+)-f^\star + \frac{\gamma_{i+1}}{2}\left|v_{i+1}-x^{\star}\right|_{2}^{2}$ 的收敛速度。证明一会再说，有了这个迭代关系式，那么就可以有</p>
<script type="math/tex; mode=display">
\begin{aligned}f\left(x_{k}\right)-f^{\star} & \leq \lambda_{k}\left(\left(1-\theta_{0}\right)\left(f\left(x_{0}\right)-f^{\star}\right)+\frac{\gamma_{1}-m \theta_{0}}{2}\left\|x_{0}-x^{\star}\right\|_{2}^{2}\right) \\& \leq \lambda_{k}\left(\left(1-\theta_{0}\right)\left(f\left(x_{0}\right)-f^{\star}\right)+\frac{\theta_{0}^{2}}{2 t_{0}}\left\|x_{0}-x^{\star}\right\|_{2}^{2}\right)\end{aligned}</script><p>其中 $\lambda_1=1$，$\lambda_{k}=\prod_{i=1}^{k-1}\left(1-\theta_{i}\right) \text { for } k&gt;1$，如果能证明 $\lambda_k\sim O(1/k^2)$ 就能证明收敛速度了。好了，下面就是非常巧妙而又繁琐的证明过程了。</p>
<p>这个证明过程很繁琐，为了更容易顺下来，这里列出来其中几个关键的等式/不等式（为了简便省略了下标）：</p>
<ol>
<li>$\gamma^+-m\theta=(1-\theta)\gamma$（易证）</li>
<li>$\gamma^+v^+=\gamma ^+ v+ m\theta(y-v)-\theta G_t(y)$</li>
<li>$\begin{aligned}<br>f\left(x^{+}\right)-f^{\star} \leq &amp;(1-\theta)\left(f(x)-f^{\star}\right)-G_{t}(y)^{T}\left((1-\theta) x+\theta x^{\star}-y\right) -\frac{t}{2}\left|G_{t}(y)\right|_{2}^{2}-\frac{m \theta}{2}\left|x^{\star}-y\right|_{2}^{2}<br>\end{aligned}$</li>
<li>$\begin{aligned}<br>\frac{\gamma^{+}}{2}\left|v^{+}-x^{\star}\right|_{2}^{2} \leq &amp; \frac{\gamma^{+}-m \theta}{2}\left|v-x^{\star}\right|_{2}^{2}+G_{t}(y)^{T}\left(\theta x^{\star}+(1-\theta) x-y\right) +\frac{t}{2}\left|G_{t}(y)\right|_{2}^{2}+\frac{m \theta}{2}\left|x^{\star}-y\right|_{2}^{2}<br>\end{aligned}$</li>
</ol>
<p>(3,4) 条结合就能得到上面的迭代关系式，很多项刚好消掉。下面就是要证明 $\lambda_k\sim O(1/k^2)$：</p>
<script type="math/tex; mode=display">
\gamma_{k+1}=(1-\theta_k)\gamma_k+m\theta_k \\\lambda_{i+1}=\left(1-\theta_{i}\right) \lambda_{i}=\frac{\gamma_{i+1}-\theta_{i} m}{\gamma_{i}} \lambda_{i} \leq \frac{\gamma_{i+1}}{\gamma_{i}} \lambda_{i} \Longrightarrow \lambda_k\le \gamma_k/\gamma_1 \\\frac{1}{\sqrt{\lambda_{i+1}}}- \frac{1}{\sqrt{\lambda_{i}}} \ge \frac{\theta_i}{2\sqrt{\lambda_{i+1}}}=\frac{1}{2}\sqrt{\gamma_1t_i}</script><p>然后就可以有</p>
<script type="math/tex; mode=display">
\lambda_{k} \leq \frac{4}{\left(2+\sqrt{\gamma_{1}} \sum_{i=1}^{k-1} \sqrt{t_{i}}\right)^{2}}=\frac{4 t_{0}}{\left(2 \sqrt{t_{0}}+\theta_{0} \sum_{i=1}^{k-1} \sqrt{t_{i}}\right)^{2}}</script><p>如果取 $t_0=t_k=1/L,\theta_0=1$，则有</p>
<script type="math/tex; mode=display">
\lambda_k\le \frac{4}{(k+1)^2}</script><p>如果有强凸性质，也即 $m&gt;0$，那么取 $\theta_0\ge\sqrt{mt_0}\Longrightarrow \theta_k\ge \sqrt{mt_k}$</p>
<script type="math/tex; mode=display">
\lambda_k \le \Pi_{i=1}^{k-1}(1-\sqrt{mt_i})</script><p>这就可以变成线性收敛了。</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>PG 算法</tag>
        <tag>APG 算法</tag>
        <tag>FISTA 算法</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记20：对偶近似点梯度下降</title>
    <url>/2020/04/23/optimization/ch20-dual-proximal-gradient/</url>
    <content><![CDATA[<p>前面讲了梯度下降、次梯度下降、近似点梯度下降方法并分析了收敛性。一开始我们还讲了对偶原理，那么如果原问题比较难求解的时候，我们可不可以转化为对偶问题并应用梯度法求解呢？当然可以，不过有一个问题就是对偶函数的梯度或者次梯度怎么计算呢？这就是这一节要关注的问题。</p>
<p>首先一个问题是哪些形式的问题，其对偶问题相比于原问题更简单呢？可能有很多种，这一节主要关注一种：<strong>线性等式/不等式约束的优化问题</strong>。之所以考虑此类问题是因为如果有线性的等式/不等式约束，应用 Lagrange 对偶原理之后，我们可以自然的用原函数的<strong>共轭函数</strong>来表示其<strong>对偶问题</strong>，而共轭函数的次梯度是容易求解的，因为我们可以用 <strong>Legendre transformation</strong>。那么接下来就是详细的原理和例子了。</p>
<a id="more"></a>
<h2 id="1-对偶分解原理"><a href="#1-对偶分解原理" class="headerlink" title="1. 对偶分解原理"></a>1. 对偶分解原理</h2><p>假如我们考虑如下无约束优化问题，通过添加线性等式约束以及对偶原理可以容易获得对偶问题的形式</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\quad\min \ & f(x)+g(Ax) \\
(D)\quad\max \ & -g^\star(z)-f^\star(-A^Tz)
\end{aligned}</script><p>如果我们现在想对偶问题应用梯度下降方法，一个关键的问题就是如何求解 $f^\star,g^\star$ 的梯度/次梯度？会议一下前面讲共轭函数的时候提到了一些性质：</p>
<blockquote>
<p>关于共轭函数有以下性质</p>
<ol>
<li>若 $f$ 为凸的且是闭的($\text{epi }f$ 为闭集)，则 $f^{**}=f$ (可以联系上面提到一系列支撑超平面)</li>
<li>(Fenchel’s inequality) $f(x)+f^*(y)\ge x^Ty$，这可以类比均值不等式</li>
<li>(<strong>Legendre transform</strong>)如果 $f$ 且为<strong>凸的、闭的</strong>，设 $x^<em>=\arg\max\{y^Tx-f(x)\}$，那么有 $x^</em>\in\partial f^<em>(y)\iff y\in\partial f(x^</em>)$。这可以用来求极值，比如 $\min f(x)\Longrightarrow 0\in\partial f(x)\iff x\in\partial f^*(0)$</li>
</ol>
</blockquote>
<p>所以只需要找到</p>
<script type="math/tex; mode=display">
\hat{x}=\arg\max_x -z^TAx-f(x) = \arg\min_x z^TAx+f(x)</script><p>就可以有 $\hat{x}\in\partial f^\star(-A^Tz)$。如果函数 $f$ 为<strong>严格凸函数</strong>，意味着上面的 $\hat{x}$ 唯一，也说明 $\partial f^\star=\nabla f^\star$；如果条件加强，$f$ 为$\mu$-强凸函数那么就有</p>
<script type="math/tex; mode=display">
\left\|\nabla f^{*}(y)-\nabla f^{*}\left(y^{\prime}\right)\right\| \leq \frac{1}{\mu}\left\|y-y^{\prime}\right\|_{*} \quad \text { for all } y, y^{\prime}</script><p>也就是说共轭函数的梯度是 Lipschitz continuous 的，也即共轭函数 $f^\star$ 是 $1/\mu$-smooth 的，因此我们也能证明对偶问题下用梯度方法的收敛性。</p>
<p><strong><em>例子</em></strong>：现在我们把上面的 $g$ 换成一个指示函数，也即表示一个线性等式约束</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\quad\text{minimize } \quad& f(x) \\
\text{subject to } \quad& Ax=b \\
(D)\quad\text{minimize } \quad& -b^Tz-f^\star(-A^Tz)
\end{aligned}</script><p>怎么做梯度下降呢？</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+z^{T} A x\right) \\
z^{+} &=z+t(A \hat{x}-b)
\end{aligned}</script><p>上面第一步就是为了求解 $f^\star$ 的次梯度，第二部就是对 $z$ 进行梯度上升。我们再来观察一下 Lagrange 函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(x,z)&=f(x)+z^T(Ax-b) \\
&=-b^Tz+(f(x)+z^TAx)
\end{aligned}</script><p>我么前面说过最优解实际上是拉格朗日函数的鞍点，也就是关于 $x$ 的极小值点，关于 $z$ 的极大值点。而这里我们要做的两部就分别是对 $x$ 求一个极小值点 $\hat{x}$，然后对 $z$ 并没有求极大值点，而是做了一个梯度上升。</p>
<p>好了，对偶问题的梯度下降方法原理就这些，但是标题里面“对偶分解”还有一个“<strong>分解</strong>”是什么意思呢？我们说如果原问题比较复杂就可以考虑解对偶问题，那么具体是哪一种问题呢？看个例子</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize } \quad& f_1(x_1)+f_2(x_2) \\
\text{subject to } \quad& A_1x_1+A_2x_2\preceq b \\
\end{aligned}</script><p>这个问题有什么特点呢？目标函数实际上是由两个不相关的函数 $f_1(x_1),f_2(x_2)$ 求和得到的，注意不仅是 $f_1,f_2$ 不同，他们的自变量 $x_1,x_2$ 也是相互独立的，也即是说假如没有这个约束条件，我们完全可以分解为两个独立的最小化问题分别求解。但是现在<strong>由于这个约束条件，这两个问题耦合在一起了</strong>，这就有点麻烦了。那么在对偶问题中能不能解耦合呢？好消息是可以！他们的对偶问题可以写为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize } \quad& -f_1^\star(-A_1^Tz)-f_2^\star(-A_2^Tz)-b^Tz \\
\text{subject to } \quad& z\succeq 0 \\
\end{aligned}</script><p>这个时候我们只需要分别求解 $f_1^\star,f_2^\star$ 的次梯度，这两个是可以并行计算的，即下面的 $\hat{x}_1,\hat{x}_2$ 可以并行计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\hat{x}_{j}=\underset{x_{j}}{\operatorname{argmin}}\left(f_{j}\left(x_{j}\right)+z^{T} A_{j} x_{j}\right) \quad \text { for } j=1,2\\
&z^{+}=\left(z+t\left(A_{1} \hat{x}_{1}+A_{2} \hat{x}_{2}-b\right)\right)_{+}
\end{aligned}</script><p>所以每次迭代我们都可以先并行计算 $\hat{x}_j$，然后把他们传给中心节点，中心节点再对 $z$ 计算梯度上升。需要注意的一点是这里对 $z^+$ 只取正值，这是因为有 $z\succeq 0$ 的约束，从另一个角度理解也是向 $C=\{z|z\succeq0\}$ 做了投影（回忆上一节的近似点算子与投影的关系，以及近似点梯度下降实际上就是先算梯度再做投影）。</p>
<p><strong><em>例子  1</em></strong>：来看一个二次优化的例子，假设其中的 $P_j\succ0$</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\text { minimize } & \sum_{j=1}^{r}\left(\frac{1}{2} x_{j}^{T} P_{j} x_{j}+q_{j}^{T} x_{j}\right) \\
\text { subject to } & B_{j} x_{j} \preceq d_{j}, \quad j=1, \ldots, r \\
& \sum_{j=1}^{r} A_{j} x_{j} \preceq b
\end{array}</script><p>这里约束条件比较多，我们可以转化一下，考虑 $f_j(x_j)=\frac{1}{2} x_{j}^{T} P_{j} x_{j}+q_{j}^{T} x_{j}$，定义域为 $\{x_j|B_jx_j\preceq d_j\}$。对偶问题就变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize } \quad& -b^Tz-\sum_{j=1}^r f_j^\star(-A_j^Tz) \\
\text{subject to } \quad& z\succeq 0 \\
\end{aligned}</script><p>为了保证梯度下降方法的收敛性，还需要验证目标函数的 Lipschitz smooth 性质，考虑 $h(z)=\sum_j f_j^\star(-A_j^Tz)$，有</p>
<script type="math/tex; mode=display">
\left\|\nabla h(z)-\nabla h\left(z^{\prime}\right)\right\|_{2} \leq \frac{\|A\|_{2}^{2}}{\min _{j} \lambda_{\min }\left(P_{j}\right)}\left\|z-z^{\prime}\right\|_{2}</script><p>其中 $A=[\begin{array}{ccc}A_1&amp;\cdots&amp;A_r\end{array}]$。那么怎么求 $\hat{x}_j=\partial f_j^\star(-A_j^Tz)$ 呢？就是求解下面的优化问题</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\text { minimize } & \frac{1}{2} x_{j}^{T} P_{j} x_{j}+(q_{j}+A_j^Tz)^T x_{j} \\
\text { subject to } & B_{j} x_{j} \preceq d_{j}
\end{array}</script><p><strong><em>例子 2</em></strong>：网络优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P) \quad \text { maximize } \quad& \sum_{j=1}^{n} U_{j}\left(x_{j}\right)\\
\text { subject to } \quad& R x \leq c\\
(D) \quad \text { minimize } \quad& c^{T} z+\sum_{j=1}^{n}\left(-U_{j}\right)^{*}\left(-r_{j}^{T} z\right)\\
\text { subject to } \quad& z \geq 0
\end{aligned}</script><p>只需要将 $R$ 的各列拆分开就行了。</p>
<h2 id="2-对偶近似点梯度方法"><a href="#2-对偶近似点梯度方法" class="headerlink" title="2. 对偶近似点梯度方法"></a>2. 对偶近似点梯度方法</h2><p>这节的一开始我们考虑的问题是</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\quad\min \ & f(x)+g(Ax) \\
(D)\quad\min \ & g^\star(z)+f^\star(-A^Tz)
\end{aligned}</script><p>不过举得几个例子中都没有见到 $g$ 的影子，这是因为我们都加了线性等式/不等式约束，也就等价于 $g=\delta_C(x)$ 是一个指示函数的形式。那如果考虑一般的(不光滑的) $g$ 呢？很简单，我们本质上还是在做梯度下降(上升)，如果函数不光滑，就可以用上一篇文章的近似点梯度方法，还记得近似点梯度下降的公式吗？</p>
<script type="math/tex; mode=display">
z^{+}=\operatorname{prox}_{t g^{*}}\left(z+t A \nabla f^{*}\left(-A^{T} z\right)\right)</script><p>实际上刚刚举得几个例子也都是在做近似点梯度下降，只不过对于指示函数的近似点就是在做投影，比较简单。所以对上面的问题求解实际上就是两步：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+z^{T} A x\right) \\
z^{+} &=\operatorname{prox}_{t g^{*}}(z+t A \hat{x})
\end{aligned}</script><p>如果有时候 $g^\star$ 的近似点不好计算，也可以利用 Moreau 分解(参见上上一节近似点算子)，那么可以计算</p>
<script type="math/tex; mode=display">
z^{+} =z+tA\hat{x}-t\operatorname{prox}_{t^{-1} g}(t^{-1}z+A \hat{x})</script><p>这个式子其实可以跟增广拉格朗日方法(后面会讲)联系起来。我们可以令 $\hat{y}$ 为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{y} &=\operatorname{prox}_{t^{-1} g}\left(t^{-1} z+A \hat{x}\right) \\
&=\underset{y}{\operatorname{argmin}}\left(g(y)+\frac{t}{2}\left\|A \hat{x}-t^{-1} z-y\right\|_{2}^{2}\right) \\
&=\underset{y}{\operatorname{argmin}}\left(g(y)+z^{T}(A \hat{x}-y)+\frac{t}{2}\|A \hat{x}-y\|_{2}^{2}\right)
\end{aligned}</script><p>那么就有 $z^{+}=z+t(A\hat{x}-\hat{y})$，所以这里 $\hat{y}$ 一定程度上可以理解为 $g^\star(z)$ 的次梯度。把前面的分析综合起来，我们梯度下降的每次迭代过程实际上可以由下面 3 个步骤组成</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+z^{T} A x\right) \\
\hat{y} &=\underset{y}{\operatorname{argmin}}\left(g(y)-z^{T} y+\frac{t}{2}\|A \hat{x}-y\|_{2}^{2}\right) \\
z^{+} &=z+t(A \hat{x}-\hat{y})
\end{aligned}</script><p>这个时候可以跟<strong>增广拉格朗日方法(augmented Lagrangian method)</strong>比较一下，ALM 的计算公式为</p>
<script type="math/tex; mode=display">
(\hat{x}, \hat{y})=\underset{x, y}{\operatorname{argmin}}\left(f(x)+g(y)+z^{T}(A x-y)+\frac{t}{2}\|A x-y\|_{2}^{2}\right)</script><p>首先 ALM 要优化的这个增广拉格朗日函数函数是在普通拉格朗日函数的基础上加了一个最后的二阶项，然后对增广拉函数优化 $x,y$。而前面的对偶近似梯度方法是怎么做呢？观察增广拉函数实际上可以分解为两项</p>
<script type="math/tex; mode=display">
L(x,y,z)=\left(f(x)+z^{T}A x\right)+\left(g(y)-z^Ty+\frac{t}{2}\|A x-y\|_{2}^{2}\right)</script><p>而对偶近似梯度方法实际上就是先优化第一项，只考虑 $x$，计算得到 $\hat{x}$ 以后再代入到第二项单独优化 $y$ 得到 $\hat{y}$，最后对 $z$ 做梯度上升！巧妙不巧妙！显然对偶近似梯度方法计算起来更简单，但是也有代价，那就是 ALM 并不要求函数 $f$ 是强凸的，而对偶近似梯度方法则要求 $f$ 为强凸的。</p>
<p>基本的理论就完了，下面主要是一些例子。</p>
<p><strong><em>例子 1</em></strong>：$g$ 为范数正则项</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P)\quad\text{minimize}\quad& f(x)+\|A x-b\| \\
(D)\quad\text{maximize} \quad& -b^{T} z-f^{*}\left(-A^{T} z\right)\\
\text{subject to} \quad& \|z\|_{*} \leq 1
\end{aligned}</script><p>我们可以取 $g(y)=\Vert y-b\Vert$</p>
<script type="math/tex; mode=display">
g^{*}(z)=\left\{\begin{array}{ll}
b^{T} z & \|z\|_{*} \leq 1 \\
+\infty & \text { otherwise }
\end{array} \quad \operatorname{prox}_{t g *}(z)=P_{C}(z-t b)\right.</script><p>对偶近似梯度下降方法每次迭代过程为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+z^{T} A x\right) \\
z^{+} &=P_{C}(z+t(A \hat{x}-b))
\end{aligned}</script><p><strong><em>例子 2(重要)</em></strong>：前面都只有 1 个 $g$，这次考虑 $g$ 为多个 $g_i$ 求和</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P) \quad \text { minimize } \quad& f(x)+\sum_{i=1}^{p}\left\|B_{i} x\right\|_{2}\\
(D) \quad \text { maximize } \quad& -f^{*}\left(-B_{1}^{T} z_{1}-\cdots-B_{p}^{T} z_{p}\right)\\
\text { subject to }\quad& \left\|z_{i}\right\|_{2} \leq 1, \quad i=1, \ldots, p
\end{aligned}</script><p>这个推导就有点麻烦了，我们考虑一般的情况 $g(x)=g_1(B_1x)+…+g_p(B_px)$，那么可以取 $B=[B_1;…;B_p]$(排成一列)，$g(x)=\hat{g}(Bx)$，那么原问题实际上等价于 </p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize } \quad& f(x)+\hat{g}(y) \\
\text{subject to } \quad& y=Bx \\
\end{aligned}</script><p>拉格朗日函数为 $L(x,y,z)=f(x)+\sum_i g_i(y_i) + \sum_i z_i^T(B_ix-y_i)$，对偶问题就变成了</p>
<script type="math/tex; mode=display">
\text{maximize}\quad -f^\star(-\sum_i B_i^Tz_i) - \sum_i g_i^\star(z_i)</script><p>注意到对偶问题中 $g^\star(z)=\sum_i g^\star_i(z_i)$，利用近似点算子公式就可以得到</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{tg^\star}(x)=\left[\begin{array}{c}\operatorname{prox}_{tg^\star_1}(x_1)\\ \vdots \\ \operatorname{prox}_{tg^\star_p}(x_p) \end{array}\right]</script><p>所以我们的 $z_i^+$ 之间的计算是互不相关的，可以并行进行，也即下面的式子中 $\hat{x}$此时不能并行计算了，但是 $z_i^+$可以分别计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{x} &=\underset{x}{\operatorname{argmin}}\left(f(x)+\left(\sum_{i=1}^{p} B_{i}^{T} z_{i}\right)^{T} x\right) \\
z_{i}^{+} &=P_{C_{i}}\left(z_{i}+t B_{i} \hat{x}\right), \quad i=1, \ldots, p
\end{aligned}</script><p>注意第一部分当中我们考虑 $f_1(x_1)+f_2(x_2)$ 形式的优化问题，这使得对偶问题可以分解为 $\sum_i f^\star(-A_i^Tz)$，可以并行计算 $\hat{x}_i$，而这里我们考虑 $\sum_i g(x)$ 则对偶问题可以表示为 $\sum_i g^\star(z_i)$，这使得计算近似点梯度的时候可以对 $z_i^+$ 并行计算，非常的对称！</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>共轭函数</tag>
        <tag>PG 算法</tag>
        <tag>拉格朗日函数</tag>
        <tag>近似点算子</tag>
        <tag>ALM</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记19：近似点梯度下降</title>
    <url>/2020/04/17/optimization/ch19-proximal-gradient/</url>
    <content><![CDATA[<p>前面讲了梯度下降法、次梯度下降法，并分析了他们的收敛性。上一节讲了近似梯度算子，我们说主要是针对非光滑问题的，这一节就要讲近似梯度算子在非光滑优化问题中的应用。先回顾一下上一节最重要的一部分内容：对于指示函数 $\delta_C$ 来说近似梯度算子得到的实际上就是向集合 $C$ 的投影。</p>
<h2 id="1-近似点梯度下降"><a href="#1-近似点梯度下降" class="headerlink" title="1. 近似点梯度下降"></a>1. 近似点梯度下降</h2><p>这一部分考虑的问题主要是</p>
<script type="math/tex; mode=display">
\text{minimize } f(x)=g(x)+h(x)</script><p>这里面 $g$ 是全空间可导的凸函数，$\text{dom }g=R^n$，$h$ 是存在不可导部分的凸函数，并且一般需要 $h$ 的近似点计算较为简单。近似点梯度下降算法是什么呢？</p>
<script type="math/tex; mode=display">
x_{k+1}=\text{prox}_{th}(x_k-t_k\nabla g(x_k))</script><a id="more"></a>
<p>这里跟之前的梯度下降(GD)和次梯度下降(SD)的形式都不太一样，实际上看了后面的推导会发现经过转换他们还是很像的。不过怎么理解这个式子呢？举一个例子，假如 $h$ 是集合 $C$ 的指示函数，那么这个式子实际上是先沿着 $g$ 的梯度走步长 $t_k$，然后再投影到集合 $C$ 里面，可以看下面这张图。而考虑原始优化问题，$\min f=g+h$ 本身是一个无约束优化问题，但实际上把 $h$ 用一个约束函数表示，他就是一个带约束的优化问题 $\min g(x),\text{ s.t. }x\in C$，而近似点梯度下降方法要做的事情就是先优化 $g$，然后投影到约束区域 $C$ 中，可以参考下图。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/19-proximal-gradient.PNG" alt="19-proximal-gradient"></p>
<p>根据 $\text{prox}_{th}$ 的定义，我们把上面的式子展开可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
x^{+} &=\underset{u}{\operatorname{argmin}}\left(h(u)+\frac{1}{2 t}\|u-x+t \nabla g(x)\|_{2}^{2}\right) \\
&=\underset{u}{\operatorname{argmin}}\left(h(u)+g(x)+\nabla g(x)^{T}(u-x)+\frac{1}{2 t}\|u-x\|_{2}^{2}\right)
\end{aligned}</script><p>可以发现括号里面的式子实际上就是在 $x$ 附近对光滑的 $g$ 进行了二阶展开，而 $x^+$ 就是对近似后函数取最小值点。再进一步地</p>
<script type="math/tex; mode=display">
0\in t\partial h(x^+) + (x^+-x+t\nabla g(x)) \\
\Longrightarrow G_t(x):=\frac{x-x^+}{t}\in \partial h(x^+)+\nabla g(x)</script><p>可以发现 $G_t(x)=\partial h(x^+)+\nabla g(x)$ 实际上就近似为函数 $f$ 的次梯度，但并不严格是，因为 $\partial f(x)=\partial h(x)+\nabla g(x)$。而此时我们也可以将 $x^+$ 写成比较简单的形式</p>
<script type="math/tex; mode=display">
x^+ = x-tG_t(x)</script><p>这跟之前的梯度下降法就统一了，并且也说明了 $G_t(x)$ 就相当于是 $f$ 的梯度。</p>
<p>这里还需要说明的一点是 $G_t(x)=(1/t)(x-\text{prox}_{th}(x-t\nabla g(x))$ 这是一个连续函数，这是因为近似点算子是 Lipschitz 连续的(在下面一小节中会解释说明)，又由于 $G_t(x)=0\iff x=\arg\min f(x)$，因此 $\Vert x-x^+\Vert\le \varepsilon$ 就可以作为 stopping criterion。与之成对比的是非光滑函数的次梯度下降，$\Vert x-x^+\Vert$ 就不是一个很好的 stopping criterion，因为即使 $\Vert x-x^+\Vert$ 很小，也可能离最优解比较远。</p>
<h2 id="2-收敛速度分析"><a href="#2-收敛速度分析" class="headerlink" title="2. 收敛速度分析"></a>2. 收敛速度分析</h2><p>在分析收敛速度之前，我们需要首先分析一下 $g(x)$ 和 $h(x)$ 这两部分函数的性质。</p>
<p>首先是 $h$，如果 $h$ 为闭的凸函数，那么 $\text{prox}_h(x)=\arg\min_u\left(h(u)+(1/2)\Vert u-x\Vert^2\right)$ 对每个 $x$ 一定存在唯一的解。并且 $u=\text{prox}_h(x) \iff x-u\in \partial h(u)$，那么我们就可以得到 <strong>ﬁrmly nonexpansive(co-coercivite)</strong> 性质：</p>
<script type="math/tex; mode=display">
\left(\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right)^{T}(x-y) \geq\left\|\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right\|_{2}^{2}</script><p>证明过程可以取 $u=\text{prox}_h(x),v=\text{prox}_h(y)$，然后根据 $x-u\in \partial h(u),y-v\in \partial h(v)$，再利用次梯度算子的单调性质就可以得到。之前在梯度下降法中第一次讲到 co-coercive 性质的时候也提到，他跟 Lipschitz continuous 性质实际上是等价的，因此我们也有(<strong>nonexpansiveness</strong>性质)</p>
<script type="math/tex; mode=display">
\left\|\operatorname{prox}_{h}(x)-\operatorname{prox}_{h}(y)\right\|_2 \le \left\|x-y\right\|_2</script><p>然后我们再来看函数 $g$ 的性质，类似前面梯度下降法中的两个重要性质：</p>
<ol>
<li><strong>L-smooth</strong>：$\frac{L}{2}x^Tx-g(x)$ convex</li>
<li><strong>m-strongly convex</strong>：$g(x)-\frac{m}{2}x^Tx$ convex</li>
</ol>
<p>然后就可以得到两个二次的界</p>
<script type="math/tex; mode=display">
\frac{m t^{2}}{2}\left\|G_{t}(x)\right\|_{2}^{2} \leq g\left(x-t G_{t}(x)\right)-g(x)+t \nabla g(x)^{T} G_{t}(x) \leq \frac{L t^{2}}{2}\left\|G_{t}(x)\right\|_{2}^{2}</script><p>如果取 $0&lt; t\le 1/L$，那么就有 $Lt\le1,mt\le 1$。</p>
<p>结合上面对 $g$ 和 $h$ 性质的分析，就能得到下面这个<strong>非常重要</strong>的式子：</p>
<blockquote>
<script type="math/tex; mode=display">
f\left(x-t G_{t}(x)\right) \leq f(z)+G_{t}(x)^{T}(x-z)-\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}-\frac{m}{2}\|x-z\|_{2}^{2} \qquad (\bigstar)</script><p><strong>证明</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x-t G_{t}(x)\right) & \\
\leq & g(x)-t \nabla g(x)^{T} G_{t}(x)+\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}+h\left(x-t G_{t}(x)\right) \\
\leq & g(z)-\nabla g(x)^{T}(z-x)-\frac{m}{2}\|z-x\|_{2}^{2}-t \nabla g(x)^{T} G_{t}(x)+\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2} \\
&+h\left(x-t G_{t}(x)\right) \\
\leq & g(z)-\nabla g(x)^{T}(z-x)-\frac{m}{2}\|z-x\|_{2}^{2}-t \nabla g(x)^{T} G_{t}(x)+\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2} \\
&+h(z)-\left(G_{t}(x)-\nabla g(x)\right)^{T}\left(z-x+t G_{t}(x)\right) \\
=& g(z)+h(z)+G_{t}(x)^{T}(x-z)-\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}-\frac{m}{2}\|x-z\|_{2}^{2}
\end{aligned}</script><p>其中第一个不等号用到了 $g(x)$ 凸函数以及 Lipschitz 连续的性质，第二个不等号用到了 $g(x)$ 凸函数的性质，第三个不等号用到了 $h(x)$ 凸函数的性质。</p>
</blockquote>
<p>有了上面这个式子就可以分析收敛性了。</p>
<p>如果我们取 $z=x$，那么就有下面的式子，说明序列 $\{f(x_k\}$ 总是在减小的，如果 $f(x)$ 存在下界，那么 $f(x_k)$ 将趋向于这个下界。</p>
<script type="math/tex; mode=display">
f(x^+)\le f(x)-\frac{t}{2}\Vert G_t(x)\Vert^2</script><p>如果我们取 $z=x^\star$，那么就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x^{+}\right)-f^{\star} & \leq G_{t}(x)^{T}\left(x-x^{\star}\right)-\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}-\frac{m}{2}\left\|x-x^{\star}\right\|_{2}^{2} \\
&=\frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x-x^{\star}-t G_{t}(x)\right\|_{2}^{2}\right)-\frac{m}{2}\left\|x-x^{\star}\right\|_{2}^{2} \\
&=\frac{1}{2 t}\left((1-m t)\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}\right) \\
& \leq \frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}\right)
\end{aligned}</script><p>从这个式子就可以看出来很多有用的性质了：</p>
<ol>
<li>$\left|x^{+}-x^{\star}\right|_{2}^{2}\le (1-m t)\left|x-x^{\star}\right|_{2}^{2}$，如果满足强凸性质的话，也即 $m&gt;0$，就有 $\left|x^{+}-x^{\star}\right|_{2}^{2}\le c^k\left|x-x^{\star}\right|_{2}^{2},c=1-m/L$；</li>
<li>$\sum_i^k (f(x_i)-f^\star) \le \frac{1}{2t}\left|x^{+}-x^{\star}\right|_{2}^{2}$，由于 $f(x_i)$ 不增，因此 $f(x_k)-f^\star \le \frac{1}{2kt}\left|x^{+}-x^{\star}\right|_{2}^{2}$，因此收敛速度也是 $O(1/k)$。</li>
</ol>
<p>注意到前面的分析是针对固定步长 $t\in(0,1/L]$ 的，如果我们想走的更远一点，下降的快一点呢？就可以用前几节提到的线搜索方法。也就是说每次选择步长 $t_k$ 的时候需要迭代 $t:=\beta t$ 来进行搜索，使得满足下面的式子</p>
<script type="math/tex; mode=display">
g\left(x-t G_{t}(x)\right) \leq g(x)-t \nabla g(x)^{T} G_{t}(x)+\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}</script><p>这个式子就是 Lipschitz 连续导出的二次上界，注意应用线搜索的时候，每次迭代我们都要额外计算一次 $g$ 和 $\text{prox}_{th}$，这个计算可能并不简单，因此不一定会使算法收敛更快，需要慎重考虑。另外为了保证能在有限步停止搜索 $t_k$，还需要加入最小步长的约束 $t\ge t_{\min}=\min \{\hat{t},\beta/L\}$。线搜索直观理解可以如下图所示</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/19-line-search.PNG" alt="19-line-search"></p>
<p>我们再来分析一下收敛性，跟前面固定步长很像，只需要将原来的式子中 $t$ 替换为 $t_i$，就可以得到</p>
<script type="math/tex; mode=display">
t_{i}\left(f\left(x_{i+1}\right)-f^{\star}\right) \leq \frac{1}{2}\left(\left\|x_{i}-x^{\star}\right\|_{2}^{2}-\left\|x_{i+1}-x^{\star}\right\|_{2}^{2}\right)</script><p>于是有</p>
<ol>
<li>$\left|x^{+}-x^{\star}\right|_{2}^{2}\le (1-m t_i)\left|x-x^{\star}\right|_{2}^{2}\le (1-m t_{\min})\left|x-x^{\star}\right|_{2}^{2}$，如果满足强凸性质的话，也即 $m&gt;0$，就有 $\left|x^{+}-x^{\star}\right|_{2}^{2}\le c^k\left|x-x^{\star}\right|_{2}^{2},c=1-mt_{\min}=\max \{1-\beta m/L,1-m\hat{t}\}$；</li>
<li>$\sum_i^k t_i(f(x_i)-f^\star) \le \frac{1}{2}\left|x^{+}-x^{\star}\right|_{2}^{2}$，由于 $f(x_i)$ 不增，因此 $f(x_k)-f^\star \le \frac{1}{2kt_{\min}}\left|x^{+}-x^{\star}\right|_{2}^{2}$，因此收敛速度也是 $O(1/k)$。</li>
</ol>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>PG 算法</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记18：近似点算子 Proximal Mapping</title>
    <url>/2020/04/16/optimization/ch18-proximal-mapping/</url>
    <content><![CDATA[<p>前面讲了梯度下降法，分析了其收敛速度，对于存在不可导的函数介绍了次梯度的计算方法以及次梯度下降法，这一节要介绍的内容叫做<strong>近似点算子(Proximal mapping)</strong>，也是为了处理非光滑问题。</p>
<a id="more"></a>
<h2 id="1-闭函数"><a href="#1-闭函数" class="headerlink" title="1. 闭函数"></a>1. 闭函数</h2><p>在引入<strong>闭函数(closed function)</strong>的概念之前，我们先回顾一下<strong>闭集</strong>的概念：集合 $\mathcal{C}$ 是闭的，如果它包含边界，也即</p>
<script type="math/tex; mode=display">
x^{k} \in \mathcal{C}, \quad x^{k} \rightarrow \bar{x} \quad \Rightarrow \quad \bar{x} \in \mathcal{C}</script><p>并且有以下几个简单的原则可以保持集合闭的性质：</p>
<ol>
<li>闭集的<strong>交集</strong>还是闭集；</li>
<li><strong>有限个</strong>闭集的<strong>并集</strong>还是闭集；</li>
<li>如果 $\mathcal{C}$ 是闭集，则<strong>线性映射</strong>的<strong>原象</strong>也是闭集，也即 $\{x|Ax\in\mathcal{C}\}$ 是闭集。</li>
</ol>
<p>第 3 条原则反过来则不一定成立，也即如果 $x\in\mathcal{C}$ 是闭集，那么 $\{Ax|x\in\mathcal{C}\}$ 则不一定是闭集，比如我们可以取函数 $f(x)=1/x$ 的 epigraph 为闭集 $\mathcal{C}$，然而 $(x,y)$ 向 $x$ 轴的投影则是一个开集，严格表示与图示如下</p>
<script type="math/tex; mode=display">
\mathcal{C}=\left\{\left(x_{1}, x_{2}\right) \in \mathbb{R}_{+}^{2} | x_{1} x_{2} \geq 1\right\}, \quad A=[1,0], A \mathcal{C}=\mathbb{R}_{++}</script><div class="table-container">
<table>
<thead>
<tr>
<th>第3条逆原则反例</th>
<th>第3条逆原则充分条件</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/18-closed-set.png" alt="counter example"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/18-closed-set2.PNG" alt="sufficient condition"></td>
</tr>
</tbody>
</table>
</div>
<p>当然，如果加一些其他的约束条件，则可以保证第 3 条反过来也成立：$A\mathcal{C}$ 是闭的，如果</p>
<ol>
<li>$\mathcal{C}$ 是闭的且为凸集；</li>
<li>并且 $\mathcal{C}$ 不存在一个可以无穷延伸的方向(recession direction)属于 $A$ 的零空间，也即 $A y=0, \hat{x} \in \mathcal{C}, \hat{x}+\alpha y \in \mathcal{C}, \forall \alpha&gt;0 \Rightarrow y=0$，图示即如上。</li>
</ol>
<p>然后我们就可以定义<strong>闭函数(closed function)</strong>了，函数 $f$ 为闭的，如果他的 epigraph 为闭集或者他的所有下水平集为闭集。有以下两种简单的特殊情况：</p>
<ol>
<li>如果 $f$ 连续且定义域 $\text{dom}f$ 为闭的，则 $f$ 为闭函数；</li>
<li>如果 $f$ 连续且定义域 $\text{dom}f$ 为开的，则 $f$ 为闭函数<strong>当且仅当</strong>其在 $\text{dom}f$ 边界处收敛至 $\infty$。</li>
</ol>
<p><strong><em>例子 1</em></strong>：$f(x)=x\log x,\quad\text{dom}f=R_+,f(0)=0$</p>
<p><strong><em>例子 2</em></strong>：闭集的指示函数 $\delta_C(x)=\begin{cases}0&amp;x\in C\\ +\infty &amp; o.w.\end{cases}$</p>
<p><strong><em>反例 3</em></strong>：$f(x)=x\log x,\quad\text{dom}f=R_{++}$ 或者 $f(x)=x\log x,\quad\text{dom}f=R_+,f(0)=1$ 不是闭函数</p>
<p><strong><em>反例 4</em></strong>：开集的指示函数不是闭函数</p>
<p>闭函数有一些有用的性质，比如：</p>
<ol>
<li>$f$ 为闭函数<strong>当且仅当</strong>他的所有下水平集都是闭集；</li>
<li>如果 $f$ 为闭函数，且下水平集有界，那么存在<strong>最小值点(minimizer)</strong>。</li>
</ol>
<p><strong>Theorem (Weierstrass) </strong>：假设集合 $D\subset \mathcal{E}$ ($R^n$空间中有限维向量子空间) 非空且闭，并且连续函数 $f:D\to R$ 的所有下水平集都有界，则 $f$ 存在<strong>全局最小值点(global minimizer)</strong>。</p>
<p>对于闭函数来说也有一些原则可以保持闭的性质：</p>
<ol>
<li>如果 $f,g$ 均为闭函数，则 $f+g$ 为闭函数</li>
<li>如果 $f$ 为闭函数，则 $f(Ax+b)$ 为闭函数</li>
<li>如果任意 $f_\alpha$ 都是闭函数，则 $\sup_\alpha f_\alpha(x)$ 为闭函数</li>
</ol>
<h2 id="2-共轭函数"><a href="#2-共轭函数" class="headerlink" title="2. 共轭函数"></a>2. 共轭函数</h2><p><strong>共轭函数(conjugate function)</strong> 前面已经讲过了，这里再简单回顾一遍。函数 $f$ 的共轭函数定义为</p>
<script type="math/tex; mode=display">
f^\star(y)=\sup_{x\in\text{dom}f} (y^Tx-f(x))</script><blockquote>
<p>并且共轭函数有一些重要的性质：</p>
<ol>
<li>共轭函数一定是闭函数，且为凸函数，不论 $f$ 是否为凸函数或闭函数（因为 $f^\star$ 的 epigraph 可以看成很多个半空间的交集）；</li>
<li><strong>(Fenchel’s inequality)</strong> $f(x)+f^{\star}(y) \geq x^{\top} y, \forall x, y$</li>
<li><strong>(Legendre transform)</strong> 如果 $f$ 为凸函数且为闭函数，则有 $y \in \partial f(x) \Leftrightarrow x \in \partial f^{\star}(y) \Leftrightarrow x^{\top} y=f(x)+f^{\star}(y)$</li>
<li>如果 $f$ 为凸函数且为闭函数，则 $f^{\star\star}=f$</li>
</ol>
<p>除此之外还有一些代数变换的原则，推导也都比较简单：</p>
<ol>
<li>$f\left(x_{1}, x_{2}\right)=g\left(x_{1}\right)+h\left(x_{2}\right), \quad f^{\star}\left(y_{1}, y_{2}\right)=g^{\star}\left(y_{1}\right)+h^{\star}\left(y_{2}\right)$</li>
<li>$f(x)=\alpha g(x), \quad f^{\star}(y) {=} \alpha g^{\star}(y / \alpha) \quad(\bigstar)$</li>
<li>$f(x)=g(x)+a^{\top} x+b \quad f^{\star}(y)=g^{\star}(y-a)-b$</li>
<li>$f(x)=\inf _{u+v=x}(g(u)+h(v)) \quad f^{\star}(y)=g^{\star}(y)+h^{\star}(y)$</li>
</ol>
</blockquote>
<p>共轭函数的计算就不多举例子了，这里主要列出来后面用的比较多的而且比较重要的，其他的可以参考前面的笔记 6：</p>
<p><strong><em>例子 1</em></strong>：$C$ 为凸集，则<strong>指示函数</strong> $f(x)=\delta_C(x)$，其共轭函数为<strong>支撑函数</strong></p>
<script type="math/tex; mode=display">
f^\star(y) = \sup\{y^Tx|x\in C\}</script><p>如果求两次共轭函数也很容易得到：支撑函数的共轭函数为指示函数。</p>
<p><strong><em>例子 2</em></strong>：范数 $f(x)=\Vert x\Vert$ 的共轭函数也是<strong>指示函数</strong></p>
<script type="math/tex; mode=display">
f^\star(y) = \left\{\begin{array}{ll}
0 & \|y\|_{*} \leq 1 \\
\infty & \text { otherwise }
\end{array}\right.</script><h2 id="3-近似点算子"><a href="#3-近似点算子" class="headerlink" title="3. 近似点算子"></a>3. 近似点算子</h2><p>首先给出来<strong>近似点算子(Proximal mapping)</strong>的定义：<strong>闭凸函数</strong> $f$ 的近似点算子定义为</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{f}(x)=\underset{u}{\operatorname{argmin}}\left(f(u)+\frac{1}{2}\|u-x\|_{2}^{2}\right)</script><p>根据这个定义，实际上我们是在求解函数 $g(u)=f(u)+\frac{1}{2}|u-x|_{2}^{2}$ 的最小值，由于 $g$ 是闭函数且下水平集有界，因此最小值一定<strong>存在</strong>；同时由于 $g$ 为<strong>强凸函数</strong>，因此最小值点<strong>唯一</strong>。</p>
<p>那么怎么理解这个算子函数 $\text{prox}_f(x)$ 呢？可以看到这实际上是一个 $\text{prox}_f:R^n\to R^n$ 的映射。如果 $u=\text{prox}_f(x)$，则应该有 $x-u\in \partial f(u)$。下面看一些简单的例子。</p>
<p><strong><em>例子 1</em></strong>：二次函数 $A\succeq 0$</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{2} x^{T} A x+b^{T} x+c, \quad \operatorname{prox}_{t f}(x)=(I+t A)^{-1}(x-t b)</script><p><strong><em>例子 2</em></strong>：欧几里得范数 $f(x)=\Vert x\Vert_2$</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{t f}(x)=\left\{\begin{array}{ll}
\left(1-t /\|x\|_{2}\right) x & \|x\|_{2} \geq t \\
0 & \text { otherwise }
\end{array}\right.</script><p><strong><em>例子 3</em></strong>：Logarithmic barrier</p>
<script type="math/tex; mode=display">
f(x)=-\sum_{i=1}^{n} \log x_{i}, \quad \operatorname{prox}_{t f}(x)_{i}=\frac{x_{i}+\sqrt{x_{i}^{2}+4 t}}{2}, \quad i=1, \ldots, n</script><blockquote>
<p>上面是比较简单的例子，近似点算子也有一些很容易验证的代数运算规律：</p>
<ol>
<li>$f\left(\left[\begin{array}{l} x \\ y \end{array}\right]\right)=g(x)+h(y), \quad \operatorname{prox}_{f}\left(\left[\begin{array}{l} x \\ y<br>\end{array}\right]\right)=\left[\begin{array}{l}<br>\operatorname{prox}_{g}(x) \\<br>\operatorname{prox}_{h}(y)<br>\end{array}\right]$</li>
<li>$f(x)=g(a x+b), \quad \operatorname{prox}_{f}(x)=\frac{1}{a}\left(\operatorname{prox}_{a^{2} g}(a x+b)-b\right)$ (注意 $a\ne0$ 是标量)</li>
<li>$f(x)=\lambda g(x / \lambda), \quad \operatorname{prox}_{f}(x)=\lambda \operatorname{prox}_{\lambda^{-1} g}(x / \lambda) \quad(\bigstar)$</li>
<li>$f(x)=g(x)+a^{T} x, \quad \quad \operatorname{prox}_{f}(x)=\operatorname{prox}_{g}(x-a)$</li>
<li>$f(x)=g(x)+\frac{\mu}{2}|x-a|_{2}^{2}, \quad \operatorname{prox}_{f}(x)=\operatorname{prox}_{\theta g}(\theta x+(1-\theta) a)$，其中 $\mu&gt;0,\theta=1/(1+\mu)$</li>
<li>$f(x)=g(Ax+b)$，对于一般的 $A$ 并不能得到比较好的性质，但如果 $AA^T=(1/\alpha)I$，则有 </li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}\operatorname{prox}_{f}(x) &=\left(I-\alpha A^{T} A\right) x+\alpha A^{T}\left(\operatorname{prox}_{\alpha^{-1} g}(A x+b)-b\right) \\&=x-\alpha A^{T}\left(A x+b-\operatorname{prox}_{\alpha^{-1} g}(A x+b)\right)\end{aligned}</script><p>前面几条都比较容易证明，最后一条证明可以等价于求解</p>
<script type="math/tex; mode=display">
\begin{aligned}\text { minimize } \quad& g(y)+\frac{1}{2}\|u-x\|_{2}^{2}\\\text { subject to } \quad& A u+b=y\end{aligned}</script><p>可以先求解 $x$ 向超平面 $Au+b=y$ 投影来消去 $u$，然后再计算 $\text{prox}_f(y)$。</p>
</blockquote>
<p>除此之外，有一个非常重要的等式：</p>
<blockquote>
<p><strong>Moreau decomposition</strong>：</p>
<script type="math/tex; mode=display">
x=\operatorname{prox}_{f}(x)+\operatorname{prox}_{f^{*}}(x) \quad\text { for all } x</script></blockquote>
<p><strong>Remarks</strong>：为什么说这个式子重要呢？因为他把原函数和共轭函数的 proximal mapping 联系起来了，如果其中一个比较难计算，那么我们可以通过另一个来计算。这个式子可以怎么理解呢？可以看成是一种正交分解，举个栗子，如果我们取一个子空间 $L$，他的正交空间为 $L^\perp$，令函数 $f$ 为子空间 $L$ 的指示函数也即 $f=\delta_L$，那么很容易验证共轭函数 $f^\star=\delta_{L^\perp}$。而根据定义也可以得到 $\text{prox}_f(x)$ 恰好就是 $x$ 在子空间 $L$ 上的投影，记为 $P_L(x)=\text{prox}_f(x)$，同样的 $P_{L^\perp}(x)=\text{prox}_{f^\star}(x)$，因此上面的 Moreau decomposition 就可以写为 $x=P_L(x)+P_{L^\perp}(x)$，这正好就是一个正交分解。可以根据下图理解</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/18-moreau.PNG" alt="moreau decomposition"></p>
<p>如果对原始的 Moreau decomposition 做简单的代数变换，就可以得到 $\lambda&gt;0$</p>
<script type="math/tex; mode=display">
x=\operatorname{prox}_{\lambda f}(x)+\lambda \operatorname{prox}_{\lambda^{-1} f^{*}}(x / \lambda) \quad \text { for all } x</script><p>证明过程用到了共轭函数的性质 $(\lambda f)^{\star}(y)=\lambda f^{\star}(y / \lambda)$。</p>
<p>后面两个小节则主要是近似点算子的应用，一个是计算投影，另一个是与支撑函数、距离相关的内容。</p>
<h2 id="4-投影"><a href="#4-投影" class="headerlink" title="4. 投影"></a>4. 投影</h2><p>为什么突然讲到投影呢？因为对指示函数应用近似点算子，实质上就是在计算投影。举个栗子就明白了：对于集合 $C$ 与集合外一点 $x$，$x$ 向集合 $C$ 的投影可以表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}\text { minimize } \quad& \frac{1}{2}\|y-x\|_{2}^{2}\\\text { subject to } \quad& y\in C\end{aligned}</script><p>若投影点为 $y^\star$，则这可以等价表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}y^\star &= \arg\min_y \frac{1}{2}\|y-x\|_{2}^{2}+\delta_C(y) \\&= \text{prox}_{\delta}(x)\end{aligned}</script><p>因此 $\text{prox}_{\delta}(x)$ 就是 $x$ 向集合 $C$ 的投影点(对于 $x\in C$ 同样成立)。那么只要我们取不同的 $C$，就能得到各种类型集合的投影表达式，下面举一些例子。</p>
<p><strong>超平面</strong>：$C=\{x|a^Tx=b\}$ with $a\ne0$</p>
<script type="math/tex; mode=display">
P_{C}(x)=x+\frac{b-a^{T} x}{\|a\|_{2}^{2}} a</script><p><strong>仿射集</strong>：$C=\{x | A x=b\}\left(\text { with } A \in \mathbf{R}^{p \times n} \text { and } \operatorname{rank}(A)=p\right)$</p>
<script type="math/tex; mode=display">
P_{C}(x)=x+A^{T}\left(A A^{T}\right)^{-1}(b-A x)</script><p><strong>半空间</strong>：$C=\{x|a^Tx\le b\}$ with $a\ne0$</p>
<script type="math/tex; mode=display">
P_{C}(x)=\begin{cases}x+\frac{b-a^{T} x}{\|a\|_{2}^{2}} a & \text {if } a^{T} x>b \\ x & \text {if } a^{T} x \leq b\end{cases}</script><p><strong>矩形</strong>：$C=[l, u]=\left\{x \in \mathbf{R}^{n} | l \leq x \leq u\right\}$</p>
<script type="math/tex; mode=display">
P_{C}(x)_{k}=\left\{\begin{array}{ll}l_{k} & x_{k} \leq l_{k} \\x_{k} & l_{k} \leq x_{k} \leq u_{k} \\u_{k} & x_{k} \geq u_{k}\end{array}\right.</script><p><strong>非负象限</strong>：$C=R_+^n$</p>
<script type="math/tex; mode=display">
P_{C}(x)=x_{+}=\left(\max \left\{0, x_{1}\right\}, \max \left\{0, x_{2}\right\}, \ldots, \max \left\{0, x_{n}\right\}\right)</script><p><strong>概率单形</strong>：$C=\left\{x | \mathbf{1}^{T} x=1, x \geq 0\right\}$</p>
<script type="math/tex; mode=display">
P_{C}(x)=(x-\lambda \mathbf{1})_{+}</script><p>其中 $\lambda$ 由以下方程解出</p>
<script type="math/tex; mode=display">
\mathbf{1}^{T}(x-\lambda \mathbf{1})_{+}=\sum_{i=1}^{n} \max \left\{0, x_{k}-\lambda\right\}=1</script><p>这个的证明有一点难度，关键是首先要把约束条件 $x\ge0$ 转换为指示函数表示</p>
<script type="math/tex; mode=display">
\begin{aligned}\text { minimize } \quad& \frac{1}{2}\|y-x\|_{2}^{2} + \delta_{R_+^n}(y) \\\text { subject to } \quad& \mathbf{1}^{T} y=1\end{aligned}</script><p>然后将拉格朗日函数分解成求和的形式</p>
<script type="math/tex; mode=display">
\begin{array}{l}\frac{1}{2}\|y-x\|_{2}^{2}+\delta_{\mathbf{R}_{+}^{n}}(y)+\lambda\left(\mathbf{1}^{T} y-1\right) \\\quad=\quad \sum_{k=1}^{n}\left(\frac{1}{2}\left(y_{k}-x_{k}\right)^{2}+\delta_{\mathbf{R}_{+}}\left(y_{k}\right)+\lambda y_{k}\right)-\lambda\end{array}</script><p>对上面这个求和项进行分情况讨论就能得到解析表达式了，不过真的很繁琐。</p>
<p><strong>超平面与矩形交集</strong>：$C=\{x|a^Tx=b,l\preceq x\preceq u\}$</p>
<script type="math/tex; mode=display">
P_{C}(x)=P_{[l,u]}(x-\lambda a)</script><p>其中 $\lambda$ 由以下方程解出</p>
<script type="math/tex; mode=display">
a^{T} P_{[l, u]}(x-\lambda a)=b</script><p>证明跟上面的概率单形是类似的，也需要拆写成多项求和的形式分别求解。</p>
<p><strong>欧几里得球</strong>：$C=\{x| \Vert x\Vert_2\le1\}$</p>
<script type="math/tex; mode=display">
P_{C}(x)=\begin{cases}\frac{x}{\|x\|_{2}} & \text {if } \Vert x\Vert_2>1 \\ x & \text {if } \Vert x\Vert_2\le1\end{cases}</script><p><strong>1 范数球</strong>：$C=\{x| \Vert x\Vert_1\le1\}$</p>
<p>若 $\Vert x\Vert_1\le1$ 则 $P_C(x)=x$；否则</p>
<script type="math/tex; mode=display">
P_{C}(x)_{k}=\operatorname{sign}\left(x_{k}\right) \max \left\{\left|x_{k}\right|-\lambda, 0\right\}=\left\{\begin{array}{ll}x_{k}-\lambda & x_{k}>\lambda \\0 & -\lambda \leq x_{k} \leq \lambda \\x_{k}+\lambda & x_{k}<-\lambda\end{array}\right.</script><p>其中 $\lambda$ 由以下等式获得</p>
<script type="math/tex; mode=display">
\sum_{k=1}^n \max \{\vert x\vert_k-\lambda, 0\}=1</script><p>证明业与前面的类似，需要写成求和项的形式，然后对每一项求解。</p>
<p><strong>二阶锥</strong>：$C=\{(x,t)\in R^{n\times 1}| \Vert x\Vert_2 \le t \}$</p>
<script type="math/tex; mode=display">
P_{C}(x,t)=\begin{cases}(x,t) & \text {if } \Vert x\Vert_2\le t \\ (0,0) & \text {if } \Vert x\Vert_2\le -t \\\frac{t+\|x\|_{2}}{2\|x\|_{2}}\left[\begin{array}{c} x \\ \|x\|_{2} \end{array}\right] & \text {if } \Vert x\Vert_2> \vert t\vert \end{cases}</script><p><strong>正定锥</strong>：$C=S^n_+$</p>
<script type="math/tex; mode=display">
P_{C}(X)=\sum_{i=1}^{n} \max \left\{0, \lambda_{i}\right\} q_{i} q_{i}^{T}</script><p>其中 $X=\sum_i \lambda_i q_iq_i^T$</p>
<h2 id="5-支撑函数、范数与距离"><a href="#5-支撑函数、范数与距离" class="headerlink" title="5. 支撑函数、范数与距离"></a>5. 支撑函数、范数与距离</h2><p>这一小节标题看起来很复杂，牵涉到了支撑函数、范数、到集合的距离，但<strong>实际上都还是在计算投影</strong>，为什么这么说呢？回忆一下，支撑函数的共轭函数是不是 $\delta$ 函数？范数的共轭函数是不是 $\delta$ 函数？到集合的距离是不是就等于到投影点的距离？所以这一小节是上一小节“投影”的自然延伸，其中为了把原函数与共轭函数联系在一起，用到了 Moreau decomposition。我们一个一个来看。</p>
<script type="math/tex; mode=display">
x=\operatorname{prox}_{f}(x)+\operatorname{prox}_{f^{*}}(x) \quad\text { for all } x</script><p><strong>支撑函数</strong>：$f(x)=\sup_{y\in C}x^Ty,f^\star(y)=\delta_C(y)$，因此近似点算子为</p>
<script type="math/tex; mode=display">
\begin{aligned}\operatorname{prox}_{t f}(x) &=x-t \operatorname{prox}_{t^{-1} f^{*}}(x / t) \\&=x-t P_{C}(x / t)\end{aligned}</script><p><strong>范数</strong>：$f(x)=\Vert x\Vert,f^\star(y)=\delta_B(y)$，其中 $B=\{y| \Vert y\Vert_\star \le 1\}$，近似点算子为</p>
<script type="math/tex; mode=display">
\begin{aligned}\operatorname{prox}_{t f}(x) &=x-t \operatorname{prox}_{t^{-1} f^{*}}(x / t) \\&=x-t P_{B}(x / t) \\&=x- P_{tB}(x)\end{aligned}</script><p>其中 $tB=\{y| \Vert y\Vert_\star \le t\}$</p>
<p><strong>与一点的距离</strong>：$f(x)=\Vert x-a\Vert$，可以取 $g(x)=\Vert x\Vert$</p>
<script type="math/tex; mode=display">
\begin{aligned}\operatorname{prox}_{t f}(x) &=a + \operatorname{prox}_{tg}(x-a) \\&=a+x-a-tP_B(\frac{x-a}{t}) \\&=x- P_{tB}(x-a)\end{aligned}</script><p><strong>到集合的距离</strong>：到闭凸集 $C$ 的距离定义为 $d(x)=\inf_{y\in C}\Vert x-y\Vert_2$</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{t d}(x)=\left\{\begin{array}{ll}x+\frac{t}{d(x)}\left(P_{C}(x)-x\right) & d(x) \geq t \\P_{C}(x) & \text { otherwise }\end{array}\right.</script><p>如果是距离取平方 $f(x)=d(x)^2/2$，则有</p>
<script type="math/tex; mode=display">
\operatorname{prox}_{t f}(x)=\frac{1}{1+t} x+\frac{t}{1+t} P_{C}(x)</script><p>这个证明贴在下面</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/18-distance-proof1.PNG" alt="proof 1"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/18-distance-proof2.PNG" alt="proof 2"></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>共轭函数</tag>
        <tag>近似点算子</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记17：次梯度下降</title>
    <url>/2020/04/10/optimization/ch17-subgradient-descent/</url>
    <content><![CDATA[<p>对于光滑函数，我们可以用梯度下降法，并且证明了取不同的步长，可以得到次线性收敛，如果加上强凸性质，还可以得到线性收敛速度。那如果现在对于不可导的函数，我们就只能沿着次梯度下降，同样会面临步长的选择、方向的选择、收敛性分析等问题。</p>
<a id="more"></a>
<h2 id="1-收敛性分析"><a href="#1-收敛性分析" class="headerlink" title="1. 收敛性分析"></a>1. 收敛性分析</h2><p>次梯度下降的一般形式为</p>
<script type="math/tex; mode=display">
x^{(k)}=x^{(k-1)}-t_{k} g^{(k-1)}, \quad k=1,2, \ldots \quad g\in\partial f(x^{(k-1)})</script><p>一般有 3 种步长的选择方式：</p>
<ol>
<li>fix step： $t_k$ 为常数</li>
<li>fix length：$t_{k}\left|g^{(k-1)}\right|_{2}=\left|x^{(k)}-x^{(k-1)}\right|_{2}$ 为常数</li>
<li>diminishing：$t_{k} \rightarrow 0, \sum_{k=1}^{\infty} t_{k}=\infty$</li>
</ol>
<p>要证明这几种方法的收敛性，需要先引入 Lipschitz continuous 假设，即</p>
<script type="math/tex; mode=display">
|f(x)-f(y)| \leq G\|x-y\|_{2} \quad \forall x, y</script><p>这等价于 $\Vert g\Vert_2\le G$ 对任意 $g\in\partial f(x)$ 都成立。</p>
<p>在分析收敛性之前，我们需要引入一个<strong>非常重要的式子</strong>:arrow_down:！！！在后面的分析中会一直用到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{+}-x^{\star}\right\|_{2}^{2} &=\left\|x-t g-x^{\star}\right\|_{2}^{2} \\
&=\left\|x-x^{\star}\right\|_{2}^{2}-2 t g^{T}\left(x-x^{\star}\right)+t^{2}\|g\|_{2}^{2} \\
& \leq\left\|x-x^{\star}\right\|_{2}^{2}-2 t\left(f(x)-f^{\star}\right)+t^{2}\|g\|_{2}^{2}
\end{aligned}</script><p>那么如果定义 $f_{\mathrm{best}}^{(k)}=\min _{0 \leq i&lt;k} f\left(x^{(i)}\right)$，就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
2\left(\sum_{i=1}^{k} t_{i}\right)\left(f_{\text {best }}^{(k)}-f^{\star}\right) & \leq\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}-\left\|x^{(k)}-x^{\star}\right\|_{2}^{2}+\sum_{i=1}^{k} t_{i}^{2}\left\|g^{(i-1)}\right\|_{2}^{2} \\
& \leq\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}+\sum_{i=1}^{k} t_{i}^{2}\left\|g^{(i-1)}\right\|_{2}^{2}
\end{aligned}</script><p>根据上面的式子，就可以得到对于</p>
<p><strong>Fixed step size</strong>：$t_i=t$</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}}{2 k t}+\frac{G^{2} t}{2}</script><p><strong>Fixed step length</strong>：$t_{i}=s /\left|g^{(i-1)}\right|_{2}$</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{G\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}}{2 k s}+\frac{G s}{2}</script><p>这两个式子中的第一项都随着 $k$ 增大而趋于 0，但第二项却没有办法消掉，也就是与最优解的误差不会趋于 0。并且他们有一个微妙的不同点在于，fixed step size 情况下 $G^2t/2\sim O(G^2),Gs/2\sim O(G)$，$G$ 一般是较大的。</p>
<p><strong>Diminishing step size</strong>：$t_{k} \rightarrow 0, \sum_{k=1}^{\infty} t_{k}=\infty$</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}+G^{2} \sum_{i=1}^{k} t_{i}^{2}}{2 \sum_{i=1}^{k} t_{i}}</script><p>可以证明，$\left(\sum_{i=1}^{k} t_{i}^{2}\right) /\left(\sum_{i=1}^{k} t_{i}\right) \rightarrow 0$，因此 $f_{\text {best }}^{(k)}$ 会收敛于 $f^\star$。</p>
<p>下面看几幅图片，对于优化问题 $\min\Vert Ax-b\Vert_1$</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Fixed step length</th>
<th>Diminishing step size</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/17-fixed-step.PNG" alt="fixed-step"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/17-diminishing.PNG" alt="diminishing"></td>
</tr>
</tbody>
</table>
</div>
<p>前面考虑了固定步长的情况，假设现在我们固定总的迭代次数为 $k$，可不可以优化步长 $s$ 的大小来尽可能使 $f_\text{best}^{(k)}$ 接近 $f^\star$ 呢？这实际上可以表示为优化问题</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{R^{2}+\sum_{i=1}^{k} s_{i}^{2}}{2 \sum_{i=1}^{k} s_{i}/G} \Longrightarrow \min_s \frac{R^{2}}{2 ks/G}+\frac{s}{2/G}</script><p>其中 $R=\left|x^{(0)}-x^{\star}\right|_{2}$，那么最优步长为 $s=R/\sqrt{k}$，此时 </p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{GR}{\sqrt{k}}</script><p>因此收敛速度为 $O(1/\sqrt{k})$，对比之前光滑函数的梯度下降，收敛速度为 $O(1/k)$。</p>
<p>我们对前面的收敛速度并不满意，如果有更多的信息，比如已知最优解 $f^\star$ 的大小，能不能改进收敛速度呢？根据前面的式子，有</p>
<script type="math/tex; mode=display">
\left\|x^{+}-x^{\star}\right\|_{2}^{2}  
\leq\left\|x-x^{\star}\right\|_{2}^{2}-2 t_i\left(f(x)-f^{\star}\right)+t_i^{2}\|g\|_{2}^{2}</script><p>这实际上是关于 $t_i$ 的一个二次函数，因此可以取 $t_{i}=\frac{f\left(x^{(i-1)}\right)-f^{\star}}{\left|g^{(i-1)}\right|_{2}^{2}}$，就可以得到</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{GR}{\sqrt{k}}</script><p>可见还是没有改进收敛速度。</p>
<p>如果引入<strong>强凸性质</strong>呢？如果假设满足 $\mu$ 强凸，则 $f^\star \ge f^k+g^{kT}(x^k-x^\star)+\mu/2\Vert x^k-x^\star\Vert_2^2$，可以取 $t_k=\frac{2}{\mu(k+1)}$，那么就可以得到</p>
<script type="math/tex; mode=display">
f_{\text {best }}^{(k)}-f^{\star} \leq \frac{2G^2}{\mu(k+1)}</script>]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>次梯度</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记16：次梯度 Subgradient</title>
    <url>/2020/04/10/optimization/ch16-subgradient/</url>
    <content><![CDATA[<p>前面讲了梯度下降的方法，关键在于步长的选择：固定步长、线搜索、BB方法等，但是如果优化函数本身存在不可导的点，就没有办法计算梯度了，这个时候就需要引入<strong>次梯度(Subgradient)</strong>，这一节主要关注次梯度的计算。</p>
<a id="more"></a>
<h2 id="1-次梯度"><a href="#1-次梯度" class="headerlink" title="1. 次梯度"></a>1. 次梯度</h2><p><strong>次梯度(subgradient)</strong>的定义为</p>
<script type="math/tex; mode=display">
\partial f(x)= \{g|f(y)\ge f(x)+g^T(y-x),\forall y\in\text{dom} f \}</script><p>该如何理解次梯度 $g$ 呢？实际上经过变换，我们可以得到</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
g \\
-1
\end{array}\right]^{\top}\left(\left[\begin{array}{l}
y \\
t
\end{array}\right]-\left[\begin{array}{c}
x \\
f(x)
\end{array}\right]\right) \leq 0, \forall(y, t) \in \operatorname{epi}f</script><p>实际上这里 $[g^T \ -1]^{T}$ 定义了 <strong>epigraph 的一个支撑超平面</strong>，并且这个支撑超平面是<strong>非垂直的</strong>，如下面的图所示</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>光滑函数</th>
<th>非光滑函数</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-subgradient.PNG" alt="subgradient"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-subgradient2.PNG" alt="subgradient2"></td>
</tr>
</tbody>
</table>
</div>
<p>而对于任意的下水平集 $\{y|f(y)\le f(x)\}$，都有</p>
<script type="math/tex; mode=display">
f(y)\le f(x) \Longrightarrow g^T(y-x)\le0</script><p>这说明次梯度 $g$ 实际上也是下水平集的一个支撑超平面</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-subgradient3.PNG" alt="subgradient3"></p>
<p><strong>Remarks</strong>：很有意思的一件事是函数的<strong>梯度 $\nabla f$</strong> 包含有大量的信息，他的<strong>方向</strong>代表了函数下降最快的方向，也就是下水平集的法线方向；而他的<strong>模长</strong>代表了下降的速度，$[\nabla f\ -1]^T$ 是 epigraph 的法线方向，可以直观想象，如果 $\Vert \nabla f\Vert$ 越大，那么这个法线方向越趋向于水平，也就是说 epigraph 的标面越趋近于竖直，函数下降速度当然也越快。</p>
<p>每个点的次梯度 $\partial f(x)$ 实际上是一个<strong>集合</strong>，我们先来看这个集合有什么性质呢？我们先列出来然后一一解释：</p>
<ol>
<li>次梯度映射是单调算子</li>
<li>如果 $x\in\text{ri dom}f$，则 $\partial f(x)\ne \varnothing$，而且是<strong>有界、闭的凸集</strong></li>
<li>$x^\star= \arg\min f(x) \iff 0\in \partial f(x^\star)$</li>
</ol>
<p>首先，他是一个 set-valued mapping，而且是一个<strong>单调算子</strong>，也即</p>
<script type="math/tex; mode=display">
(u-v)^T(y-x)\ge0,\forall u\in\partial f(y),v\in\partial f(x)</script><p>这个性质很容易由定义导出。</p>
<p>其次，内点处的次梯度总是非空的闭凸集，且有界。</p>
<p>首先可以证明其<strong>非空</strong>，(默认我们考虑的 $f$ 为凸函数)因为函数 $f$ 是凸的，其 epigraph 在 $(x,f(x))$ 一定存在一个支撑超平面</p>
<script type="math/tex; mode=display">
\exists(a, b) \neq 0, \quad\left[\begin{array}{l}
a \\
b
\end{array}\right]^{T}\left(\left[\begin{array}{l}
y \\
t
\end{array}\right]-\left[\begin{array}{c}
x \\
f(x)
\end{array}\right]\right) \leq 0 \quad \forall(y, t) \in \text { epi } f</script><p>由于 $t$ 可以趋于 $+\infty$，因此 $b\le0$，如果 $b=0$，由于 $x$ 为内点，也很容易导出矛盾，因此可以证明 $b&lt;0$，于是就可以得到 $g=a/|b|$。</p>
<p>其次可以证明其为<strong>闭凸集</strong>，次梯度还可以表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\partial f(x)&= \{g|f(y)\ge f(x)+g^T(y-x),\forall y\in\text{dom} f \} \\
&= \bigcap_{y\in \text{dom}f} \{g|g^T(y-x)\le f(y)-f(x) \}
\end{aligned}</script><p>这是很多个半空间的交集，因此 $\partial f(x)$ 是一个闭的凸集。</p>
<p>最后可以证明次梯度集合是<strong>有界的</strong>，为了证明他有界，只需证明他的 $l_\infty$ 范数有界即可。可以取</p>
<script type="math/tex; mode=display">
B=\left\{x \pm r e_{k} | k=1, \ldots, n\right\} \subset \operatorname{dom} f</script><p>定义 $M=\max _{y \in B} f(y)&lt;\infty$，应用次梯度的定义就可以得到</p>
<script type="math/tex; mode=display">
\|g\|_{\infty} \leq \frac{M-f(x)}{r} \quad \text { for all } g \in \partial f(x)</script><p>注意上面的非空、有界、闭凸集都要求 $x$ 为定义域的内点，如果是边界上则无法保证。</p>
<p><strong><em>例子 1</em></strong>：对于凸集 $C$，定义函数 $\delta_C(x)=\begin{cases}0,&amp;x\in C\+\infty,&amp;x\notin C\end{cases}$，那么次梯度为</p>
<script type="math/tex; mode=display">
\begin{aligned}
g\in\partial \delta_C(x) &\iff \delta_C(y)\ge \delta_C(x)+g^T(y-x),\forall y\in C \\
&\iff g^T(y-x)\le0, \forall y\in C \\
&\iff g\in N_C(x) \quad \text{ (normal cone at $x$)} 
\end{aligned}</script><p><strong>Remarks</strong>：集合 $C$ 的 normal cone 的定义为</p>
<script type="math/tex; mode=display">
\forall x\in C,\quad N_C(x)=\{g| g^T(y-x)\le0,\forall y\in C\}</script><p><strong><em>例子 2</em></strong>：函数 $f(x)=\vert x\vert,\partial f(x)=\begin{cases}1,&amp;x&gt;0\\ [-1,1],&amp;x=0\\ -1&amp;x&lt;0\end{cases}$</p>
<p><strong><em>例子 3</em></strong>：函数 $f(x)=\Vert x\Vert_2,\partial f(0)=\{g|\Vert g\Vert_2\le1\}$</p>
<p><strong><em>例子 4</em></strong>：对于任意范数 $f(x)=\Vert x\Vert$</p>
<script type="math/tex; mode=display">
\partial f(x)=\{y|\Vert y\Vert_*\le1,\left<y,x\right>=\Vert x\Vert \}</script><p>证明：$\forall g\in\partial f(x)$，需要 $\Vert y\Vert \ge \Vert x\Vert+ g^T(y-x)\quad(\Delta)$</p>
<p>可以取 $y=2x \Longrightarrow \Vert x\Vert \ge g^Tx$，也可以取 $y=0\Longrightarrow 0\ge \Vert x\Vert-g^Tx$，因此有 $g^Tx=\Vert x\Vert$</p>
<p>由 $(\Delta)$ 式可知应有 $\Vert y\Vert \ge g^Ty \iff \Vert g\Vert_*\le1$。</p>
<h2 id="2-次梯度计算"><a href="#2-次梯度计算" class="headerlink" title="2. 次梯度计算"></a>2. 次梯度计算</h2><p>每个点的次梯度是一个集合，这里有两个概念</p>
<p><strong>Weak subgradient calculus</strong>：只需要计算其中一个次梯度就够了；</p>
<p><strong>Strong subgradient calculus</strong>：要计算出 $\partial f(x)$ 中的所有元素。</p>
<p>要想计算出所有的次梯度是很难的，所以大多数时候只需要得到一个次梯度就够了，也就是 Weak subgradient calculus。不过，对于下面这几种特殊情况，我们可以得到完整的次梯度描述(也即 Strong subgradient calculus)，他们是：</p>
<ol>
<li>如果 $f$ 在 $x$ 是可微的，那么 $\partial f(x)=\{\nabla f(x)\}$</li>
<li>非负线性组合：$f(x)=\alpha_1 f_1(x)+\alpha_2 f_2(x)$，那么 $\partial f(x)=\alpha_1 \partial f_1(x)+\alpha_2 \partial f_2(x)$，第二个式子是集合的加法；</li>
<li>仿射变换：$f(x)=h(Ax+b)$，那么 $\partial f(x)=A^T h(Ax+b)$</li>
</ol>
<p>对于第一条的证明，我们可以取 $y=x+r(p-\nabla f(x)),p\in\partial f(x)$，那么根据次梯度的定义就有 $\Vert p-\nabla f(x)\Vert^2\le \frac{O(r^2)}{r}\to0$ 随着 $r\to0$，因此就有 $\nabla f(x)=p$。</p>
<p>对于第三条的证明，只需要分别证明 $A^T h(Ax+b) \subseteq \partial f(x)$ 和 $\partial f(x) \subseteq A^T h(Ax+b)$，前者很容易，主要是后者。由于次梯度 $d\in \partial f(x)$ 需要满足 $f(z)\ge f(x)+d^T(z-x) \iff h(Az+b)-d^Tz\ge h(Ax+b)-d^Tx$，也就是说 $(x,Ax+b)$ 实际上是如下问题的最优解</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min \quad& h(z)-d^Ty \\
\text{s.t.}\quad& Ay+b = z,\quad z\in\text{dom}h
\end{aligned}</script><p>如果 $(Range(A)+b)\cap \text{ri dom}h \ne \varnothing$，说明 SCQ 成立，则强对偶性成立，于是根据拉格朗日对偶原理有</p>
<script type="math/tex; mode=display">
\exists \lambda,\text{ s.t. }(x,Ax+b) \in \arg\min \{h(z)-d^Ty+\lambda^T(Ay+b-z)\} \\
\Longrightarrow
\begin{cases}
\nabla_y L(y,z,\lambda)=0 \Longrightarrow 0\in \partial h(z)-\lambda \\
\nabla_z L(y,z,\lambda)=0 \Longrightarrow d=A^T\lambda
\end{cases}</script><p><strong>推论</strong>：根据第 3 条，可以得到：如果考虑函数 $F(x)=f_1(x)+…+f_m(x)$，且 $\bigcap_i \text{ri dom}f_i\ne \varnothing $，则 $\partial F(x)=\partial f_1(x)+… + \partial f_m(x)$。（这个实际上可以直接右上面的第二条得到，这里只不过又验证了一次）</p>
<p><strong>证明</strong>：我们可以考虑函数 $f(x)=f_1(x_1)+…+f_m(x_m)$，在定义 $A=[I,…,I]^T$，那么就有 $F(x)=f(Ax)$，所以 $\partial F(x)=A^T \partial f(Ax)=[I\ …\ I][\partial f_1(x),…,\partial f_m(x)]^T = \partial f_1(x)+… + \partial f_m(x)$。</p>
<p>上面是能获得 Strong subgradient calculus 的几个原则，对于其他情况，我们考虑找到一个次梯度就够了。下面给出一些常见的情况。</p>
<p><strong>点点最大值</strong>：$f(x)=\max\{f_1(x),…,f_m(x) \}$，可以定义 $I(x)=\{i|f_i(x)=f(x)\}$，那么他的</p>
<ul>
<li><strong>weak result</strong>：choose any $g\in\partial f_k(x)$，其中 $k\in I(x)$</li>
<li><strong>strong result</strong>：$\partial f(x)=\text{conv}\bigcup_{i\in I(x)}\partial f_i(x)$</li>
</ul>
<p><strong>点点上确界</strong>：$f(x)=\sup_{\alpha\in \mathcal{A}} f_\alpha (x)$，其中 $f_\alpha(x)$ 关于 $x$ 是凸的，定义 $I(x)=\{\alpha\in\mathcal{A}|f_\alpha(x)=f(x)\}$，那么</p>
<ul>
<li><strong>weak result</strong>：choose any $g\in\partial f_\beta (\hat{x})$，其中 $f(\hat{x})=f_\beta(\hat{x})$</li>
<li><strong>strong result</strong>：$\text{conv}\bigcup_{\alpha\in I(x)}\partial f_\alpha(x) \subseteq \partial f(x)$，如果要取等号，需要额外的条件</li>
</ul>
<p><strong>下确界</strong>：$f(x)=\inf_y h(x,y)$，其中 $h(x,y)$ 关于 $(x,y)$ 是联合凸的，那么</p>
<ul>
<li><strong>weak result</strong>：$(g,0)\in \partial h(\hat{x},\hat{y})$，其中 $\hat{y}=\arg\min h(\hat{x},y)$</li>
</ul>
<p><strong>复合函数</strong>：$f(x)=h(f_1(x),…,f_k(x))$，其中 $h$ 为单调不减的凸函数，$f_i$ 为凸函数</p>
<ul>
<li><strong>weak result</strong>：$g=z_1g_1+…+z_kg_k,z\in\partial h(f_1(x),…,f_k(x)),g_i\in \partial f_i(x)$</li>
</ul>
<p><strong>期望</strong>：$f(x)=\mathbb{E}h(x,u)$，其中 $u$ 为随机变量，$h$ 对任意的 $u$ 关于 $x$ 都是凸的</p>
<ul>
<li><strong>weak result</strong>：选择函数 $u\mapsto g(u),g(u)\in \partial_x h(\hat{x},u)$，则 $g=\mathbb{E}_u g(u) \in \partial f(\hat{x})$</li>
</ul>
<p><strong><em>例子 1</em></strong>：picewise-linear function $f(x)=\max_{i=1,…,m}(a_i^Tx+b_i),\partial f(x)=\text{conv}\{a_i|i\in I(x)\}$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-picewise-linear.PNG" alt="picewise-linear"></p>
<p><strong><em>例子 2</em></strong>：$l_1$ 范数 $f(x)=\Vert x\Vert_1=|x_1|+…+|x_n|,\partial f(x)=J_1\times \cdots \times J_n$，其中 $J_k=\begin{cases}[-1,1]&amp;x_k=0\\ {1}&amp;x_k&gt;0\\ {-1}&amp;x_k&lt;0 \end{cases}$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-l1-norm.PNG" alt="l1-norm"></p>
<p><strong><em>例子 3</em></strong>：$f(x)=\lambda_{\max}(A(x))=\sup_{\Vert y\Vert_2=1}y^TA(x)y$，其中 $A(x)=A_0+x_1A_1+\cdots +x_nA_n$，则取 $\lambda_{\max}(A(\hat{x}))$ 对应的单位特征向量 $y$，次梯度可以表示为 $(y^TA_1y,…,y^TA_ny)\in \partial f(\hat{x})$</p>
<p><strong><em>例子 4</em></strong>：到凸集的欧氏距离 $f(x)=\inf_{y\in C}\Vert x-y\Vert_2=\inf_{y} h(x,y)$，其中集合 $C$ 为凸集，函数 $h$ 关于 $(x,y)$ 是联合凸的。</p>
<script type="math/tex; mode=display">
g=\begin{cases}0&\hat{x}\in C\\ \frac{1}{\|\hat{y}-\hat{x}\|_{2}}(\hat{x}-\hat{y})=\frac{1}{\|\hat{x}-P(\hat{x})\|_{2}}(\hat{x}-P(\hat{x})) & \hat{x}\notin C \end{cases}</script><p><strong><em>例子 5</em></strong>：定义如下凸优化问题的最优解为 $f(u,v)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_{0}(x) \\
\text{subject to}\quad& f_{i}(x) \leq u_{i}, \quad i=1, \ldots, m\\
&A x=b+v
\end{aligned}</script><p>如果假设 $f(\hat{u},\hat{v})$ 有界且强对偶性成立，那么对于对偶问题如下对偶问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{maximize} \quad& \inf _{x}\left(f_{0}(x)+\sum_{i} \lambda_{i}\left(f_{i}(x)-\hat{u}_{i}\right)+v^{T}(A x-b-\hat{v})\right) \\
\text{subject to}\quad& \lambda\succeq 0
\end{aligned}</script><p>若其最优解为 $(\hat\lambda,\hat\nu)$，则有 $(-\hat\lambda,-\hat\nu)\in \partial f(\hat{u},\hat{v})$。</p>
<h2 id="3-对偶原理与最优解条件"><a href="#3-对偶原理与最优解条件" class="headerlink" title="3. 对偶原理与最优解条件"></a>3. 对偶原理与最优解条件</h2><p>前面我们对于可导函数获得了对偶原理以及 KKT 条件，那如果是不可导的函数呢？我们有</p>
<script type="math/tex; mode=display">
f(y) \geq f\left(x^{\star}\right)+0^{T}\left(y-x^{\star}\right) \quad \text { for all } y \quad \Longleftrightarrow \quad 0 \in \partial f\left(x^{\star}\right)</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-optimal.PNG" alt="optimal"></p>
<p><strong><em>例子</em></strong>：对于优化问题 $\min f(x),\text{s.t. }x\in C \iff \min f(x)+\delta_C(x)=F(x)$，因此</p>
<script type="math/tex; mode=display">
0\in \partial f(x^\star)+\partial \delta_C(x^\star) \Longrightarrow \exists p\in\partial f(x^\star),\quad\text{s.t.}-p\in \partial\delta_C(x^\star)=N_C(x^\star)</script><p>如果 $f\in C^1$，则有 $-\nabla f(x^\star)\in N_C(x^\star)$。</p>
<p>KKT 条件怎么变呢？只需要修改一下梯度条件：</p>
<ol>
<li>原问题可行性 $x^\star$ is primal feasible</li>
<li>对偶问题可行性 $\lambda^\star \succeq0$</li>
<li>互补性条件 $\lambda_i^\star f_i(x^\star)=0,i=1,…,m$</li>
<li>梯度条件 $0\in \partial f_0(x^\star)+\sum_i \lambda_i^\star \partial f_i(x^\star)$</li>
</ol>
<h2 id="4-方向导数"><a href="#4-方向导数" class="headerlink" title="4. 方向导数"></a>4. 方向导数</h2><p><strong>方向导数(directional derivative)</strong>的定义为</p>
<script type="math/tex; mode=display">
\begin{aligned}
f^{\prime}(x ; y) &=\lim _{\alpha \searrow 0} \frac{f(x+\alpha y)-f(x)}{\alpha} \\
&=\lim _{t \rightarrow \infty}\left(tf\left(x+\frac{1}{t} y\right)-t f(x)\right)
\end{aligned}</script><p>方向导数是<strong>齐次</strong>的，也即</p>
<script type="math/tex; mode=display">
f'(x;\lambda y)=\lambda f'(x;y) \quad \text{for }\lambda\ge0</script><p>对于<strong>凸函数</strong>，方向导数也可以定义为</p>
<script type="math/tex; mode=display">
\begin{aligned}
f^{\prime}(x ; y) &=\inf _{\alpha > 0} \frac{f(x+\alpha y)-f(x)}{\alpha} \\
&=\inf _{t >0}\left(tf\left(x+\frac{1}{t} y\right)-t f(x)\right)
\end{aligned}</script><p>要证明的话，只需要证明 $g(\alpha)=\frac{f(x+\alpha y)-f(x)}{\alpha}$ 随着 $\alpha$ 单调递减有下界。</p>
<p>实际上方向导数定义了沿着 $y$ 方向的函数下界，也即 </p>
<script type="math/tex; mode=display">
f(x+\alpha y) \geq f(x)+\alpha f^{\prime}(x ; y) \quad \text { for all } \alpha \geq 0</script><p>对于凸函数，$x\in\text{int dom}f$，也有</p>
<script type="math/tex; mode=display">
f^{\prime}(x ; y)=\sup _{g \in \partial f(x)} g^{T} y</script><p>也即 $f’(x;y)$ 是 $\partial f(x)$ 的支撑函数</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-directional-derivate.PNG" alt="directional-derivate"></p>
<p><strong>Remarks</strong>：需要注意的是<strong>负的次梯度方向</strong>不一定是函数值下降方向，而只有<strong>方向导数 &lt;0 的方向</strong>才是函数值下降方向。反例如下图</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-directional-derivate2.PNG" alt="directional-derivate2"></p>
<p>如果我们想找到下降最快的方向(Steepest descent direction)，则需要</p>
<script type="math/tex; mode=display">
\Delta x_{\mathrm{nsd}}=\underset{\|y\|_{2} \leq 1}{\operatorname{argmin}} f^{\prime}(x ; y)</script><p>根据前面的式子我们知道 $\min f’(x;y)=\min_{\Vert y\Vert_2\le1}\sup_{g\in\partial f(x)}g^Ty$，如果假设极大极小可以换序，则可以等价为 $\sup_g \inf_y g^Ty = \sup_{g\in\partial f(x)} -\Vert g\Vert_2$，上面过程可以表述为原问题与对偶问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f'(x;y) \\
\text{subject to}\quad& \|y\|_{2} \leq 1
\end{aligned}
\qquad
\begin{aligned}
\text{minimize} \quad& -\|g\|_{2} \\
\text{subject to}\quad& g \in \partial f(x)
\end{aligned}</script><p>于是就有 $f^{\prime}\left(x ; \Delta x_{\mathrm{nsd}}\right)=-\left|g^{\star}\right|_{2}$，$\text { if } 0 \notin \partial f(x), \Delta x_{\mathrm{nsd}}=-g^{\star} /\left|g^{\star}\right|_{2}$，如下图所示</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/16-steepest-descent.PNG" alt="16-steepest-descent"></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>次梯度</tag>
      </tags>
  </entry>
  <entry>
    <title>MATLAB R2016a 无法启动并行池</title>
    <url>/2020/04/08/tools/matlab-parpool/</url>
    <content><![CDATA[<p>最近在用 MATLAB 跑仿真，但是不知怎么回事，之前并行计算 parfor 用的好好的，昨天突然就不能用了，一直报错无法启动并行池，报错原因还特别奇怪。在网上找了一大堆教程互相抄来抄去，没一个能用的。最后还是在官网论坛找到了一个答案成功解决问题。</p>
<a id="more"></a>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>我用的是 MATLAB R2016a，当我运行带有 parfor 的代码时，左下角会有如下提示，表示无法启动并行池，而正常情况应该是右边这幅图</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>报错情况</th>
<th>正常情况</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/matlab-parpool-error.png" alt="error info"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/matlab-parpool-normal.png" alt="normal info"></td>
</tr>
</tbody>
</table>
</div>
<p>当我点击查看 more details 时，会报如下的错误，参数不对？这不是自带函数吗？</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/matlab-parpool-error2.png" alt="error info"></p>
<p>然后我参考了网上的教程，查看 <code>Home-&gt;Parallel-&gt;Manage Cluster Profiles</code>，但是网上教程说如果是下面这个样子</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>我的错误是这样的</th>
<th>网上只有这样的</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/matlab-parpool-error3.png" alt="my error"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/matlab-parpool-error4.png" alt="others"></td>
</tr>
</tbody>
</table>
</div>
<p>很显然我的第一步就 fail 了，我按照网上的说法运行下面这句话也没用</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">distcomp.feature( <span class="string">'LocalUseMpiexec'</span>, <span class="built_in">false</span> )</span><br></pre></td></tr></table></figure>
<p>最后幸运的是在官网论坛找到了一篇可以解决我的问题的回答，下面给出解决方法。</p>
<h2 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2. 解决方法"></a>2. 解决方法</h2><blockquote>
<p>参考链接：<a href="https://www.mathworks.com/matlabcentral/answers/92124-why-am-i-unable-to-use-parpool-with-the-local-scheduler-or-validate-my-local-configuration-of-parall" target="_blank" rel="noopener">https://www.mathworks.com/matlabcentral/answers/92124-why-am-i-unable-to-use-parpool-with-the-local-scheduler-or-validate-my-local-configuration-of-parall</a></p>
<p>我用<strong>第 5 步(Clear the local scheduler data folder)</strong>解决了我的问题，不过下面还是贴出来完整的 debug 过程</p>
</blockquote>
<h3 id="2-1-检查-license"><a href="#2-1-检查-license" class="headerlink" title="2.1 检查 license"></a>2.1 检查 license</h3><p>运行命令检查 <code>Parallel Computing Toolbox</code> 的 license 正确</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">license checkout Distrib_Computing_Toolbox</span><br></pre></td></tr></table></figure>
<p>如果得到的回答是 <code>ans=1</code>，则说明 license 没问题。否则需要添加 license。</p>
<h3 id="2-2-确保你的-MATLAB-版本与-PCT-版本匹配"><a href="#2-2-确保你的-MATLAB-版本与-PCT-版本匹配" class="headerlink" title="2.2 确保你的 MATLAB 版本与 PCT 版本匹配"></a>2.2 确保你的 MATLAB 版本与 PCT 版本匹配</h3><p>运行命令 <code>ver</code> 查看，如果不匹配则无法使用。这种情况 …… 建议重装。</p>
<h3 id="2-3-Disable-local-mpiexec"><a href="#2-3-Disable-local-mpiexec" class="headerlink" title="2.3 Disable local mpiexec"></a>2.3 Disable local mpiexec</h3><p>运行下面的命令</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">distcomp.feature( <span class="string">'LocalUseMpiexec'</span>, <span class="built_in">false</span> )</span><br></pre></td></tr></table></figure>
<p>这也是网上绝大部分教程的方法，不过对我就不适用，如果对你也不适用的话，往下继续看。</p>
<h3 id="2-4-Check-your-local-scheduler-configuration"><a href="#2-4-Check-your-local-scheduler-configuration" class="headerlink" title="2.4 Check your local scheduler configuration"></a>2.4 Check your local scheduler configuration</h3><p>这一部分我贴出原文吧，大概是说如果你修改了默认配置，则可以重置他们。</p>
<blockquote>
<p>There are no changes that need to be made in order to use the local scheduler, but if you have made changes to the configuration, you may want to reset these. This can be done by creating a new local scheduler configuration. To do so,</p>
<ol>
<li>Go to the Parallel menu in MATLAB and select “Manage Cluster Profiles…” (“Manage Configurations…” for R2011b or earlier) </li>
<li>Click on Add &gt; Custom &gt; Local (for older releases: From the File menu, select New &gt; Local Configuration)</li>
<li>Click the radio option in the default column to set this as the default configuration</li>
</ol>
<p>Once complete, close the Manage Configuration windows and try again.</p>
</blockquote>
<h3 id="2-5-Clear-the-local-scheduler-data-folder"><a href="#2-5-Clear-the-local-scheduler-data-folder" class="headerlink" title="2.5 Clear the local scheduler data folder"></a>2.5 Clear the local scheduler data folder</h3><p> 出现无法启动并行池的原因也可能是本地的 local scheduler data 有问题，可以把他删除。删除的步骤为</p>
<ol>
<li><p>运行命令</p>
<figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">&gt;&gt;prefdir</span><br><span class="line">ans =</span><br><span class="line">C:<span class="symbol">\U</span>sers<span class="symbol">\A</span>dministrator<span class="symbol">\A</span>ppData<span class="symbol">\R</span>oaming<span class="symbol">\M</span>athWorks<span class="symbol">\M</span>ATLAB<span class="symbol">\R</span>2016a</span><br></pre></td></tr></table></figure>
<p>然后我们就可以在路径 <code>C:\Users\Administrator\AppData\Roaming\MathWorks\MATLAB</code> 下面找到文件夹 <code>local_scheduler_data</code> 或者 <code>local_cluster_jobs</code></p>
</li>
<li><p>关闭 MATLAB</p>
</li>
<li><p>将上面的文件夹 <code>local_scheduler_data</code> 或者 <code>local_cluster_jobs</code> 重命名或者直接删除</p>
</li>
<li><p>重启 MATLAB，试着开启并行池</p>
</li>
</ol>
<p>我做完这一步就能解决问题了，如果还不行，原文还给出了其他可能的原因，后面的我就直接贴出来原文了。</p>
<h3 id="2-6-Ensure-that-hostname-resolution-works-on-your-computer"><a href="#2-6-Ensure-that-hostname-resolution-works-on-your-computer" class="headerlink" title="2.6 Ensure that hostname resolution works on your computer"></a>2.6 Ensure that hostname resolution works on your computer</h3><p>In order to use the local scheduler, your computer’s own hostname must be resolvable. To confirm this, run the following command in MATLAB:</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">!hostname</span><br></pre></td></tr></table></figure>
<p>This will give you your computer’s hostname. You must be able to resolve this hostname to the computer’s IP address. To test this you can run:</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">!ping &lt;hostname&gt;</span><br></pre></td></tr></table></figure>
<p>Where <hostname> is the output of the hostname command above. If the results indicate the wrong IP address or say that your computer is an “unknown host”, there is a network issue on your computer that needs to be resolved in order to use the local scheduler. In that case, ask your network administrator for help.</p>
<h3 id="2-7-Check-to-see-if-you-have-a-startup-m-file-on-the-MATLAB-path"><a href="#2-7-Check-to-see-if-you-have-a-startup-m-file-on-the-MATLAB-path" class="headerlink" title="2.7 Check to see if you have a startup.m file on the MATLAB path"></a>2.7 Check to see if you have a startup.m file on the MATLAB path</h3><p>It may be causing an error, even if it works fine in MATLAB when run as code.</p>
<p>To see if you have a startup.m file on the MATLAB path run the below command in MATLAB:</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">which startup.m</span><br></pre></td></tr></table></figure>
<p>Either delete or move that file outside of the MATLAB path.</p>
<p>If you are still unable to run parpool, run a validation of your local scheduler configuration and submit this to support. To validate:</p>
<ol>
<li>Go to the Parallel menu in MATLAB and select “Manage Cluster Profiles…” (“Manage Configurations…” for R2011b or earlier)</li>
<li>Highlight your local scheduler configuration and click the “Validate” button (“Start Validation” for R201b or earlier)</li>
<li>Once the validation completes, click the “details” link to see the results</li>
</ol>
<p>You can then forward your output of validation, the results of the tests below, and your license number to support here: <a href="http://www.mathworks.com/support/contact_us/index.html" target="_blank" rel="noopener">http://www.mathworks.com/support/contact_us/index.html</a></p>
]]></content>
      <categories>
        <category>软件</category>
      </categories>
      <tags>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记15：梯度方法 Gradient Method</title>
    <url>/2020/04/05/optimization/ch15-gradient/</url>
    <content><![CDATA[<p>前面的章节基本上讲完了凸优化相关的理论部分，在对偶原理以及 KKT 条件那里我们已经体会到了理论之美！接下来我们就要进入求解算法的部分，这也是需要浓墨重彩的一部分，毕竟我们学习凸优化就是为了解决实际当中的优化问题。我们今天首先要接触的就是大名鼎鼎的<strong>梯度方法</strong>。现在人工智能和人工神经网络很火，经常可以听到反向传播，这实际上就是梯度下降方法的应用，他的思想其实很简单，就是沿着函数梯度的反方向走就会使函数值不断减小。</p>
<script type="math/tex; mode=display">
x_{k+1}=x_{k}-t_k \nabla f(x_k),\quad k=0,1,...</script><p>上面的式子看起来简单，但是真正应用时你会发现有各种问题：</p>
<ol>
<li>下降方向怎么选？$\nabla f(x_k)$ 吗？选择其他方向会不会好一点呢？</li>
<li>如果 $f(x)$ (在某些点)不可导又怎么办呢？</li>
<li>步长 $t_k$ 怎么选呢？固定值？变化值？选多大比较好？</li>
<li>收敛速度怎么样呢？我怎么才能知道是否达到精度要求呢？</li>
<li>…</li>
</ol>
<a id="more"></a>
<h2 id="1-凸函数"><a href="#1-凸函数" class="headerlink" title="1. 凸函数"></a>1. 凸函数</h2><p>前面讲凸函数的时候我们提到了很多等价定义：Jensen’s 不等式、“降维打击”、一阶条件、二阶条件。这里我们主要关注其中两个：</p>
<ol>
<li>Jensen’s 不等式：$f(\theta x+(1-\theta) y) \leq \theta f(x)+(1-\theta) f(y)$</li>
<li>一阶条件等价于<strong>梯度单调性</strong>：$(\nabla f(x)-\nabla f(y))^{T}(x-y) \geq 0 \quad \text { for all } x, y \in \operatorname{dom} f$</li>
</ol>
<p>也就是说凸函数的梯度 $\nabla f: R^n\to R^n$ 是一个<strong>单调映射</strong>。</p>
<h2 id="2-Lipschitz-continuity"><a href="#2-Lipschitz-continuity" class="headerlink" title="2. Lipschitz continuity"></a>2. Lipschitz continuity</h2><p>函数 $f$ 的梯度满足<strong>利普希茨连续(Lipschitz continuous)</strong>的定义为</p>
<script type="math/tex; mode=display">
\|\nabla f(x)-\nabla f(y)\|_{*} \leq L\|x-y\| \quad \text { for all } x, y \in \operatorname{dom} f</script><p>也被称为 <strong>L-smooth</strong>。有了这个条件，我们可以推出很多个等价性质，这里省略了证明过程</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-lipschitz.PNG" alt="lipschitz"></p>
<blockquote>
<p>也就是说下面的式子都是等价的</p>
<script type="math/tex; mode=display">
\|\nabla f(x)-\nabla f(y)\|_{*} \leq L\|x-y\| \quad \text { for all } x, y \in \operatorname{dom} f</script><script type="math/tex; mode=display">
(\nabla f(x)-\nabla f(y))^{T}(x-y) \leq L\|x-y\|^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><script type="math/tex; mode=display">
f(y) \leq f(x)+\nabla f(x)^{T}(y-x)+\frac{L}{2}\|y-x\|^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><script type="math/tex; mode=display">
(\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|_{*}^{2} \quad \text { for all } x, y</script><script type="math/tex; mode=display">
g(x)=\frac{L}{2}\Vert x\Vert_2^2-f(x) \ \text{ is convex}</script><p><strong>Remarks 1</strong>：</p>
<p>上面的第 3 个式子</p>
<script type="math/tex; mode=display">
f(y) \leq f(x)+\nabla f(x)^{T}(y-x)+\frac{L}{2}\Vert y-x\Vert^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><p>实际上定义了一个<strong>二次曲线</strong>，这个曲线是原始函数的 <strong>Quadratic upper bound</strong></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-quadra-upper.PNG" alt="Quadratic upper bound"></p>
<p>并且由这个式子可以推导出</p>
<script type="math/tex; mode=display">
\frac{1}{2 L}\Vert\nabla f(z)\Vert_{*}^{2} \leq f(z)-f\left(x^{\star}\right) \leq \frac{L}{2}\left\Vert z-x^{\star}\right\Vert^{2} \quad \text { for all } z</script><p>这个式子中的上界 $\frac{L}{2}\left|z-x^{\star}\right|^{2}$ 带有 $x^\star$ 是未知的，而下界只与当前值 $z$ 有关，因此在优化过程中我们可以判断当前的 $f(z)$ 与最优值的距离至少为 $\frac{1}{2 L}|\nabla f(z)|_{*}^{2}$，如果这个值大于0，那么我们一定还没得到最优解。</p>
<p><strong>Remarks 2</strong>：</p>
<p>上面的最后一个式子</p>
<script type="math/tex; mode=display">
(\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|_{*}^{2} \quad \text { for all } x, y</script><p>被称为 $\nabla f$ 的 <strong>co-coercivity</strong> 性质。（这其实有点像 $\nabla f$ 的反函数的 L-smooth 性质，所以它跟 $\nabla f$ 的 L-smooth 性质是等价的）</p>
</blockquote>
<h2 id="3-强凸函数"><a href="#3-强凸函数" class="headerlink" title="3. 强凸函数"></a>3. 强凸函数</h2><p>满足如下性质的函数被称为 <strong>m-强凸(m-strongly convex)</strong>的</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta) y) \leq \theta f(x)+(1-\theta) f(y)-\frac{m}{2} \theta(1-\theta)\|x-y\|^{2} \quad \text { for all } x, y\in\text{dom}f,\theta\in[0,1]</script><p>m-强凸跟前面的 L-smooth 实际上非常像，只不过一个定义了上界，另一个定义了下界。</p>
<blockquote>
<p>类似上面的 L-smooth 性质，我们课可以得到下面几个式子是<strong>等价</strong>的</p>
<script type="math/tex; mode=display">
f \text{ is m-strongly convex}</script><script type="math/tex; mode=display">
(\nabla f(x)-\nabla f(y))^{T}(x-y) \geq m\|x-y\|^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><script type="math/tex; mode=display">
f(y) \geq f(x)+\nabla f(x)^{T}(y-x)+\frac{m}{2}\|y-x\|^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><script type="math/tex; mode=display">
g(x) = f(x)-\frac{m}{2}\Vert x\Vert^2 \ \text{ is convex}</script></blockquote>
<p>注意上面第3个式子不等号右遍实际上又定义了一个二次曲线，这个二次曲线是原函数的下界。与前面的二次上界类比可以得到</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Quadratic lower bound</th>
<th>Quadratic upper bound</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-quadra-lower.PNG" alt="Quadratic lower bound"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-quadra-upper.PNG" alt="Quadratic upper bound"></td>
</tr>
<tr>
<td>$f(y) \geq f(x)+\nabla f(x)^{T}(y-x)+\frac{m}{2}\Vert y-x\Vert^{2}$</td>
<td>$f(y) \leq f(x)+\nabla f(x)^{T}(y-x)+\frac{L}{2}\Vert y-x\Vert^{2}$</td>
</tr>
<tr>
<td>$\Longrightarrow \frac{m}{2}\left\Vert z-x^{\star}\right\Vert^{2} \leq f(z)-f\left(x^{\star}\right) \leq \frac{1}{2 m}\Vert\nabla f(z)\Vert_{*}^{2}$</td>
<td>$\Longrightarrow \frac{1}{2 L}\Vert\nabla f(z)\Vert_{*}^{2} \leq f(z)-f\left(x^{\star}\right) \leq \frac{L}{2}\left\Vert z-x^{\star}\right\Vert^{2}$</td>
</tr>
</tbody>
</table>
</div>
<p><strong><em>例子</em></strong>：如果函数 $f$ 既是 m-强凸的，又是(关于2范数) L-smooth 的，那么</p>
<ol>
<li>函数 $h(x)=f(x)-\frac{m}{2}\Vert x\Vert^2$ 是 <strong>(L-m)-smooth</strong> 的</li>
<li>函数 $h$ 的 co-coercivity 可以写为</li>
</ol>
<script type="math/tex; mode=display">
(\nabla f(x)-\nabla f(y))^{T}(x-y) \geq \frac{m L}{m+L}\|x-y\|_{2}^{2}+\frac{1}{m+L}\|\nabla f(x)-\nabla f(y)\|_{2}^{2} \quad \text { for all } x, y \in \operatorname{dom} f</script><h2 id="4-梯度方法收敛性分析"><a href="#4-梯度方法收敛性分析" class="headerlink" title="4. 梯度方法收敛性分析"></a>4. 梯度方法收敛性分析</h2><p>下面给出一些常见梯度下降方法的分析。先回顾一下梯度方法的一般表达式</p>
<script type="math/tex; mode=display">
x_{k+1}=x_{k}-t_k \nabla f(x_k)</script><p>首先有一些假设</p>
<ol>
<li>$f$ convex 且可导，$\text{dom}f=R^n$</li>
<li>$\nabla f$ 关于2范数 L-Lipschitz continuous</li>
<li>最优解有限且可取</li>
</ol>
<h3 id="4-1-固定步长"><a href="#4-1-固定步长" class="headerlink" title="4.1 固定步长"></a>4.1 固定步长</h3><p>固定步长为 $t$，则 $x^+=x-t\nabla f(x)$，根据 L-smooth 性质有</p>
<script type="math/tex; mode=display">
f(x-t \nabla f(x)) \leq f(x)-t\left(1-\frac{L t}{2}\right)\|\nabla f(x)\|_{2}^{2}</script><p>如果 $0 &lt; t \leq 1/L$，则有</p>
<script type="math/tex; mode=display">
f\left(x^{+}\right) \leq f(x)-\frac{t}{2}\|\nabla f(x)\|_{2}^{2}</script><p>这表明(只要步长 $t$ 比较小)<strong>函数值总是在不断减小</strong>的。从上面的式子结合凸函数性质我们还可以得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x^{+}\right)-f^{\star} & \leq \nabla f(x)^{T}\left(x-x^{\star}\right)-\frac{t}{2}\|\nabla f(x)\|_{2}^{2} \\
&=\frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x-x^{\star}-t \nabla f(x)\right\|_{2}^{2}\right) \\
&=\frac{1}{2 t}\left(\left\|x-x^{\star}\right\|_{2}^{2}-\left\|x^{+}-x^{\star}\right\|_{2}^{2}\right)
\end{aligned}</script><p>从这个式子可以得到我们<strong>到最优点 $x^\star$ 的距离在不断减小</strong>。那么可以得到下面的式子</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{k}\left(f\left(x_{i}\right)-f^{\star}\right) & \leq \frac{1}{2 t} \sum_{i=1}^{k}\left(\left\|x_{i-1}-x^{\star}\right\|_{2}^{2}-\left\|x_{i}-x^{\star}\right\|_{2}^{2}\right) \\
&=\frac{1}{2 t}\left(\left\|x_{0}-x^{\star}\right\|_{2}^{2}-\left\|x_{k}-x^{\star}\right\|_{2}^{2}\right) \\
& \leq \frac{1}{2 t}\left\|x_{0}-x^{\star}\right\|_{2}^{2}
\end{aligned} \\
\Longrightarrow f(x_k)-f^\star\leq\frac{1}{k}\sum_{i=1}^{k}\left(f\left(x_{i}\right)-f^{\star}\right) \leq \frac{1}{2 kt}\left\|x_{0}-x^{\star}\right\|_{2}^{2}</script><p>因此普通的固定步长的梯度下降有以下收敛性质</p>
<ol>
<li>$f(x_{k+1}) &lt; f(x_k)$</li>
<li>$\Vert x_{k+1}-x^\star\Vert &lt; \Vert x_{k}-x^\star\Vert$</li>
<li>$f(x_k)-f^\star\leq \frac{1}{2 kt}\left|x_{0}-x^{\star}\right|_{2}^{2}$，要想满足精度 $f(x_k)-f^\star \leq \epsilon$ 需要的迭代次数为 $O(1/\epsilon)$</li>
</ol>
<h3 id="4-2-线搜索"><a href="#4-2-线搜索" class="headerlink" title="4.2 线搜索"></a>4.2 线搜索</h3><p>线搜索就是每步都要计算合适的步长，计算方法为不断地迭代 $t_k:=\beta t_k,0&lt;\beta&lt;1$ 直到 $t_k$ 满足下面的条件</p>
<script type="math/tex; mode=display">
f\left(x_{k}-t_{k} \nabla f\left(x_{k}\right)\right)<f\left(x_{k}\right)-\alpha t_{k}\left\|\nabla f\left(x_{k}\right)\right\|_{2}^{2}</script><p>形象理解就是下面这幅图，一开始我们的 $t_k$ 可能很大，表示梯度下降的步长过大，不能使函数值减小，那我们就减小步长 $t_k=\beta t_k$，直到进入绿线与蓝线交点左侧这部分，我们就可以保证一定有 $f(x_{k+1})&lt;f(x_k)$，这时就是我们要取的 $t_k$，这也是线搜索的含义，线搜索实际上就是在搜索合适的步长 $t_k$。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-line-search.PNG" alt="line search"></p>
<p> 主要到上面的式子中有一个参数 $\alpha$ 会影响我们的搜索结果，比如上图中 $\alpha$ 越大，则绿线的斜率越大，那么最终搜索到的 $t_k$ 应该就越小，表示我们每一步的步长都会更小。实际中往往取 $\alpha=1/2$，此时理想的搜索结果实际上就是 quadratic upper bound 的最小值点。也就是说我们用二次上界曲线来近似待优化的函数，而二次上界的最小值点对应的步长就是 $t=1/L$，但由于我们是线搜索，实际得到的 $t_k$ 一般会比这个值略小一点。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-line-search2.PNG" alt="line search"></p>
<p>另一方面为了保证线搜索在有限步能够终止，还需要满足 $t_k\ge t_\min =\min\{\hat{t},\beta/L\}$，其中 $\hat{t}$ 是预先指定的一个参数。</p>
<p>那么线搜索的收敛性怎么样呢？首先根据线搜索的定义一定有</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x_{i+1}\right) & \leq f\left(x_{i}\right)-\frac{t_{i}}{2}\left\|\nabla f\left(x_{i}\right)\right\|_{2}^{2} \\
& \leq f^{\star}+\nabla f\left(x_{i}\right)^{T}\left(x_{i}-x^{\star}\right)-\frac{t_{i}}{2}\left\|\nabla f\left(x_{i}\right)\right\|_{2}^{2} \\
&=f^{\star}+\frac{1}{2 t_{i}}\left(\left\|x_{i}-x^{\star}\right\|_{2}^{2}-\left\|x_{i+1}-x^{\star}\right\|_{2}^{2}\right)
\end{aligned}</script><p>这表明 $f(x_{i+1})<f(x_i),\left\|x_{i}-x^{\star}\right\|_{2}>\left|x_{i+1}-x^{\star}\right|_{2}$，类似前面固定步长的分析，可以得到</p>
<script type="math/tex; mode=display">
f(x_k)-f^\star\leq\frac{1}{k}\sum_{i=1}^{k}\left(f\left(x_{i}\right)-f^{\star}\right) \leq \frac{1}{2 kt_\min}\left\|x_{0}-x^{\star}\right\|_{2}^{2}</script><p>因此对于线搜索的方法，我们可以得到如下的收敛性质</p>
<ol>
<li>$f(x_{i+1})&lt;f(x_i)$</li>
<li>$\left|x_{i}-x^{\star}\right|_{2}&gt;\left|x_{i+1}-x^{\star}\right|_{2}$</li>
<li>$f(x_k)-f^\star\leq \frac{1}{2 kt_\min}\left|x_{0}-x^{\star}\right|_{2}^{2}$</li>
</ol>
<p>所以线搜索实际上并不能提高收敛速度的阶，他与固定步长的方法都是 $O(1/k)$ 的，为 <strong>sublinear 收敛</strong>。</p>
<h3 id="4-3-一阶方法的收敛极限"><a href="#4-3-一阶方法的收敛极限" class="headerlink" title="4.3 一阶方法的收敛极限"></a>4.3 一阶方法的收敛极限</h3><p>不管是固定步长还是线搜索，前面的方法都是一阶方法，即</p>
<script type="math/tex; mode=display">
x_{k+1}\in x_{0}+\operatorname{span}\left\{\nabla f\left(x_{0}\right), \nabla f\left(x_{1}\right), \ldots, \nabla f\left(x_{k}\right)\right\}</script><p>而理论上也证明一阶方法的收敛速度存在极限。</p>
<p><strong>定理(Nesterov)</strong>： for every integer $k ≤ (n−1)/2$ and every $x_0$, there exist functions in the problem class such that for any ﬁrst-order method</p>
<script type="math/tex; mode=display">
f\left(x_{k}\right)-f^{\star} \geq \frac{3}{32} \frac{L\left\|x_{0}-x^{\star}\right\|_{2}^{2}}{(k+1)^{2}}</script><p>也就是说收敛速度最多也就是 $O(1/k^2)$。</p>
<h3 id="4-4-强凸函数的梯度方法"><a href="#4-4-强凸函数的梯度方法" class="headerlink" title="4.4 强凸函数的梯度方法"></a>4.4 强凸函数的梯度方法</h3><p>对于强凸函数，即使采用固定步长的梯度方法，也可以得到<strong>线性收敛速度</strong>！这就是强凸性带来的好处。</p>
<p>考虑 $0&lt;t&lt;2/(m+L)$，我们有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{+}-x^{\star}\right\|_{2}^{2} &=\left\|x-t \nabla f(x)-x^{\star}\right\|_{2}^{2} \\
&=\left\|x-x^{\star}\right\|_{2}^{2}-2 t \nabla f(x)^{T}\left(x-x^{\star}\right)+t^{2}\|\nabla f(x)\|_{2}^{2} \\
& \leq\left(1-t \frac{2 m L}{m+L}\right)\left\|x-x^{\star}\right\|_{2}^{2}+t\left(t-\frac{2}{m+L}\right)\|\nabla f(x)\|_{2}^{2} \\
& \leq\left(1-t \frac{2 m L}{m+L}\right)\left\|x-x^{\star}\right\|_{2}^{2}
\end{aligned}</script><p>也就是说可以得到</p>
<script type="math/tex; mode=display">
\left\|x_{k}-x^{\star}\right\|_{2}^{2} \leq c^{k}\left\|x_{0}-x^{\star}\right\|_{2}^{2}, \quad c=1-t \frac{2 m L}{m+L} \\
f\left(x_{k}\right)-f^{\star} \leq \frac{L}{2}\left\|x_{k}-x^{\star}\right\|_{2}^{2} \leq \frac{c^{k} L}{2}\left\|x_{0}-x^{\star}\right\|_{2}^{2}</script><p>注意到前面是反比例下降，这里变成了指数下降。如果要打到精度 $f(x_k)-f^\star \leq \epsilon$ 需要的迭代次数为 $O(\log(1/\epsilon))$</p>
<h2 id="5-BB-方法"><a href="#5-BB-方法" class="headerlink" title="5. BB 方法"></a>5. BB 方法</h2><p><strong>Barzilai-Borwein (BB) method</strong> 也是梯度下降方法的一种，他主要是通过近似牛顿方法来实现更快的收敛速度，同时避免计算二阶导数带来的计算复杂度。</p>
<p>如果我们记 $\boldsymbol{g}^{(k)}=\nabla f\left(\boldsymbol{x}^{(k)}\right) \text { and } \boldsymbol{F}^{(k)}=\nabla^{2} f\left(\boldsymbol{x}^{(k)}\right)$，那么<strong>一阶方法</strong>就是 $\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\alpha_{k} \boldsymbol{g}^{(k)}$，其中步长 $\alpha_k$ 可以是固定的，也可以是线搜索获得的，一阶方法简单但是收敛速度慢。<strong>牛顿方法</strong>就是 $\boldsymbol{x}^{(k+1)}=\boldsymbol{x}^{(k)}-\left(\boldsymbol{F}^{(k)}\right)^{-1} \boldsymbol{g}^{(k)}$，其收敛速度更快，但是海森矩阵计算代价较大。而 <strong>BB方法</strong>就是用 $\alpha_{k} \boldsymbol{g}^{(k)}$ 来近似 $\left(\boldsymbol{F}^{(k)}\right)^{-1} \boldsymbol{g}^{(k)}$。</p>
<p>怎么近似呢？假如定义 $s^{(k-1)}:=x^{(k)}-x^{(k-1)} \text { and } y^{(k-1)}:=g^{(k)}-g^{(k-1)}$，那么海森矩阵实际上就是</p>
<script type="math/tex; mode=display">
\boldsymbol{F}^{(k)}s^{(k-1)}=y^{(k-1)}</script><p>现在的想法就是用 $(\alpha_k I)^{-1}$ 来近似 $\boldsymbol{F}^{(k)}$，那么应该有</p>
<script type="math/tex; mode=display">
(\alpha_k I)^{-1}s^{(k-1)}=y^{(k-1)}</script><p>这个问题用最小二乘就可以解决了，下面两种选择都可以</p>
<script type="math/tex; mode=display">
\alpha_{k}^{-1}=\underset{\beta}{\arg \min } \frac{1}{2}\left\|s^{(k-1)} \beta-\boldsymbol{y}^{(k-1)}\right\|^{2} \Longrightarrow \alpha_{k}^{1}=\frac{\left(s^{(k-1)}\right)^{T} s^{(k-1)}}{\left(s^{(k-1)}\right)^{T} \boldsymbol{y}^{(k-1)}} \\\alpha_{k}=\underset{\alpha}{\arg \min } \frac{1}{2}\left\|s^{(k-1)}-\boldsymbol{y}^{(k-1)} \alpha\right\|^{2} \Longrightarrow \alpha_{k}^{2}=\frac{\left(\boldsymbol{s}^{(k-1)}\right)^{T} \boldsymbol{y}^{(k-1)}}{\left(\boldsymbol{y}^{(k-1)}\right)^{T} \boldsymbol{y}^{(k-1)}}</script><p>这两个解有一个微妙的不同点在于 $\alpha_k^1$ 的分母 $\left(s^{(k-1)}\right)^{T} \boldsymbol{y}^{(k-1)}$ 有可能等于 0，这会给计算带来麻烦，而 $\alpha_k^2$ 则不会。</p>
<p>BB方法主要有以下几个特点：</p>
<ol>
<li>几乎不需要额外的计算量，但是往往会带来极大的性能增益；</li>
<li>实际应用中这两个表达式用哪个都可以，甚至还可以交换使用，用哪个更好一般与具体的问题有关；</li>
<li>收敛性很难证明，没有收敛性的保证。比如下面的例子，他甚至不是单调下降的。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/15-bb.PNG" alt="BB method"></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>利普希兹连续</tag>
        <tag>co-coercivity</tag>
        <tag>强凸函数</tag>
        <tag>线搜索</tag>
        <tag>BB方法</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记14：SDP Representablity</title>
    <url>/2020/04/05/optimization/ch14-sdp-rep/</url>
    <content><![CDATA[<p>这一节简单介绍一个 SDP Representablity（SDP-Rep），这个概念的提出主要是为了便于判断某个问题是否可以转化为 SDP 优化问题。</p>
<p><strong>定义</strong>：<strong>集合</strong> $X\subseteq R^n$ 是 <strong>SDP-Rep</strong> 的，如果他可以表示为</p>
<script type="math/tex; mode=display">
X=\{x| \text{there exist }u\in R^k \text{ such that for some } \\A_i,B_j,C\in R^{m\times m}:\sum_i x_iA_i+\sum_j u_jB_j +C \succeq0 \}</script><p>注：它实际上就是下面集合的一个子空间投影</p>
<script type="math/tex; mode=display">
\{(x,u)|\sum_i x_iA_i+\sum_j u_jB_j +C \succeq0\}</script><p>这个定义实际上说明了集合 $X$ 可以用一个 LMI 表示，因而如果 $X$ 为优化问题的定义域，则该定义域可以用一个 SDP 约束条件来表示。例如：如果 $X$ 为 SDP-Rep，则 $\min_{x\in X}c^Tx$ 是一个 SDP 问题。</p>
<p><strong>定义</strong>：如果如果<strong>函数</strong> $f(x)$ 的 epigraph 是 SDP-Rep 的，那么函数 $f(x)$ 就是 <strong>SDP-Rep</strong></p>
<script type="math/tex; mode=display">
\text{epi}(f)=\{(x_0,x)|f(x)\le x_0\}</script><p>该定义表明，如果 $f(x)$ 为 SDP-Rep，则优化问题 $\min_x f(x)$ 是一个 SDP 问题。</p>
<p>对 SDP-Rep 集合进行一些变换之后仍然是 SDP-Rep 的：如果 $X,Y$ 都是 SDP-Rep 的</p>
<ul>
<li>Minkowski sum $X+Y$</li>
<li>intersection $X\cap Y$</li>
<li>Affine pre-image $A^{-1}(X)$ if $A$ is affine</li>
<li>Affine map $A(X)$ if $A$ is affine</li>
<li>Cartesian product $X\times Y=\{(x,y)|x\in X,y\in Y\}$</li>
</ul>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>SDP</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记13：互补性条件</title>
    <url>/2020/03/27/optimization/ch13-complementary/</url>
    <content><![CDATA[<p>前面我们讲了凸优化问题、对偶原理、拉格朗日函数、KKT 条件，还从几何角度解释了强对偶性，那么这一节将从<strong>代数角度解释强对偶性</strong>，并给出 KKT 条件中的<strong>互补性条件</strong>的新的表达形式。</p>
<a id="more"></a>
<h2 id="1-LP-SOCP-SDP"><a href="#1-LP-SOCP-SDP" class="headerlink" title="1. LP / SOCP / SDP"></a>1. LP / SOCP / SDP</h2><p>在此之前我们先回顾一下比较重要的三类凸优化问题：LP、SOCP、SDP，为什么这么说呢？因为本质上这三类问题非常相似。我们先来回顾一下三类问题</p>
<script type="math/tex; mode=display">
\begin{aligned}LP:\quad \text{minimize} \quad& c^{T} x \\\text{subject to} \quad& A x=b \\& x \succeq 0 \qquad(\bigstar)\\SOCP:\quad \text {minimize} \quad& c^{T} x \\\text {subject to} \quad& Ax=b \\& \left\|\bar{x}\right\|_{2} \leq x_0 \iff x=(x_0,\bar{x})\succeq_Q 0 \qquad(\bigstar)\\SDP:\quad \text{minimize} \quad& \left<C,X\right>\triangleq tr(CX) \quad X\in S^n \\\text{subject to} \quad& A(X)=b \\& X \succeq 0 \qquad(\bigstar)\\\end{aligned}</script><p>注：上面 SDP 中定义了矩阵内积 $\left<C,X\right>\triangleq tr(CX)$，这跟向量内积非常类似，而且可以交换 $\left<C,X\right>=\left<X,C\right>$；还定义了算子 $A(X)=\left(\left<A_1,X\right>,\cdots,\left<A_n,X\right>\right)$。</p>
<p>上面三种凸优化问题中，不等式约束都用星号标记出来了，可以看出，他们的形式非常相似，其实都是在不同维度的<strong>正常锥</strong>里面。不过 LP 的可行域为多面体，最优解往往位于极值点，因此较为简单；但是 SOCP 的可行域则是一个多面体与一个锥的交集，同样的 SDP 和 SOCP 的可行域都是 Nonpolyhedral 的，这就使他们比 LP 要更难求解。</p>
<h2 id="2-强对偶性的代数解释"><a href="#2-强对偶性的代数解释" class="headerlink" title="2. 强对偶性的代数解释"></a>2. 强对偶性的代数解释</h2><p>考虑 <strong>LP</strong> 问题及其对偶问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P):\quad \text{minimize} \quad& c^{T} x \\
\text{subject to} \quad& A x=b \\
& x \succeq 0 \\
(D):\quad \text {maximize} \quad& b^Ty\\
\text {subject to} \quad& A^Ty+s=c \\
& s\succeq0
\end{aligned}</script><p>那么原问题与对偶问题的 <strong>duality gap</strong> 就是</p>
<script type="math/tex; mode=display">
c^Tx - b^Ty = x^Ts</script><p>我们很容易验证 $x^Ts\ge0$，也就是弱对偶性。如果满足<strong>强对偶性(Strong duality)</strong>的话，应该有</p>
<script type="math/tex; mode=display">
x^Ts=0 \iff x_is_i=0,i=1,...,n</script><p>那么我们再来回顾一下 KKT 条件的 4 个部分：</p>
<ol>
<li>Primal feasibility：$Ax=b,x\ge0$</li>
<li>Dual feasibility：$A^Ty+s=c,s\ge0$</li>
<li>Complementarity：$x_is_i=0,i=1,…,n$</li>
<li>梯度条件包含在对偶可行性里面了</li>
</ol>
<p>因此实际上利用上面 3 个约束就可以求解最优解了。对于上面的互补性条件 $x_is_i=0$ 我们可以定义一个算子</p>
<script type="math/tex; mode=display">
x\circ s := (x_1s_1,...,x_ns_n)^T = \text{diag}(x)s :=L_xs=L_xL_s\mathbf{1}</script><p>它满足 $x\circ \mathbf{1}=x$。这里其实只是定义了一个新的符号，并没有引入新的东西，之所以这么做是为了与后面的 SOCP、SDP 统一起来用类似的符号表示，便于计算机进行优化计算。</p>
<p>现在我们再来看看 <strong>SOCP</strong> 问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
(P):\quad \text {minimize} \quad& c^{T} x \\
\text {subject to} \quad& Ax=b \\
& x\succeq_Q 0 \\
(D):\quad \text {maximize} \quad& b^{T} y \\
\text {subject to} \quad& A^Ty+s=c \\
& s\succeq_Q 0
\end{aligned}</script><blockquote>
<p><strong>Remarks</strong>：注意<strong>二阶锥(Second order cone)</strong>的对偶锥还是其自身。</p>
<p>对偶锥是其自身怎么直观理解呢？大家想象三维中的这样一个锥，我们任取一个过锥的顶点$(0,0,0)$的竖直平面，切出来的是应该一个直角吧。那么假如说我们考虑任意的锥呢？切出来的这个角如果不是直角，假如是钝角，那么他的对偶锥一定不是其自身，要比自身“小”，锐角也类似。这里提到这个直观理解在后面会用到。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/norm_cone.PNG" alt="SOC"></p>
</blockquote>
<p>同样的我们可以得到 dualligity gap 为</p>
<script type="math/tex; mode=display">
x^Ts=c^Tx-b^Ty\ge0</script><p>如果想得到强对偶性则需要 </p>
<script type="math/tex; mode=display">
\begin{aligned}& x^Ts=0,x\succeq_Q 0,s\succeq_Q 0 \\\iff& x_0s_i+x_is_0=0,i=1,...,n \\\iff& x_0\bar{s}+s_0\bar{x}=0\end{aligned}</script><p>上面这个证明可以利用 Cauchy-Schwarz 不等式 $x^Ts=x_0s_0+\sum_ix_is_i\ge0$，取等条件即为上式。这从几何角度理解就如下图，其中 $x,s$ 为正交的，而且他们向后 $R^n$ 维子空间的投影 $\bar{x},\bar{s}$ 是反向平行的。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/13-cone.PNG" alt="SOC"></p>
<blockquote>
<p><strong>Remarks</strong>：$x,s$ 均为锥 $Q$ 当中的向量，二者内积为 0，则表明他们<strong>相互正交</strong>。假如我们固定了 $x$，$x$ 作为法向量定义了一个超平面 $A$，超平面内的任意一个向量都应垂直于 $x$，所里直观理解的话 $x$ 应该有无穷多个单位正交向量。但是根据上面的结果，$s=(s_0,s_0\bar{x}/x_0)$ 只有一个方向，这说明锥 $Q$ 内部只有一个方向上的向量与 $x$ 正交！也就是说超平面 $A$ 与锥 $Q$ 的交集是一条线，实际上就是说 $A$ 与 $Q$ <strong>相切</strong>！</p>
<p>再回想一下我们前面提到这个锥 $Q$ 的顶角应该是一个直角，假如 $x$ 位于 $Q$ 的内部（不在边界上），那么 $A\cap Q=\{(0,0,0)\}$，只有一个点，$s$ 也就不存在。所以如果想要 $x^Ts=0$ 有非零解，就<strong>一定要求 $x$ 在 $Q$ 的边界上</strong>，这个时候实际上就有 $x_0=\Vert\bar{x}\Vert,s_0=\Vert\bar{s}\Vert$ 。这一点很容易从我们上面的式子 $x_0s_i+x_is_0=0$ 得到验证。</p>
</blockquote>
<p>同样得回顾一下 KKT 条件</p>
<ol>
<li>$Ax=b,x\succeq_Q0$</li>
<li>$A^Ty+s=c,s\succeq_Q0$</li>
<li>$x^Ts=0,\quad x_0s_i+x_is_0=0$</li>
</ol>
<p>这里再定义一个算子</p>
<script type="math/tex; mode=display">
x\circ s = \left(\begin{array}{c}x_{0} \\x_{1} \\\vdots \\x_{n}\end{array}\right) \circ\left(\begin{array}{c}s_{0} \\s_{1} \\\vdots \\s_{n}\end{array}\right)=\left(\begin{array}{c}x^{\top} s \\x_{0} s_{1}+s_{0} x_{1} \\\vdots \\x_{0} s_{n}+s_{0} x_{n}\end{array}\right) \\</script><p>为了简化表示可以写成下面的形式，这样跟 LP 就统一了。</p>
<script type="math/tex; mode=display">
L_{x}=\operatorname{Arw}(x)=\left(\begin{array}{ll}x_{0} & \bar{x}^{\top} \\\bar{x} & x_{0} I\end{array}\right) \notag\\x \circ s=\operatorname{Arw}(x) s=\operatorname{Arw}(x) \operatorname{Arw}(s) e</script><p>算子满足性质</p>
<ol>
<li>$x\circ s = s\circ x$</li>
<li>$x\circ(x\circ x)=(x\circ x)\circ x$</li>
<li>$x \circ\left(x^{2} \circ y\right)=x^{2} \circ(x \circ y)$</li>
<li>$e=(1,0,…,0)^T,x\circ e=x$</li>
</ol>
<p>但是注意不满足结合律 $x \circ(y \circ z) \neq(x \circ y) \circ z$ 。</p>
<p>最后我们再来看看 <strong>SDP</strong> 问题及其对偶问题</p>
<script type="math/tex; mode=display">
\begin{aligned}(P):\quad \text{minimize} \quad& \left<C,X\right> \\\text{subject to} \quad& \left<A_i,X\right>=b_i,i=1,...,m \\& X \succeq 0\\(D):\quad \text{minimize} \quad& b^Ty \\\text{subject to} \quad& \sum_i y_iA_i+S=C \\& S\succeq 0\end{aligned}</script><p>同样的 duality gap 为</p>
<script type="math/tex; mode=display">
\begin{aligned}\left<X,S\right> &= \left<C,X\right> - b^Ty \\&= \left<X,S^{1/2}S^{1/2}\right> = \left<S^{1/2}X,S^{1/2}\right> \ge0\end{aligned}</script><p><strong>强对偶性</strong>则要求 </p>
<script type="math/tex; mode=display">
X\succeq0,S\succeq0,\left<X,S\right>=0 \iff XS=0 \iff \frac{XS+SX}{2}=0</script><p>证明：$\left<X,S\right>=\left<S^{1/2}X^{1/2},X^{1/2}S^{1/2}\right>=0 \iff X^{1/2}S^{1/2}=0$</p>
<p>为了简化表达，我们再定义一个算子 $X\circ S=\frac{XS+SX}{2}$，它满足以下性质</p>
<ol>
<li>$X\circ S = S\circ X$</li>
<li>$X\circ(X\circ X)=(X\circ X)\circ X$</li>
<li>$X \circ\left(X^{2} \circ Y\right)=X^{2} \circ(X \circ Y)$</li>
<li>$X\circ I = X$</li>
</ol>
<p>注意不满足结合律 $X \circ(Y \circ Z) \neq(X \circ Y) \circ Z$</p>
<p>再来复习一下 KKT 条件</p>
<ol>
<li>$\left<A_i,X\right>=b_i$</li>
<li>$\sum_i y_iA_i+S=C$</li>
<li>$X\circ S=0$</li>
</ol>
<p>最后我们总结一下</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/13-comple.jpg" alt="summary"></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>SDP</tag>
        <tag>KKT条件</tag>
        <tag>互补性条件</tag>
        <tag>LP</tag>
        <tag>SOCP</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记12：KKT 条件</title>
    <url>/2020/03/26/optimization/ch12-kkt/</url>
    <content><![CDATA[<p>上一小节讲了拉格朗日函数，可以把原始问题转化为对偶问题，并且对偶问题是凸的。我们还得到了弱对偶性和强对偶性的概念，并且提到了 Slater Condition 保证凸问题的强对偶性成立，并且给出了一些几何的直观解释。那么在这一节，我们将引出著名的 <strong>KKT 条件</strong>，它给出了最优解需要满足的必要条件，是求解优化问题最优解的一个重要方式。</p>
<a id="more"></a>
<h2 id="1-KKT-条件"><a href="#1-KKT-条件" class="headerlink" title="1. KKT 条件"></a>1. KKT 条件</h2><p>我们首先回顾一下拉格朗日函数，考虑下面的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&h_{i}(x)=0, \quad i=1, \ldots, p
\end{aligned}</script><p>那么他的拉格朗日函数就是</p>
<script type="math/tex; mode=display">
L(x,\lambda,\nu)=f_0(x)+\lambda^Tf(x)+\nu^Th(x)</script><p>首先，我们看对偶函数</p>
<script type="math/tex; mode=display">
g(\lambda,\nu)=\inf_{x\in\mathcal{D}}\left(f_0(x)+\lambda^Tf(x)+\nu^Th(x)\right)</script><p>对偶问题实际上就是</p>
<script type="math/tex; mode=display">
d^\star = \sup_{\lambda,\nu}g(\lambda,\nu)=\sup_{\lambda,\nu}\inf_x L(x,\lambda,\nu)</script><p>然后我们再看原问题，由于 $\lambda\succeq0,f(x)\preceq0$，我们有</p>
<script type="math/tex; mode=display">
f_0(x)=\sup_{\lambda,\nu}L(x,\lambda,\nu)</script><p>原问题的最优解实际上就是</p>
<script type="math/tex; mode=display">
p^\star=\inf_x f_0(x)= \inf_x \sup_{\lambda,\nu}L(x,\lambda,\nu)</script><p><strong>弱对偶性</strong> $p^\star \ge d^\star$ 实际上说的是什么呢？就是 <strong>max-min 不等式</strong></p>
<script type="math/tex; mode=display">
\inf_x \sup_{\lambda,\nu}L(x,\lambda,\nu) \ge \sup_{\lambda,\nu}\inf_x L(x,\lambda,\nu)</script><p><strong>强对偶性</strong>说的又是什么呢？就是上面能够取等号</p>
<script type="math/tex; mode=display">
\inf_x \sup_{\lambda,\nu}L(x,\lambda,\nu) = \sup_{\lambda,\nu}\inf_x L(x,\lambda,\nu) = L({x}^\star,{\lambda}^\star,{\nu}^\star)</script><p>实际上 ${x}^\star,{\lambda}^\star,{\nu}^\star$ 就是<strong>拉格朗日函数的鞍点</strong>！！！（数学家们真实太聪明了！！！妙啊！！！）那么也就是说<strong>强对偶性成立等价于拉格朗日函数存在鞍点(在定义域内)</strong>。</p>
<p>好，如果存在鞍点的话，我们怎么求解呢？还是看上面取等的式子</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_0({x}^\star) = g(\lambda^\star,\nu^\star) &= \inf_x \left( f_0(x)+\lambda^{\star T}f(x)+\nu^{\star T}h(x) \right) \\
& \le f_0(x^\star)+\lambda^{\star T}f(x^\star)+\nu^{\star T}h(x^\star) \\
& \le f_0(x^\star)
\end{aligned}</script><p>这两个不等号必须要取到等号，而第一个不等号取等条件应为</p>
<script type="math/tex; mode=display">
\nabla_x \left( f_0(x)+\lambda^{\star T}f(x)+\nu^{\star T}h(x) \right) =0</script><p>第二个不等号取等条件为</p>
<script type="math/tex; mode=display">
\lambda^\star_i f_i(x^\star)=0,\forall i</script><p>同时，由于 ${x}^\star,{\lambda}^\star,{\nu}^\star$ 还必须位于定义域内，需要满足约束条件，因此上面的几个条件共同构成了 KKT 条件。</p>
<blockquote>
<p><strong>KKT 条件</strong></p>
<ol>
<li>原始约束 $f_i(x)\le0,i=1,…,m, \quad h_i(x)=0,i=1,…,p$</li>
<li>对偶约束 $\lambda\succeq0$</li>
<li>互补性条件(complementary slackness) $\lambda_i f_i(x)=0,i=1,…,m$</li>
<li>梯度条件</li>
</ol>
<script type="math/tex; mode=display">
\nabla f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} \nabla f_{i}(x)+\sum_{i=1}^{p} \nu_{i} \nabla h_{i}(x)=0</script></blockquote>
<h2 id="2-KKT-条件与凸问题"><a href="#2-KKT-条件与凸问题" class="headerlink" title="2. KKT 条件与凸问题"></a>2. KKT 条件与凸问题</h2><blockquote>
<p><strong>Remarks(重要结论)</strong></p>
<ol>
<li>前面推导没有任何凸函数的假设，因此不论是否为凸问题，<strong>如果满足强对偶性，那么最优解一定满足 KKT 条件</strong>。</li>
<li>但是反过来不一定成立，也即 <strong>KKT 条件的解不一定是最优解</strong>，因为如果 $L(x,\lambda^\star,\nu^\star)$ 不是凸的，那么 $\nabla_x L=0$ 并不能保证 $g(\lambda^\star,\nu^\star)=\inf_x L(x,\lambda^\star,\nu^\star)\ne L(x^\star,\lambda^\star,\nu^\star)$，也即不能保证 ${x}^\star,{\lambda}^\star,{\nu}^\star$ 就是鞍点。</li>
</ol>
</blockquote>
<p>但是如果我们假设原问题为凸问题的话，那么 $L(x,\lambda^\star,\nu^\star)$ 就是一个凸函数，由梯度条件 $\nabla_x L=0$ 我们就能得到 $g(\lambda^\star,\nu^\star)=L(x^\star,\lambda^\star,\nu^\star)=\inf_x L(x,\lambda^\star,\nu^\star)$，另一方面根据互补性条件我们有此时 $f_0(x^\star)=L(x^\star,\lambda^\star,\nu^\star)$，因此我们可以得到一个结论</p>
<blockquote>
<p><strong>Remarks(重要结论)</strong>：</p>
<ol>
<li>考虑原问题为凸的，那么若 KKT 条件有解 $\tilde{x},\tilde{\lambda},\tilde{\nu}$，则原问题一定满足强对偶性，且他们就对应原问题和对偶问题的最优解。</li>
<li>但是需要注意的是，KKT 条件可能无解！此时就意味着原问题不满足强对偶性！</li>
</ol>
</blockquote>
<p>假如我们考虑上一节提到的 SCQ 条件，如果凸优化问题满足 SCQ 条件，则意味着强对偶性成立，则此时有结论</p>
<blockquote>
<p><strong>Remarks(重要结论)</strong>：</p>
<p>如果 SCQ 满足，那么 $x$ 为最优解<strong>当且仅当</strong>存在 $\lambda,\nu$ 满足 KKT 条件！</p>
</blockquote>
<p><strong><em>例子 1</em></strong>：等式约束的二次优化问题 $P\in S_+^n$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& (1/2)x^TPx+q^Tx+r \\
\text { subject to } \quad& Ax=b
\end{aligned}</script><p>那么经过简单计算就可以得到 KKT 条件为</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cc}
P & A^{T} \\
A & 0
\end{array}\right]\left[\begin{array}{l}
x^{\star} \\
\nu^{\star}
\end{array}\right]=\left[\begin{array}{c}
-q \\
b
\end{array}\right]</script><p><strong><em>例子 2</em></strong>：注水问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\text { minimize } \quad-\sum_{i=1}^{n} \log \left(\alpha_{i}+x_{i}\right)\\
&\text { subject to } \quad x \succeq 0, \quad \mathbf{1}^{T} x=1
\end{aligned}</script><p>根据上面的结论，$x$ 是最优解当且仅当 $x\succeq0,\mathbf{1}^{T} x=1$，且存在 $\lambda,\nu$ 满足</p>
<script type="math/tex; mode=display">
\lambda \succeq 0, \quad \lambda_{i} x_{i}=0, \quad \frac{1}{x_{i}+\alpha_{i}}+\lambda_{i}=\nu</script><p>根据互补性条件 $\lambda_i x_i=0$ 分情况讨论可以得到</p>
<ul>
<li>如果 $\nu&lt;1/\alpha_i$：$\lambda_i=0,x_i=1/\nu-\alpha_i$</li>
<li>如果 $\nu\ge1/\alpha_i$：$\lambda_i=\nu-1/\alpha_i,x_i=0$</li>
</ul>
<p>整理就可以得到 $\mathbf{1}^{T} x=\sum_i\max\{0,1/\nu-\alpha_i\}$，这个式子怎么理解呢？就像向一个池子里注水一样</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/12-water.png" alt="water filling"></p>
<h2 id="3-扰动与敏感性分析"><a href="#3-扰动与敏感性分析" class="headerlink" title="3. 扰动与敏感性分析"></a>3. 扰动与敏感性分析</h2><p>现在我们再回到原始问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&h_{i}(x)=0, \quad i=1, \ldots, p
\end{aligned}</script><p>我们引入了对偶函数 $g(\lambda,\nu)$，那这两个参数 $\lambda,\nu$ 有什么含义吗？假如我们把原问题放松一下</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq u_i, \quad i=1, \ldots, m\\
&h_{i}(x)=v_i, \quad i=1, \ldots, p
\end{aligned}</script><p>记最优解为 $p^\star(u,v)=\min f_0(x)$，现在对偶问题变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max \quad&  g(\lambda,\nu)-u^T\lambda -v^T\nu\\
\text{s.t.} \quad& \lambda\succeq0
\end{aligned}</script><p>假如说原始对偶问题的最优解为 $\lambda^\star,\nu^\star$，松弛后的对偶问题最优解为 $\tilde{\lambda},\tilde{\nu}$，那么根据弱对偶性原理，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
p^\star(u,v) &\ge g(\tilde\lambda,\tilde\nu)-u^T\tilde\lambda -v^T\tilde\nu \\
&\ge g(\lambda^\star,\nu^\star)-u^{T}\lambda^\star -v^{T}\nu^\star \\
&= p^\star(0,0) - u^{T}\lambda^\star -v^{T}\nu^\star
\end{aligned}</script><p>这像不像关于 $u,v$ 的一阶近似？太像了！实际上，我们有</p>
<script type="math/tex; mode=display">
\lambda_{i}^{\star}=-\frac{\partial p^{\star}(0,0)}{\partial u_{i}}, \quad \nu_{i}^{\star}=-\frac{\partial p^{\star}(0,0)}{\partial v_{i}}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/12-sensitivity.PNG" alt="sensitivity"></p>
<h2 id="4-Reformulation"><a href="#4-Reformulation" class="headerlink" title="4. Reformulation"></a>4. Reformulation</h2><p>前面将凸优化问题的时候，我们提到了Reformulation的几个方法来简化原始问题，比如消去等式约束，添加等式约束，添加松弛变量，epigraph等等。现在当我们学习了对偶问题，再来重新看一下这些方法。</p>
<h3 id="4-1-引入等式约束"><a href="#4-1-引入等式约束" class="headerlink" title="4.1 引入等式约束"></a>4.1 引入等式约束</h3><p><strong><em>例子 1</em></strong>：考虑无约束优化问题 $\min f(Ax+b)$，他的对偶问题跟原问题是一样的。如果我们引入等式约束，原问题和对偶问题变为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_{0}(y) \quad \\
\text{subject to} \quad& A x+b-y=0
\end{aligned}
\quad\qquad
\begin{aligned}
\text{minimize} \quad& b^{T} \nu-f_{0}^{*}(\nu) \\
\text{subject to} \quad& A^{T} \nu=0
\end{aligned}</script><p><strong><em>例子 2</em></strong>：考虑无约束优化 $\min \Vert Ax-b\Vert$，类似的引入等式约束后，对偶问题变为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& b^{T} \nu \\
\text{subject to} \quad& A^{T} \nu=0,\quad \Vert\nu\Vert_*\le1
\end{aligned}</script><h3 id="4-2-显示约束与隐式约束的相互转化"><a href="#4-2-显示约束与隐式约束的相互转化" class="headerlink" title="4.2 显示约束与隐式约束的相互转化"></a>4.2 显示约束与隐式约束的相互转化</h3><p><strong><em>例子 3</em></strong>：考虑原问题如下，可以看出来对偶问题非常复杂</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^{T} x \\
\text{subject to} \quad& A x=b \\
\quad& -1 \preceq x \preceq 1
\end{aligned}

\begin{aligned}
\text{maximize} \quad& -b^{T} \nu-\mathbf{1}^{T} \lambda_{1}-\mathbf{1}^{T} \lambda_{2} \\
\text{subject to} \quad& c+A^{T} \nu+\lambda_{1}-\lambda_{2}=0 \\
\quad& \lambda_{1} \succeq 0, \quad \lambda_{2} \succeq 0 
\end{aligned}</script><p>如果我们原问题的不等式约束条件转化为隐式约束，则有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_{0}(x)=\left\{\begin{array}{ll}c^{T} x & \Vert x\Vert_\infty \preceq 1 \\ \infty & \text { otherwise }\end{array}\right. \\
\text{subject to} \quad& A x=b
\end{aligned}</script><p>然后对偶问题就可以转化为无约束优化问题</p>
<script type="math/tex; mode=display">
\text{maximize} -b^T\nu-\Vert A^T\nu +c\Vert_1</script><h3 id="4-3-转化目标函数与约束函数"><a href="#4-3-转化目标函数与约束函数" class="headerlink" title="4.3 转化目标函数与约束函数"></a>4.3 转化目标函数与约束函数</h3><p><strong><em>例子 4</em></strong>：还考虑上面提到的无约束优化问题 $\min \Vert Ax-b\Vert$，我们可以把目标函数平方一下，得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& (1/2)\Vert y\Vert^2 \\
\text{subject to} \quad& Ax-b=y
\end{aligned}</script><p>然后对偶问题就可以转化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& (1/2)\Vert \nu\Vert_*^2+ b^T\nu \\
\text{subject to} \quad& A^T\nu=0
\end{aligned}</script>]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>KKT条件</tag>
        <tag>对偶原理</tag>
        <tag>拉格朗日函数</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 11：对偶原理 &amp; 拉格朗日函数</title>
    <url>/2020/03/18/optimization/ch11-dual/</url>
    <content><![CDATA[<p>前面讲了凸优化问题的定义，以及一些常见的凸优化问题类型，这一章就要引入著名的拉格朗日函数和对偶问题了。通过对偶问题，我们可以将一些非凸问题转化为凸优化问题，还可以求出原问题的非平凡下界，这对复杂优化问题是很有用的。</p>
<a id="more"></a>
<h2 id="1-拉格朗日函数"><a href="#1-拉格朗日函数" class="headerlink" title="1. 拉格朗日函数"></a>1. 拉格朗日函数</h2><p>考虑凸优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&h_{i}(x)=0, \quad i=1, \ldots, p
\end{aligned}</script><p>假设 $x\in R^n$，定义域为 $\mathcal{D}$，最优解为 $p^\star$。</p>
<p>我们定义<strong>拉格朗日函数(Lagrangian)</strong>为 $L:R^n\times R^m\times R^p\to R$，$\text{dom}L=\mathcal{D}\times R^m\times R^p$</p>
<script type="math/tex; mode=display">
L(x,\lambda,\nu)=f_0(x)+\lambda^Tf(x)+\nu^Th(x)</script><p>再取下确界得到<strong>拉格朗日对偶函数(Lagrange dual function)</strong> $g:R^m\times R^p\to R$</p>
<script type="math/tex; mode=display">
g(\lambda,\nu)=\inf_{x\in\mathcal{D}}\left(f_0(x)+\lambda^Tf(x)+\nu^Th(x)\right)</script><p>这个拉格朗日对偶函数可不得了啦！他有两个很重要的性质：</p>
<blockquote>
<ol>
<li>$g(\lambda,\nu)$ 是<strong>凹函数</strong>（不论原问题是否为凸问题）</li>
<li>如果 $\lambda\succeq 0$，那么 $g(\lambda,\nu)\le p^\star$（对任意 $\lambda\succeq0,\nu$ 都成立）</li>
</ol>
</blockquote>
<p><strong>Remarks</strong>：上面两个性质为什么重要呢？首先由于 $g(\lambda,\nu)\le p^\star$，这可以给出原问题最优解的一个<strong>不平凡下界</strong>，这意味着如果原问题很难求解的时候，我们可以转变思路，求解一个新的优化问题：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& g(\lambda,\nu)\\
\text { subject to } \quad& \lambda\succeq0
\end{aligned}</script><p>另一方面，由于不论原函数是否为凸优化问题，新的问题都是凸的，因此可以方便求解。下面举几个例子。</p>
<p><strong><em>例子 1</em></strong>：原问题为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& x^Tx\\
\text { subject to } \quad& Ax=b
\end{aligned}</script><p>那么可以很容易得到拉格朗日函数为 $L(x,\nu)=x^Tx+\nu^T(Ax-b)$，对偶函数为 $g(\nu)=-(1/4)\nu^TAA^T\nu-b^T\nu$，也即</p>
<p>$p^\star\ge g(\nu)$。</p>
<p><strong><em>例子 2</em></strong>：标准形式的线性规划(LP)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& c^Tx\\
\text { subject to } \quad& Ax=b,\quad x\succeq0
\end{aligned}</script><p>按照定义容易得到对偶问题为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& -b^T\nu\\
\text { subject to } \quad& A^T\nu+c\succeq0
\end{aligned}</script><p><strong><em>例子 3</em></strong>：原问题为最小化范数</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& \Vert x\Vert\\
\text { subject to } \quad& Ax=b
\end{aligned}</script><p>对偶函数为</p>
<script type="math/tex; mode=display">
g(\nu)=\inf_{x} (\Vert x\Vert+\nu^T(b-Ax))
=\begin{cases}b^T\nu & \Vert A^T\nu\Vert_* \le1 \\ -\infty & o.w.\end{cases}</script><p>这个推导过程中用到了<strong>共轭函数</strong>的知识。实际上上面三个例子都是线性等式约束，这种情况下，我们应用定义推导过程中可以很容易联想到共轭函数。（实际上加上线性不等式约束也可以）</p>
<p><strong><em>例子 4</em></strong>：(原问题非凸)考虑 Two-way partitioning (不知道怎么翻译了…)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& x^TWx\\
\text { subject to } \quad& x_i^2=1,\quad i=1,...,n
\end{aligned}</script><p>对偶函数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
g(\nu)&=\inf_{x}\left( x^{T}(W+\operatorname{diag}(\nu)) x \right)-\mathbf{1}^{T} \nu \\
&=\left\{\begin{array}{ll}
-\mathbf{1}^{T} \nu & W+\operatorname{diag}(\nu) \succeq 0 \\
-\infty & \text { otherwise }
\end{array}\right.
\end{aligned}</script><p>于是可以给出原问题最优解的下界为 $p^\star\ge-\mathbf{1}^{T} \nu$ if $W+\operatorname{diag}(\nu) \succeq 0$。这个下界是不平凡的，比如可以取 $\nu=-\lambda_{\min}(W)\mathbf{1}$，可以给出 $p^\star\ge n\lambda_{\min}(W)$。</p>
<h2 id="2-对偶问题"><a href="#2-对偶问题" class="headerlink" title="2. 对偶问题"></a>2. 对偶问题</h2><p>上面已经多次提到<strong>对偶问题(Lagrange dual problem)</strong>了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { maximize } \quad& g(\lambda,\nu)\\
\text { subject to } \quad& \lambda\succeq0
\end{aligned}</script><p>假如对偶问题的最优解为 $d^\star=\max g(\lambda,\nu)$，那么我们有 $p^\star \ge d^\star$。</p>
<p>现在我们当然想知道什么情况下可以取等号，也即 $p^\star = d^\star$，此时我们只需要求解对偶问题就可以获得原问题的最优解了。在此之前，我们先引入两个概念：强对偶和弱对偶。</p>
<p><strong>弱对偶(weak duality)</strong>：满足 $p^\star \ge d^\star$，原问题不论是否为凸，弱对偶总是成立；</p>
<p><strong>强对偶(strong duality)</strong>：满足 $p^\star = d^\star$，强对偶并不总是成立，如果原问题为凸优化问题，一般情况下都成立。在凸优化问题中，保证强对偶成立的条件为被称为 <strong>constraint qualiﬁcations</strong>。</p>
<p>有很多种不同的 constraint qualiﬁcations，常用到的一种为 <strong>Slater’s constraint qualiﬁcation(SCQ)</strong>，其表述为</p>
<blockquote>
<p><strong>SCQ</strong>：对于凸优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&Ax=b
\end{aligned}</script><p>如果存在可行解 $x\in\text{int}\mathcal{D}$，使得</p>
<script type="math/tex; mode=display">
Ax=b,\quad f_i(x)<0,\quad,i=1,...,m</script><p>那么就能保证强对偶性。</p>
<p><strong>Remarks</strong>：</p>
<ul>
<li>由于存在线性等式约束，因此实际定义域可能不存在内点，可以将这一条件放松为相对内点 $x\in\text{relint}\mathcal{D}$；</li>
<li>如果不等式约束中存在线性不等式，那么他也不必严格小于0。也即如果 $f_i(x)=C^Tx+d$，则只需要满足 $f_i(x)\le0$ 即可。</li>
</ul>
</blockquote>
<p>下面再举几个例子，看一看他们的 SCQ 条件是什么。</p>
<p><strong><em>例子 1</em></strong>：还是考虑线性规划(LP) 或者二次规划(QP)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& c^Tx \quad(\text{ or }x^TPx)\\
\text { subject to } \quad& Ax\preceq b
\end{aligned}</script><p>那么根据 SCQ 可以得到，如果想得到强对偶性，应该有 $\exist x, \text{ s.t. } Ax\preceq b$。</p>
<p><strong><em>例子 2</em></strong>：(原问题非凸) Trust Region Methods</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& x^TAx+2b^Tx\\
\text { subject to } \quad& x^Tx\le1
\end{aligned}</script><p>其中 $A\nsucceq 0$，因此原问题不是凸的。他的对偶函数就是</p>
<script type="math/tex; mode=display">
g(\lambda)=\inf_x\left(x^T\left(A+\lambda I\right)x+2b^Tx-\lambda\right)
=\begin{cases}-b^T(A+\lambda I)^\dagger b-\lambda & A+\lambda I\succeq0,b\in \mathcal{R}(A+\lambda I) \\ -\infty & o.w. \end{cases}</script><p>注意如果不满足 $A+\lambda I\succeq0$ 或 $b\in \mathcal{R}(A+\lambda I)$，则 $g(\lambda)\to-\infty$。那么就可以得到对偶问题为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {maximize} \quad& -b^{T}(A+\lambda I)^{\dagger} b -\lambda\\
\text {subject to} \quad& A+\lambda I \succeq 0\\
&b \in \mathcal{R}(A+\lambda I)
\end{aligned}</script><p>也可以等价转换为 SDP</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {maximize} \quad& -t-\lambda\\
\text {subject to}\quad& \left[\begin{array}{cc}A+\lambda I & b \\ b^{T} & t\end{array}\right] \succeq 0
\end{aligned}</script><blockquote>
<p><strong>Remarks</strong>：这里用到了舒尔补(Schur complement)的知识。考虑矩阵</p>
<script type="math/tex; mode=display">
X = \left[\begin{array}{cc}A & B \\ B^{T} & C\end{array}\right]</script><p>其中 $\det A\ne0,S=C-B^TA^{-1}B$。那么有以下及条性质：</p>
<ul>
<li>$X\succ0 \iff A\succ0,S\succ0$</li>
<li>若 $A\succ0$，则 $X\succeq0 \iff S\succeq 0$</li>
<li>$X\succeq0 \iff A\succeq0,(I-AA^\dagger)B=0,S=C-B^TA^{\dagger}B\succeq0$</li>
</ul>
<p>关于第 3 条中的第二个要求 $(I-AA^\dagger)B=0$，对 $A$ 进行奇异值分解，有 $A=U\Sigma V$，那么我们对任意 $v$，有 $(I-AA^\dagger)Bv=(I-UU^T)Bv=0$，而 $UU^T$ 实际上就是向 $\mathcal{R}(A)$ 的投影矩阵，因此就要求 $Bv\in\mathcal{R}(A)$。</p>
</blockquote>
<h2 id="3-SCQ-几何解释"><a href="#3-SCQ-几何解释" class="headerlink" title="3. SCQ 几何解释"></a>3. SCQ 几何解释</h2><p>前面给出的是 SCQ 的代数描述，那么如何证明呢？另外如何从几何角度直观理解呢？</p>
<p>首先我们可以考虑最简单的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_0(x)\\
\text { subject to } \quad& f_1(x)
\end{aligned}</script><p>定义集合 $\mathcal{G}=\{(f_1(x),f_0(x))|x\in\mathcal{D}\}$，那么对偶函数为</p>
<script type="math/tex; mode=display">
g(\lambda)=\inf_{(u,t)\in\mathcal{G}}(t+\lambda u)</script><p>如果我们画出下面这张图，阴影部分就是可行区域 $\mathcal{G}$，而 $(\lambda,1)^T$ 则正好定义了一个支撑超平面，$g(\lambda)$ 就等于 $t$ 轴的交点。通过取不同的 $\lambda$ 我们就可以得到不同的支撑超平面，也可以得到不同的 $g(\lambda)$，最终会有某一个 $\lambda^\star$ 对应的是 $d^\star=g(\lambda^\star)$。还需要注意这里的支撑超平面永远不可能是竖直的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-geo.PNG" alt="dual geometry"></th>
<th><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-geo2.PNG" alt="dual geometry"></th>
</tr>
</thead>
<tbody>
<tr>
<td>$(\lambda,1)^T$ 正好定义了一个支撑超平面</td>
<td>每个 $\lambda$ 对应一个支撑超平面</td>
</tr>
</tbody>
</table>
</div>
<p>那么 $p^\star$ 体现在哪个点呢？由于对于原优化问题，我们有 $f_1(x)\le0$，因此体现在这个图里面就是 $u\le0$，也就是上面左图当中的红色区域，而 $p^\star=\min f_0(x)=\min t$。</p>
<p>理解了这张图，我们现在开始证明两件事：</p>
<ol>
<li>证明弱对偶性，也即 $p^\star \ge d^\star$；</li>
<li>证明强对偶性条件 SCQ。</li>
</ol>
<p>注：在此之前，我们不妨加入等式约束，也即 $g(\lambda,\mu)=\inf_{(u,v,t)\in\mathcal{G}}(t+\lambda^T u+\mu^T v)$。</p>
<p><strong>弱对偶性的证明</strong>：我们有 $\lambda\ge0$</p>
<script type="math/tex; mode=display">
\begin{aligned}
p^\star &= \inf\{t|(u,v,t)\in\mathcal{G},u\le0,v=0\} \\
&\ge \inf\{t+\lambda^Tu+\mu^Tv|(u,v,t)\in\mathcal{G},u\le0,v=0\} \\
&\ge \inf\{t+\lambda^Tu+\mu^Tv|(u,v,t)\in\mathcal{G}\} \\
&= g(\lambda,\mu)
\end{aligned}</script><p><strong>强对偶性条件 SCQ 的证明</strong>：由 $g(\lambda,\mu)=\inf_{(u,v,t)\in\mathcal{G}}(t+\lambda^T u+\mu^Tv)$ 可以得到</p>
<script type="math/tex; mode=display">
(\lambda,\mu,1)^T(u,v,t)\ge g(\lambda,\mu),\quad \forall (u,v,t)\in\mathcal{G}</script><p>这实际上定义了 $\mathcal{G}$ 的一个超平面。特别的有 $(0,0,p^\star)\in\text{bd}\mathcal{G}$，因此也有</p>
<script type="math/tex; mode=display">
(\lambda,\mu,1)^T(0,0,p^\star)\ge g(\lambda,\mu)</script><p>这个不等式可以自然地导出弱对偶性，当“=”成立时则可以导出强对偶性。那么什么时候取等号呢？点 $(0,0,p^\star)$ 为<strong>支撑点</strong>的时候！也就是说</p>
<blockquote>
<p>如果在边界点 $(0,0,p^\star)$ 处存在一个<strong>非竖直的支撑超平面</strong>，那么我们就可以找到 $\lambda,\mu$ 使得上面的等号成立，也就是得到了强对偶性。</p>
</blockquote>
<p>注意前面的分析中我们并没有提到 SCQ，那么 SCQ 是如何保证强对偶性的呢？注意 SCQ 要求存在 $x\in\mathcal{D}$ 使得 $f(x)&lt;0$，这也就意味着 $\mathcal{G}$ 在 $u&lt; 0$ 半平面上有点，因此如果支撑超平面存在的话，就一定不是垂直的。</p>
<p>但这又引出另一个问题，那就是支撑超平面一定存在吗？答案是一定存在，这是由原问题的凸性质决定的。为了证明这一点，我们可以引入一个类似于 epigraph 的概念：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{A} &= \mathcal{G} + (R^m_+\times \{0\}\times R_+) \\
&= \left\{(u,v,t) |\ \exist x\in\mathcal{D},s.t. f(x)\le u,h(x)=v,f_0(x)\le t\right\}
\end{aligned}</script><p>由于原优化问题为凸的，可以应用定义证明集合 $\mathcal{A}$ 也是凸的，同时 $(0,0,p^\star)\in\text{bd}\mathcal{A}$，那么集合 $\mathcal{A}$ 在 $(0,0,p^\star)$ 点就一定存在一个支撑超平面。又由 SCQ 可知这个支撑超平面一定不是竖直的，因此就可以得到强对偶性了。</p>
<p>注：$(\lambda,\mu,1)^T(u,v,t)\ge g(\lambda,\mu),\quad \forall (u,v,t)\in\mathcal{A}$ 也成立。</p>
<h2 id="4-广义不等式约束与SDP"><a href="#4-广义不等式约束与SDP" class="headerlink" title="4. 广义不等式约束与SDP"></a>4. 广义不等式约束与SDP</h2><p>前面讨论拉格朗日函数的时候都只考虑了标量函数，如果约束函数为<strong>广义不等式</strong>，也即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \preceq_{K_i} 0, \quad i=1, \ldots, m\\
&h_{i}(x)=0, \quad i=1, \ldots, p
\end{aligned}</script><p>那么他的拉格朗日函数就是</p>
<script type="math/tex; mode=display">
L\left(x, \lambda_{1}, \cdots, \lambda_{m}, \nu\right)=f_{0}(x)+\sum_{i=1}^{m} \lambda_{i}^{T} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)</script><p>对偶函数就是</p>
<script type="math/tex; mode=display">
g\left(\lambda_{1}, \ldots, \lambda_{m}, \nu\right)=\inf _{x \in \mathcal{D}} L\left(x, \lambda_{1}, \cdots, \lambda_{m}, \nu\right)</script><p>其同样满足 $p^\star\ge g\left(\lambda_{1}, \ldots, \lambda_{m}, \nu\right)$。对偶问题为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {maximize} \quad& g\left(\lambda_{1}, \ldots, \lambda_{m}, \nu\right) \\
\text {subject to}\quad& \lambda_i\succeq_{K_i^*}0,i=1,...,m
\end{aligned}</script><p>强对偶性以及 Slater’s Condition 是类似的。</p>
<p>对于 <strong>SDP 问题</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {maximize} \quad& c^Tx \\
\text {subject to}\quad& x_1F_1+\cdots +x_nF_n\preceq G
\end{aligned}</script><p>拉格朗日函数就是</p>
<script type="math/tex; mode=display">
L(x, Z)=c^{T} x+\operatorname{tr}\left(Z\left(x_{1} F_{1}+\cdots+x_{n} F_{n}-G\right)\right)</script><p>对偶函数为</p>
<script type="math/tex; mode=display">
g(Z)=\inf _{x} L(x, Z)=\left\{\begin{array}{ll}
-\operatorname{tr}(G Z) & \operatorname{tr}\left(F_{i} Z\right)+c_{i}=0, \quad i=1, \ldots, n \\
-\infty & \text { otherwise }
\end{array}\right.</script><p>对偶问题就是</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {maximize} \quad& -\operatorname{tr}(G Z)\\
\text {subject to} \quad& Z \succeq 0, \quad \operatorname{tr}\left(F_{i} Z\right)+c_{i}=0, \quad i=1, \ldots, n
\end{aligned}</script><p>强对偶性以及 Slater’s Condition 是类似的。</p>
<h2 id="5-对偶问题的强对偶性与可行性"><a href="#5-对偶问题的强对偶性与可行性" class="headerlink" title="5. 对偶问题的强对偶性与可行性"></a>5. 对偶问题的强对偶性与可行性</h2><p>注意我们说<strong>强对偶性</strong>需要<strong>严格满足</strong>不等式约束(也即最优解需要满足 $h(x^\star)&lt;0$ 而不能是 $h(x^\star)\le0$)，但如果存在线性不等式约束，则可以取到等号(也即 $Ax^\star+b\le0$)。这就会出现下面的现象：</p>
<ol>
<li>对于 <strong>LP</strong> 问题，由于约束是线性的，因此强对偶性只要求有可行解，而不要求 <strong>strictly feasible</strong>；</li>
<li>对于其他问题，若存在非线性约束，比如 <strong>SOCP/SDP</strong> 问题，如果想要满足强对偶性，就需要满足 <strong>strictly feasible</strong>，这就会出现两种情况：1）问题本身的可行域不可能满足 <strong>strictly feasible</strong>，那么就达不到强对偶性，于是 $p^\star\ned^\star$；2）问题可行域满足 <strong>strictly feasible</strong>，但是由于最优解达不到(比如 $\min 1/x$)，那么此时原问题和对偶问题仍满足强队偶性，但是原问题最优解达不到，而对偶问题则可以达到。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-counter0.PNG" alt="LP duality"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-counter1.PNG" alt="SDP/SOCP duality"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-counter2.PNG" alt="SOCP duality"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/11-dual-counter3.PNG" alt="SDP duality"></p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>对偶原理</tag>
        <tag>拉格朗日函数</tag>
        <tag>SCQ</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 7：层次分析法</title>
    <url>/2020/03/15/fuzzy/ch7-ahp/</url>
    <content><![CDATA[<p>日常生活中有许多<strong>决策问题</strong>。决策是指在面临多种方案时需要依据一定的标准选择某一种方案。 比如买钢笔，一般要依据质量、颜色、实用性、价格、外形等方面的因素选择某一支钢笔。 又比如假期旅游，是去风光秀丽的苏州，还是去迷人的北戴河，或者是去山水甲天下的桂林，一般会依据景色、费用、食宿条件、旅途等因素选择去哪个地方。</p>
<p>我们可以利用上一节讲的<strong>模糊综合评判</strong>的方法，对每一个备选方案都进行一次打分，最后取分最高的。不过既然这是新的一篇笔记，肯定还有其他方法啦。美国运筹学家托马斯.赛迪(T. Saaty等人)20世纪在七十年代为美国国防部提出了一种能有效处理这类问题的实用方法——<strong>层次分析法</strong>。 </p>
<a id="more"></a>
<h2 id="1-层次分析法-AHP"><a href="#1-层次分析法-AHP" class="headerlink" title="1. 层次分析法 AHP"></a>1. 层次分析法 AHP</h2><p>层次分析法(Analytic Hierarchy  Process, AHP)是一种定性和定量相结合的、系统化的、层次化的分析方法。是系统分析问题的数学工具之一。层次分析法一般包含以下几个主要步骤：</p>
<ol>
<li>建立层次结构模型</li>
<li>构造成对比较矩阵</li>
<li>层次单排序及一致性检验</li>
<li>层次总排序及其一致性检验</li>
</ol>
<p>下面逐步解释各个步骤。</p>
<h3 id="1-1-建立层次结构模型"><a href="#1-1-建立层次结构模型" class="headerlink" title="1.1 建立层次结构模型"></a>1.1 建立层次结构模型</h3><p>一般分为三层，最上面为目标层，最下面为方案层，中间是准则层或指标层。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-layers.PNG" alt="layers"></p>
<p>若上层的每个因素都支配着下一层的所有因素，或被下一层所有因素影响，称为<strong>完全层次结构</strong>，否则称为<strong>不完全层次结构</strong>（都是概念性的东西，我觉得不重要，只要理解层次结构就好啦）。</p>
<h3 id="1-2-构造成对比较矩阵"><a href="#1-2-构造成对比较矩阵" class="headerlink" title="1.2 构造成对比较矩阵"></a>1.2 构造成对比较矩阵</h3><p>设某层有 $n$ 个因素，$X=\{x_1,…,x_n\}$ 。要比较它们对上一层某一准则（或目标）的影响程度，也就是把 $n$ 个因素对上 层某一目标的影响程度排序。这种比较是凉凉元素之间的比较，比较时取1~9 尺度。用 $a_{ij}$ 表示第 $i$ 个因素相对于第 $j$ 个因素的比较结果，则有 $a_{ij}=1/a_{ji}$（这里应该完全是人为定义）</p>
<script type="math/tex; mode=display">
A=\left(a_{i j}\right)_{n \times n}=\left(\begin{array}{llll}a_{11} & a_{12} & \cdots & a_{1 n} \\a_{21} & a_{22} & \cdots & a_{2 n} \\\cdots & \cdots & \cdots & \cdots \\a_{n 1} & a_{n 2} & \cdots & a_{n n}\end{array}\right)</script><p>$A$ 则称为<strong>成对比较矩阵</strong>。</p>
<p>这里的成对比较矩阵有点像图论里的<strong>邻接矩阵</strong>，就是说任意两个元素都要进行一次比较。</p>
<p>前边说了比较尺度我们一般选择 1~9，并且一般选择奇数（不知道为啥），各尺度含义为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>尺度</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>第 $i$ 个因素与第 $j$ 个因素的影响<strong>相同</strong></td>
</tr>
<tr>
<td>3</td>
<td>第 $i$ 个因素比第 $j$ 个因素的影响<strong>稍强</strong></td>
</tr>
<tr>
<td>5</td>
<td>第 $i$ 个因素比第 $j$ 个因素的影响<strong>强</strong></td>
</tr>
<tr>
<td>7</td>
<td>第 $i$ 个因素比第 $j$ 个因素的影响<strong>明显强</strong></td>
</tr>
<tr>
<td>9</td>
<td>第 $i$ 个因素比第 $j$ 个因素的影响<strong>绝对地强</strong></td>
</tr>
</tbody>
</table>
</div>
<p>2,4,6,8 表示第 $i$ 个因素相对于第 $j$ 个因素的影响介于上述两个相邻等级之间。</p>
<p>根据上面的定义，可以知道成对比较矩阵 $A$ 满足以下三条性质：</p>
<ol>
<li>$a_{ij}&gt;0$</li>
<li>$a_{ij}=1/a_{ji}$</li>
<li>$a_{ii}=1$</li>
</ol>
<p>此时 $A$ 也成为<strong>正互反阵</strong>。</p>
<p>比如旅游问题中，第二层 $A$ 的各因素对目标层 $Z$ 的影响两两比较结果如下：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-matA.PNG" alt="matrix"></p>
<blockquote>
<p><strong>Remarks</strong>：（我个人觉得）一般认为各因素是有序的，也就是说如果 $x_1&gt;x_2,x_2&gt;x_3$，那么应该有 $x_1&gt;x_3$，这样一来互反矩阵 $A$ 就需要满足一定的性质了。</p>
</blockquote>
<h3 id="1-3-层次单排序及一致性检验"><a href="#1-3-层次单排序及一致性检验" class="headerlink" title="1.3 层次单排序及一致性检验"></a>1.3 层次单排序及一致性检验</h3><p>层次单排序就是确定下层各因素对上层某因素影响程度的过程。 用权值表示影响程度，先从一个简单的例子看如何确定权值。 例如 一块石头重量记为 1，打碎分成 n 个小块，各块的重量分别记为 $w_1,…,w_n$，那么可以得到成对比较矩阵</p>
<script type="math/tex; mode=display">
A=\left(a_{i j}\right)_{n \times n}=\left(\begin{array}{llll}1 & \frac{w_1}{w_2} & \cdots & \frac{w_1}{w_n} \\\frac{w_2}{w_1} & 1 & \cdots & \frac{w_2}{w_n} \\\cdots & \cdots & \cdots & \cdots \\\frac{w_n}{w_1} & \frac{w_n}{w_2} & \cdots & 1\end{array}\right)</script><p>根据上面的定义可以有 $a_{ik}a_{kj}=a_{ij},\forall i,j$。前面我们提到的互反矩阵并不满足这个性质，如果互反矩阵 $A$ 满足此性质，则我们称其为<strong>一致阵</strong>。一致阵有以下性质：</p>
<ol>
<li>$a_{ij}=1/a_{ji},a_{ii}=1$</li>
<li>$a_{ik}a_{kj}=a_{ij}$</li>
<li>$A^T$ 也是一致阵</li>
<li>$A$ 的各行成比例，故 $rank(A)=1$</li>
<li>$A$ 的最大特征值为 $n$，其余特征值均为 0</li>
<li>$A$ 的任一列都是对应于特征根的特征向量。 </li>
</ol>
<p>若成对比较矩阵是一致阵，则我们自然会取对应于最大特征根 $n$ 的归一化<strong>特征向量</strong> $\{w_1,…,w_n\}$，且 $\sum w_i=1$，$w_i$ 即表示下层第 $i$ 个因素对上层某因素影响程度的权值。若成对比较矩阵不是一致阵，Saaty等人建议用其最大特征根对应的归一化特征向量作为权向量 $\boldsymbol{w}$，这样确定权向量的方法称为<strong>特征根法</strong>。</p>
<p><strong>定理</strong>： $n$ 阶互反阵 $A$ 的最大特征根 $\lambda\ge n$，当且仅当 $\lambda=n$ 时，$A$ 为一致阵。</p>
<p>由于 $\lambda$ 连续的依赖于 $a_{ij}$，则 $\lambda$ 比 $n$ 大的越多，$A$ 的不一致性越严重。用最大特征值对应的特征向量作为被比较 因素对上层某因素影响程度的权向量，其不一致程度越大，引起的判断误差越大。因而可以用 $\lambda-n$ 数值的大小来衡量 $A$ 的不一致程度。</p>
<p><strong>一致性指标</strong>：$CI=\frac{\lambda-n}{n-1}$</p>
<p><strong>随即一致性指标</strong>：构造 500 个成对比较矩阵 $A_1,…,A_{500}$，可得一致性指标 $CI_1,…,CI_{500}$</p>
<script type="math/tex; mode=display">
R I=\frac{C I_{1}+C I_{2}+\cdots C I_{500}}{500}=\frac{\frac{\lambda_{1}+\lambda_{2}+\cdots+\lambda_{500}}{500}-n}{n-1}</script><p>对于 1 阶和 2 阶成对比较矩阵，总是有 $RI=0$。一般有以下表格，使用时直接查表</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-ri1.PNG" alt="RI"></p>
<p>有一些文献取随机取 1000 个成对矩阵，分别计算它们的一致性指标 CI 进而得到如下的随机一致性指标 RI：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-ri2.PNG" alt="RI"></p>
<p>一般，当一致性比率 $CR=CI/RI&lt;0.1$ 时，认为 $A$ 的不一致程度在容许范围之内，可用其最大特征值对应的归一化特征向量作为权向量，否则要重新构造成对比较矩阵，对 $A$ 加以调整。</p>
<p><strong>一致性检验</strong>：上面利用一致性指标及随机一致性指标的数值表，进而计算一致性比率并进行判断的过程称为是对 $A$ 的一致性检验。</p>
<h3 id="1-4-层次总排序及其一致性检验"><a href="#1-4-层次总排序及其一致性检验" class="headerlink" title="1.4 层次总排序及其一致性检验"></a>1.4 层次总排序及其一致性检验</h3><p>确定某层所有因素对于总目标相对重要性的排序权值过程， 称为<strong>层次总排序</strong>。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-layers2.PNG" alt="layers"></p>
<p>那么 $B$ 层第 $i$ 个因素对总目标的权值为 $B_i = \sum_j a_jb_{ij}$。</p>
<p>假设 $B$ 层 $B_1,…,B_n$ 对上层 $A$ 中因素 $A_j$ 的层次单排序一致性指标为 $CI_j$，随机一致性指标为 $RI_j$，则层次总排序的一致性比率为：</p>
<script type="math/tex; mode=display">
CR=\frac{a_1CI_1+\cdots+a_mCI_m}{a_1RI_1+\cdots+a_mRI_m}</script><p>当 $CR&lt;0.1$ 时，认为层次总排序通过一致性检验。到此，根据最下层（决策层）的层次总排序做出最后决策。</p>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>层次分析法</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 10：凸优化问题</title>
    <url>/2020/03/14/optimization/ch10-cvx-optimization/</url>
    <content><![CDATA[<p>前面讲了那么多关于凸集、凸函数的知识，然而都是铺垫，现在我们才来到了这门课的重头戏部分——凸优化问题！</p>
<a id="more"></a>
<h2 id="1-一般优化问题"><a href="#1-一般优化问题" class="headerlink" title="1. 一般优化问题"></a>1. 一般优化问题</h2><p>一般优化问题的形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&h_{i}(x)=0, \quad i=1, \ldots, p
\end{aligned}</script><p>其中 $f_0(x)$ 为目标函数，$f_i(x)$ 为不等式约束函数， $h_i$ 为等式约束函数。优化问题的<strong>最优解</strong>为 </p>
<script type="math/tex; mode=display">
p^{\star}=\inf \left\{f_{0}(x) | f_{i}(x) \leq 0, i=1, \ldots, m, h_{i}(x)=0, i=1, \ldots, p\right\}</script><p>如果 $p^<em>=\infty$，则问题不可行；如果 $p^</em>=-\infty$ 则该问题没有下界。</p>
<p><strong>最优解</strong>则有 $f_0(x)=p^<em>$，<em>*局部最优解(local optimal)</em></em>有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} (\text{over } z) \quad& f_{0}(z) \\
\text{subject to} \quad&f_{i}(z) \leq 0, \quad i=1, \ldots, m, \quad h_{i}(z)=0, \quad i=1, \ldots, p \\
&\|z-x\|_{2} \leq R
\end{aligned}</script><p>也即只在一个小的邻域内考虑优化问题。</p>
<p>注意：</p>
<ul>
<li>有的优化问题有最小值，但是没有可行解，比如 $f_0(x)=1/x$；</li>
<li>有的问题根本就没有最小值，比如 $f_0(x)=-\log x$</li>
<li>有的问题只有局部最小值，比如 $f_0(x)=x^3-3x$</li>
</ul>
<p>上面提到的优化问题中有等式和不等式约束，这些我们都称为<strong>显式约束(explicit constraints)</strong>，同时由于 $x$ 应属于各个函数的定义域内，因此还有<strong>隐式约束(implicit constraint)</strong>，即</p>
<script type="math/tex; mode=display">
x \in \mathcal{D}=\bigcap_{i=0}^{m} \operatorname{dom} f_{i} \cap \bigcap_{i=1}^{p} \operatorname{dom} h_{i}</script><p>没有显式约束的优化问题被称为<strong>无约束优化问题(unconstrained)</strong>。比如</p>
<script type="math/tex; mode=display">
\text { minimize } \quad f_{0}(x)=-\sum_{i=1}^{k} \log \left(b_{i}-a_{i}^{T} x\right)</script><p>是一个无约束优化问题，包含了隐式约束 $a_i^Tx&lt; b_i$。</p>
<p>其实有约束优化问题也可以转化为无约束优化问题，只需要加一个指示函数，一开始提到的一般优化问题就可以利用 $\delta_C$ 转化为下面的无约束优化问题，不过这种转化可能并没有太大的意义</p>
<script type="math/tex; mode=display">
\min_x f_0(x)+\delta_{C}(x) \\
\delta_C(x)=\begin{cases}0&f_i(x)\le0,h_i(x)=0 \\ \infty \end{cases}</script><p>除了优化问题，还有一种<strong>可行解问题(Feasibility problem)</strong>，也就是给定一系列约束来寻找是否有可行解</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {find} \quad& x \\
\text { subject to} \quad& f_{i}(x) \leq 0,\quad i=1, \ldots, m \\ 
& h_{i}(x)=0,\quad i=1, \ldots, p 
\end{aligned}</script><p>这实际上也可以转化为一般优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text {minimize} \quad& 0 \\
\text { subject to} \quad& f_{i}(x) \leq 0,\quad i=1, \ldots, m \\ 
& h_{i}(x)=0,\quad i=1, \ldots, p 
\end{aligned}</script><h2 id="2-凸优化问题"><a href="#2-凸优化问题" class="headerlink" title="2. 凸优化问题"></a>2. 凸优化问题</h2><h3 id="2-1-凸优化问题定义"><a href="#2-1-凸优化问题定义" class="headerlink" title="2.1 凸优化问题定义"></a>2.1 凸优化问题定义</h3><p><strong>凸优化问题(Convex optimization problem)</strong>要求目标函数为凸函数，而且定义域为凸集，这样可以利用凸函数和凸集的优良性质简化问题，因此凸优化问题的一般形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&Ax=b, \qquad\qquad\qquad\qquad\quad\bigstar
\end{aligned}</script><p>其中要求目标函数和约束函数 $f_0,f_1,…,f_m$ 均为<strong>凸函数</strong>。</p>
<blockquote>
<p><strong>Remarks</strong>：需要注意这里还要求<strong>等式约束均为仿射函数</strong>，这是因为我们希望定义域是凸集，假设等式约束 $h_i$ 不是线性的，即使 $h_i$ 是凸函数，$\{x|h_i(x)=0\}$ 也不一定是凸集。比如二次等式约束 $\Vert x\Vert_2=r$，得到的定义域就是一个球面，显然不是一个凸集，这对优化不利。</p>
</blockquote>
<p>有时候我们直接拿到的优化问题并不符合上面的形式，但是可以经过化简得到等价问题，就是凸的了，比如</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)=x_{1}^{2}+x_{2}^{2}\\
\text { subject to } \quad& f_{1}(x)=x_{1} /\left(1+x_{2}^{2}\right) \leq 0\\
&h_{1}(x)=\left(x_{1}+x_{2}\right)^{2}=0
\end{aligned}</script><p>经过简单化简就有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& x_{1}^{2}+x_{2}^{2}\\
\text { subject to } \quad& x_{1} \leq 0\\
&x_{1}+x_{2}=0
\end{aligned}</script><h3 id="2-2-凸优化问题的最优解"><a href="#2-2-凸优化问题的最优解" class="headerlink" title="2.2 凸优化问题的最优解"></a>2.2 凸优化问题的最优解</h3><p>对于凸优化问题有一个极其重要的性质，就是</p>
<blockquote>
<p><strong>凸优化问题的局部最优解就是全局最优解</strong></p>
</blockquote>
<p>证明也很简单，若 $x^<em>$ 为局部最优解，只需要假设另外一个全局最优解 $y\ne x^</em>,f(y)&lt;x$，那么利用凸函数的性质，就可以在 $x^*$ 的邻域内导出矛盾，如下图图示。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-global-opti.PNG" alt="optimal"></p>
<p>凸优化问题的最优解还有一个很好的判据</p>
<blockquote>
<p>$x$ 为最优解，<strong>当且仅当</strong> </p>
<script type="math/tex; mode=display">
\nabla f_{0}(x)^{T}(y-x) \geq 0 \quad \text { for all feasible } y</script></blockquote>
<p>证明过程只需要应用凸函数的一阶等价定义即可，即 $f(y)\ge f(x)+\nabla f^T(x)(y-x)$。</p>
<p>这个怎么直观理解呢？还记得我们之前在拟凸函数那里提到的“支撑超平面”吗？实际上 $f_0(x)$ 定义了一个<strong>等高线</strong>，由于 $f_0$ 是一个凸函数，因此这个等高线实际上围成了一个凸集，这个凸集也就是一个下水平集。而这里的 $\nabla f_0(x)$ 就是这个下水平集的一个支撑超平面，正如下图所示。同时注意，$\nabla f_0(x)$ 也代表着函数指上升的方向，如果说对任意定义域内的 $y$，都有 $\nabla f_{0}(x)^{T}(y-x) \geq 0$ 成立，那么说明我们从 $x$ 走到 $y$ 总会使 $f_0$ 增大，也就是说 $x$ 就是最优解，对应最小值。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-global-opti2.PNG" alt="gradient"></p>
<p>利用上面这两个性质，我们可以对很多类型的凸优化问题的最优解有一个认识。</p>
<p><strong>无约束优化问题</strong>：对无约束优化问题，$x$ 为最优解，当且仅当</p>
<script type="math/tex; mode=display">
x\in\text{dom} f_0,\quad \nabla f_0(x)=0</script><p><strong>等式约束优化问题</strong>：$\min\quad f_0(x),\qquad s.t.\quad Ax=b$，则有 $x$ 为最优解，当且仅当存在 $\nu$</p>
<script type="math/tex; mode=display">
x \in \operatorname{dom} f_{0}, \quad A x=b, \quad \nabla f_{0}(x)+A^{T} \nu=0</script><p>证明：因为 $Ax=b$ 实际上定义了一个超平面，如果 $x$ 为最优解，那么 $\nabla f_0(x)$ 一定没有这个平面内的分量，也就是说 $\nabla f_0(x)\in \ker(A)^\perp$。</p>
<p>这个实际上就等价于 <strong>Lagrange 乘子法</strong>，我们来看拉格朗日函数的定义</p>
<script type="math/tex; mode=display">
L(x,v)=f_0(x)+<Ax-b,v> \\
\Longrightarrow 
\begin{cases}\nabla f_0(x)+A^Tv=0 \\ Ax-b=0\end{cases}</script><p><strong>非负象限内的优化</strong>：$\min\quad f_0(x),\qquad s.t.\quad x\succeq 0$，$x$ 为最优解，当且仅当</p>
<script type="math/tex; mode=display">
x \in \operatorname{dom} f_{0}, \quad x \succeq 0, \quad\left\{\begin{array}{l}
\nabla f_{0}(x)_{i} \geq 0 \quad x_{i}=0 \\
\nabla f_{0}(x)_{i}=0 \quad x_{i}>0
\end{array}\right.</script><p>证明：首先由于定义域为非负象限，意味着 $(y-x)$ 可以取到正无穷，因此可以推导出 $\nabla f_0(x) \succeq 0$，故 $\nabla f_0^T(x)x\ge0$。另一方面，对于凸优化问题的最优解，有 $\nabla f_0^T(x)(y-x)\ge0$，取 $y=0$，因此有 $\nabla f_0^T(x)x\le0$，故 $\nabla f_0^T(x)x=0$。</p>
<p>实际上这里被称为<strong>互补性条件(complementary condition)</strong>，也就是 <strong>KKT 条件</strong>的一部分。</p>
<h3 id="2-3-等价问题化简"><a href="#2-3-等价问题化简" class="headerlink" title="2.3 等价问题化简"></a>2.3 等价问题化简</h3><p>有时原始优化问题比较难，可以通过等价转换进行简化。</p>
<p><strong>消去等式约束</strong>：比如对一般的凸优化问题，等式约束实际上定义了一个超平面，这可以表示为特解 + 一组基的形式</p>
<script type="math/tex; mode=display">
Ax=b \iff x=Fz+x_0 \text { for some } z</script><p>原始凸优化问题就可转化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize (over }z) \quad& f_{0}\left(F z+x_{0}\right)\\
\text { subject to } \quad& f_{i}\left(F z+x_{0}\right) \leq 0, \quad i=1, \dots, m
\end{aligned}</script><p><strong>添加等式约束</strong>：实际上就是上面的一个逆过程，这个过程中取药添加一个等式约束 $y=Ax+b$，由于添加了变量 $y$，会使问题变量数增加，同时优化变量也需要加上 $y$。</p>
<p><strong>引入松弛变量</strong>：比如对于线性不等式约束的优化问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& a_i^Tx\le b_i,\quad i=1,...,m
\end{aligned}</script><p>可以引入松弛因子 $s_i$，得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}(x)\\
\text { subject to } \quad& a_i^Tx + s_i = b_i,\quad i=1,...,m \\
& s_i \ge0,\quad i=1,...,m
\end{aligned}</script><p><strong><em>例子</em></strong>：下面两个优化问题是等价的吗？其中 $W^TW=I$</p>
<script type="math/tex; mode=display">
\min_x f(x)+g(Wx) \\
\min_c f(W^T c)+g(c)</script><p>不一定等价。由于 $W^TW=I$，若 $W$ 为方针，则二者等价，否则说明 $W\in R^{m\times n},m\ge n$，也即 $W$ 为一个瘦高型的矩阵，如果我们取 $f\equiv 0$，那么很显然 $\min_x g(Wx)$ 与 $\min_c g(c)$ 并不等价，因为 $W$ 列不满秩。</p>
<p><strong>epigraph 形式</strong>：任意标准形式的凸优化问题都可以转化为下面的形式</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize (over }x,t) \quad& t\\
\text { subject to } \quad& f_{0}(x)\\
\quad& f_{i}(x) \leq 0, \quad i=1, \ldots, m\\
&Ax=b
\end{aligned}</script><p>这种变化很重要，可以将优化目标转化为约束函数，对于后面一些典型凸优化问题的转化很有用。</p>
<p><strong>对某些变量最小化</strong>：实际上就是对于存在多个优化变量时，提前通过计算消去一些变量</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } \quad& f_{0}\left(x_{1}, x_{2}\right)\\
\text { subject to } \quad& f_{i}\left(x_{1}\right) \leq 0, \quad i=1, \dots, m
\end{aligned}
\iff
\begin{aligned}
\text { minimize } \quad& \tilde{f}_{0}\left(x_{1}\right)\\
\text { subject to } \quad& \tilde{f}_{i}\left(x_{1}\right) \leq 0, \quad i=1, \dots, m
\end{aligned}</script><p>其中 $\tilde{f}_0(x_1)=\inf_{x_2}f_0(x_1,x_2)$。</p>
<h2 id="3-拟凸优化问题"><a href="#3-拟凸优化问题" class="headerlink" title="3. 拟凸优化问题"></a>3. 拟凸优化问题</h2><p>拟凸函数跟凸函数有一些相似的性质，尤其是拟凸函数的任意下水平集都是凸集，因此很多时候对于拟凸问题，也可以用凸优化的一些方法有效解决。</p>
<p><strong>拟凸优化问题(Quasi convex optimization)</strong> 的一般定义为与凸优化基本相同，只不过<strong>目标函数 $f_0(x)$ 可以是拟凸函数</strong>，但约束函数 $f_1,…,f_m$ 仍需要是凸函数。</p>
<blockquote>
<p><strong>Remarks</strong>：我个人觉得这里其实约束函数也可以是拟凸函数？因为即使是拟凸函数，$f_i(x)\le0$ 也可以得到凸的定义域？</p>
</blockquote>
<p>但是此时拟凸优化问题就没有凸优化那么好的性质了，比如局部最优解不一定是全局最优解</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-quasicvx.PNG" alt="quasicvx"></p>
<p>尽管如此，由于拟凸函数任意下水平集 $\{x|f(x)\le t\}$ 都是凸集，我们可以利用这个性质将其转化为凸函数 $\phi_t(x)\le0$ 来表示，由此就可以用凸优化来求解。</p>
<p><strong><em>例子</em></strong></p>
<p>最简单的例子，拟凸函数 $f(x)$ 的下水平集可以表示为 $\{x|f(x)\le t\}$，我们可以用函数 $\phi_t(x)\le0$ 来等价表示</p>
<script type="math/tex; mode=display">
\phi_t(x)=\begin{cases}0 & f(x)\le t \\ \infty \end{cases}</script><p>不过这种表示方法意义不大， 这个函数不连续不可微。我们还有其他的表示方法比如</p>
<script type="math/tex; mode=display">
\phi_t(x)=\text{dist}\left(x,\{z|f(z)\le t\}\right)</script><p>另外，如果拟凸函数 $f_0(x)$ 有一些特定的性质，比如 $f_0(x)=p(x)/q(x)$，其中 $p(x)$ 为凸函数，而 $q(x)&gt;0$ 为凹函数（容易证明此时 $f_0$ 为拟凸函数），那么我们还可以取 $\phi_t(x)$ 为</p>
<script type="math/tex; mode=display">
\phi_t(x)=p(x)-t q(x)</script><p><strong>拟凸优化问题的求解</strong></p>
<p>假如当前拟凸优化问题的最优解为 $p^*$，那么对于寻找可行解问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{find} \quad& x \\
s.t. \quad& \phi_{t}(x) \leq 0, \quad f_{i}(x) \leq 0, \quad i=1, \ldots, m, \\
&A x=b
\end{aligned}</script><p>如果 $t\ge p^<em>$，则该问题有可行解，如果 $t&lt;p^</em>$，则没有可行解。因此对于原始凸优化问题，可以利用二分法迭代求解</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-quasicvx-solve.PNG" alt="quasicvx-solve"></p>
<h2 id="4-典型凸优化问题"><a href="#4-典型凸优化问题" class="headerlink" title="4. 典型凸优化问题"></a>4. 典型凸优化问题</h2><h3 id="4-1-线性规划-LP"><a href="#4-1-线性规划-LP" class="headerlink" title="4.1 线性规划(LP)"></a>4.1 线性规划(LP)</h3><p>线性规划(Linear program)问题的一般形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^{T} x+d \\
\text{subject to} \quad& G x \preceq h \\
&A x=b
\end{aligned}</script><p>联系我们前面提到的凸优化问题最优解性质，有 $c^T(y-x)\ge0$，也即目标函数的等高线是一系列超平面</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-lp4.PNG" alt="LP"></p>
<p><strong><em>例子 1</em></strong>：对于 piecewise-linear minimization 问题(无约束优化)</p>
<script type="math/tex; mode=display">
\text { minimize } \max _{i=1, \ldots, m}\left(a_{i}^{T} x+b_{i}\right)</script><p>可以转化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize} \quad& t\\
\text { subject to} \quad& a_i^Tx + b_i \le t
\end{aligned}</script><p><strong><em>例子 2</em></strong>：多面体的切比雪夫中心(Chebyshev center)</p>
<script type="math/tex; mode=display">
\mathcal{P}=\left\{x | a_{i}^{T} x \leq b_{i}, i=1, \dots, m\right\}</script><p>可以用优化问题表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{maximize} \quad& r\\
\text{subject to} \quad& a_{i}^{T} x_{c}+r\left\|a_{i}\right\|_{2} \leq b_{i}, \quad i=1, \ldots, m
\end{aligned}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-chebyshev_center.PNG" alt="chebyshev_center"></p>
<h3 id="4-2-线性分式规划"><a href="#4-2-线性分式规划" class="headerlink" title="4.2 线性分式规划"></a>4.2 线性分式规划</h3><p>线性分式规划(Linear-fractional program) 的一般形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_0(x) \\
\text{subject to} \quad& G x \preceq h \\
&A x=b
\end{aligned}</script><p>其中 $f_0(x)=\frac{c^Tx+d}{e^Tx+f},\quad \text{dom}f_0(x)=\{x|e^Tx+f&gt;0\}$。这个问题可以等价转化为线性规划。</p>
<h3 id="4-3-二次规划-QP"><a href="#4-3-二次规划-QP" class="headerlink" title="4.3 二次规划(QP)"></a>4.3 二次规划(QP)</h3><p>二次规划(Quadratic program)的一般形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& (1/2)x^TPx+q^Tx+r \\
\text{subject to} \quad& G x \preceq h \\
&A x=b
\end{aligned}</script><p>其中 $P\in S_{+}^n$。</p>
<p>与线性规划不同的是，目标函数的等高线变成了椭球面</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-qp.PNG" alt="QP"></p>
<p><strong><em>例子</em></strong>：最小二乘就是最经典的二次规划的例子，$\min \Vert Ax+b\Vert_2^2$</p>
<h3 id="4-4-二次约束二次规划-QCQP"><a href="#4-4-二次约束二次规划-QCQP" class="headerlink" title="4.4 二次约束二次规划(QCQP)"></a>4.4 二次约束二次规划(QCQP)</h3><p>二次约束二次规划(Quadratically constrained quadratic program)的一般形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& (1/2)x^TP_0x+q_0^Tx+r_0 \\
\text{subject to} \quad& (1/2)x^TP_ix+q_i^Tx+r_i \le 0 \\
&A x=b\end{aligned}</script><p>其中 $P_i\in S_{+}^n$。</p>
<p>一般会限制 $P_1,…,P_m\in S_{++}^n$，也就是不能为 0 矩阵(有什么意义吗？不关键)</p>
<h3 id="4-5-二次锥规划-SOCP"><a href="#4-5-二次锥规划-SOCP" class="headerlink" title="4.5 二次锥规划(SOCP)"></a>4.5 二次锥规划(SOCP)</h3><p>二次锥规划(Second-order cone programming)的一般形式为</p>
<script type="math/tex; mode=display">
\begin{array}{cl}
\text { minimize } & f^{T} x \\
\text { subject to } & \left\|A_{i} x+b_{i}\right\|_{2} \leq c_{i}^{T} x+d_{i}, \quad i=1, \ldots, m \\
& F x=g
\end{array}</script><p>其实 SOCP 比前面几种问题都更广泛，他们都可以看作是 SOCP 的一种情况</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-socp.PNG" alt="SOCP"></p>
<h3 id="4-6-鲁棒线性规划"><a href="#4-6-鲁棒线性规划" class="headerlink" title="4.6 鲁棒线性规划"></a>4.6 鲁棒线性规划</h3><p>对于优化问题，有时候我们的参数比如 $a_i,b_i$ 等都是不确定的，他们可能是在一定范围内属于某个集合，也可能是一个随机变量，这个时候就引入了鲁棒优化的概念。</p>
<p>对于线性规划问题来说，比如</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^{T} x+d \\
\text{subject to} \quad& a_i^Tx\le b_i
\end{aligned}</script><p>一种是考虑<strong>确定性模型</strong>，也即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^{T} x+d \\
\text{subject to} \quad& a_i^Tx\le b_i \text{ for all } a_i\in\mathcal{E}_i
\end{aligned}</script><p>如果 $\mathcal{E}_{i}=\left\{\bar{a}_{i}+P_{i} u ||u|_{2} \leq 1\right\}$ 是一个椭球，则该问题可以转化为一个 SOCP 问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{maximize} \quad& c^Tx\\
\text{subject to} \quad& \bar{a}_{i}^{T} x+\left\|P_{i}^Tx\right\|_{2} \leq b_{i}, \quad i=1, \ldots, m
\end{aligned}</script><p>另一种是<strong>随机性模型</strong>，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^{T} x \\
\text{subject to} \quad& \operatorname{prob}\left(a_{i}^{T} x \leq b_{i}\right) \geq \eta, \quad i=1, \ldots, m
\end{aligned}</script><p>假如 $a_i\sim\mathcal{N}(\bar{a}_i,\Sigma_i)$ 服从高斯分布，则该问题同样可以转化为一个 SOCP 问题</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{maximize} \quad& c^Tx\\
\text{subject to} \quad& \bar{a}_{i}^{T} x+\Phi^{-1}(\eta)\left\|\Sigma_{i}^{1 / 2} x\right\|_{2} \leq b_{i}, \quad i=1, \ldots, m
\end{aligned}</script><h3 id="4-7-几何规划-GP"><a href="#4-7-几何规划-GP" class="headerlink" title="4.7 几何规划(GP)"></a>4.7 几何规划(GP)</h3><p>首先定义<strong>单项式函数(monomial function)</strong> $f(x)=cx_1^{a_1}x_2^{a_2}\cdots x_n^{a_n},\quad \text{dom}f=R_{++}^n$，其中 $c&gt;0,a_i$ 为任意实数；</p>
<p><strong>正项式函数(posynomial function)</strong> $f(x)=\sum_k c_k x_1^{a_{1k}} x_2^{a_{2k}}\cdots x_n^{a_{nk}},\quad \text{dom}f=R_{++}^n$</p>
<p>然后就可以定义<strong>几何规划(geometric program)</strong>了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_0(x) \\
\text{subject to} \quad& f_i(x)\le 1,\quad i=1,...,m \\
&h_i(x)=1,\quad i=1,...,p
\end{aligned}</script><p>其中 $f_i$ 为正项式，$h_i$ 为单项式。</p>
<p>首先说明，这个优化问题并不一定是凸的，因为 $a_i$ 可以取任意实数，比如 $a_i=1/2$ 就不是凸的。那么我们这里为什么要介绍这个问题呢？别急，一会稍微做一个变换我们就可以解决这个问题了。那还有一个问题，这种形式的函数有什么意义呢？为什么专门引入这样一种非凸优化问题呢？我们看这个单项式函数 $cx_1^{a_1}x_2^{a_2}\cdots x_n^{a_n}$，像不像体积或者面积的表达式？这也是他被称为“几何规划”的原因吧。</p>
<p>好，现在我们怎么把这个非凸的问题转化为凸优化问题呢？加个 $\log$ 就行啦！对单项式来说</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log f(x) &= \sum_i a_i \log x_i +\log c = a^Ty+b \\
f(x) &= e^{a^Ty+b}
\end{aligned}</script><p>对多项式来说</p>
<script type="math/tex; mode=display">
\log f(x)=\log \left(\sum_i \exp(a_i^Ty+b_i)\right)</script><p>这么一来，取完对数后的问题就是凸的了，而且我们也知道 $\log$ 是一个单射函数，原始优化问题就变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& \log \left(\sum_k \exp(a_{0k}^Ty+b_{0k})\right) \\
\text{subject to} \quad& \log \left(\sum_k \exp(a_{ik}^Ty+b_{ik})\right)\le 0,\quad i=1,...,m \\
&Gy+d=0
\end{aligned}</script><h3 id="4-8-半正定规划-SDP"><a href="#4-8-半正定规划-SDP" class="headerlink" title="4.8 半正定规划(SDP)"></a>4.8 半正定规划(SDP)</h3><p>前面所讲到的都是标量函数，约束条件也都是函数值与 0 比大小，而前面的章节中我们也提到了广义不等式，对于正常锥则可以定义不等号。所以我们可以定义一种凸优化问题，这种凸优化问题的约束条件不再是普通不等式，而是广义不等式</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& f_0(x) \\
\text{subject to} \quad& f_i(x)\preceq_{K_i} 0,\quad i=1,...,m \\
&Ax=b
\end{aligned}</script><p>其中 $f_0:R^n\to R$ 为凸函数，$f_i:R^n\to R^{k_i}$ 为关于正常锥 $K_i$ 的凸函数。</p>
<p>注意这种带有广义不等式约束的凸优化问题与普通凸优化问题有着相同的性质，比如可行集为凸的，局部最优解就是全局最优解等。</p>
<p>一种简单形式的凸优化问题就是向量形式的，也就是说目标函数与约束都是仿射函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^Tx \\
\text{subject to} \quad& Fx+g\preceq_K 0 \\
&Ax=b
\end{aligned}</script><p>这种向量形式的广义不等式实际上就是对每个元素进行比较，因此实际上可以按照每一行拆分成多个不等式，如果取 $K=R_+^m$ 就与普通的线性规划(LP)没什么区别了。</p>
<p>接下来要介绍的就是重头戏<strong>半正定规划(Semideﬁnite program)</strong>了，我们取 $K=S^n_+$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& c^Tx \\
\text{subject to} \quad& x_1 F_1+x_2F_2+\cdots+x_nF_n+G\preceq_K 0 \\
&Ax=b
\end{aligned}</script><p>其中 $F_i,G\in S^n$。</p>
<p>这里的不等式约束就是大名鼎鼎的<strong>线性矩阵不等式(linear matrix inequality, LMI)</strong>。</p>
<p>如果说我们现在有两个不等式约束怎么办呢？</p>
<script type="math/tex; mode=display">
x_1 \hat{F}_1+x_2\hat{F}_2+\cdots+x_n\hat{F}_n+G\preceq_K 0 \\
x_1 \tilde{F}_1+x_2\tilde{F}_2+\cdots+x_n\tilde{F}_n+G\preceq_K 0</script><p>合成一个更大的矩阵就可以了，实际上这种操作在后面也会经常见到</p>
<script type="math/tex; mode=display">
x_{1}\left[\begin{array}{cc}
\hat{F}_{1} & 0 \\
0 & \tilde{F}_{1}
\end{array}\right]+x_{2}\left[\begin{array}{cc}
\hat{F}_{2} & 0 \\
0 & \tilde{F}_{2}
\end{array}\right]+\cdots+x_{n}\left[\begin{array}{cc}
\hat{F}_{n} & 0 \\
0 & \tilde{F}_{n}
\end{array}\right]+\left[\begin{array}{cc}
\hat{G} & 0 \\
0 & \tilde{G}
\end{array}\right] \preceq 0</script><p>这是因为分块对角矩阵为正定矩阵等价于每一个子块都为正定矩阵。</p>
<p><strong><em>例子 1</em></strong>：半正定规划之所以重要，是因为他的形式更广泛，前面说 SOCP 包含了 LP、QP、QCQP，而半正定规划则包含了 SOCP！比如下面的 SOCP 问题就可以转化为 SDP</p>
<script type="math/tex; mode=display">
\begin{aligned}
SOCP:\qquad \text{minimize}&\quad f^{T} x \\
\text{subject to}&\quad \left\|A_{i} x+b_{i}\right\|_{2} \leq c_{i}^{T} x+d_{i}, \quad i=1, \ldots, m \\
\\
SDP:\qquad \text{minimize}&\quad f^{T} x \\
\text{subject to}&\quad \left[\begin{array}{cc}
\left(c_{i}^{T} x+d_{i}\right) I & A_{i} x+b_{i} \\
\left(A_{i} x+b_{i}\right)^{T} & c_{i}^{T} x+d_{i}
\end{array}\right] \succeq 0, \quad i=1, \ldots, m
\end{aligned}</script><p><strong><em>例子 2</em></strong>：最小化矩阵的最大特征值 $\min \lambda_{\max}(A(x))$，也可以通过半正定规划来描述</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} \quad& t \\
\text{subject to} \quad& A(x)\preceq tI
\end{aligned}</script><p>其中优化变量为 $x\in R^n,t\in R$。这种等价转化是因为 $\lambda_{\max}(A)\le t\iff A\preceq tI$。</p>
<p><strong><em>例子 3</em></strong>：最小化矩阵范数 $\min \Vert A(x)\Vert_2=\left(\lambda\left(A\left(x\right)^TA\left(x\right)\right)\right)^{1/2}$，也可以等价为SDP</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize}&\quad t \\
\text{subject to}&\quad \left[\begin{array}{cc}
t I & A(x) \\
A(x)^T & tI
\end{array}\right] \succeq 0
\end{aligned}</script><h3 id="4-9-向量优化"><a href="#4-9-向量优化" class="headerlink" title="4.9 向量优化"></a>4.9 向量优化</h3><p>前面介绍的所有优化问题的目标函数都是标量（尽管约束可能会出现广义不等式），如果目标函数为向量怎么办呢？前面的章节中我们介绍了广义的凸函数，同样也是基于锥定义的（实际上高维空间中“比大小”我们一般都需要通过锥来定义）。</p>
<p>一般的向量优化问题可以表示为</p>
<script type="math/tex; mode=display">
\begin{alignat}{}
&\text{minimize(w.r.t. K)} \quad& f_0(x) \\
&\text{subject to} \quad& f_i(x)\le 0,\quad i=1,...,m \\
& &h_i(x)=0,\quad i=1,...,p
\end{alignat}</script><p>凸的向量优化问题只需要将上面的等式约束换为仿射函数 $Ax=b$，同时要求 $f_0$ 为 $K-$convex 的，$f_1,…,f_m$ 为凸的。</p>
<p>向量约束优化问题的最优解相当于在下面的集合中寻找最优解</p>
<script type="math/tex; mode=display">
\mathcal{O}=\{f_0(x)|x \text{ feasible}\}</script><p>前面在将广义不等式和凸集的时候，我们讲过<strong>最小元</strong>和<strong>极小元</strong>的概念，这两个概念是不是已经忘得差不多啦！反正我基本全忘了……让我们来复习一下。</p>
<blockquote>
<p><strong>复习</strong>：最小元与极小元</p>
<p>下面两幅图分别表示最小元和极小元</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/3-minimum.png" alt="minimum and minimal"></p>
<p>利用对偶锥，我们可以获得<strong>最小元</strong>的等价定义，即</p>
<blockquote>
<p>$x$ 是集合 $S$ 关于 $\preceq_{K}$ 的最小元 $\iff$ 对任意的 $\lambda \succ_{K*} 0$，$x$ 为 $\lambda^Tz$ 在集合 $S$ 上的唯一最小解</p>
</blockquote>
<p>什么意思呢？也就是说任意的 $\lambda\in K^{\star}$，实际上都代表了一个法向量，也就是一个支撑超平面。如果 $x$ 是最小元，则意味着对任意一个 ($K^{\star}$所定义的) 支撑超平面来说，$x$ 都是支撑点，就像下面这条幅图一样</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-minimal.png" alt="minimal"></p>
<p>而<strong>极小元</strong>的定义是什么呢？</p>
<blockquote>
<ul>
<li>充分条件：若对于某些 $\lambda \succ_{K*} 0$，$x$ minimizes $\lambda^Tz$ over $S$，$\Longrightarrow$ $x$ 为极小元</li>
<li>必要条件：$x$ 为<strong>凸集</strong> $S$ 的极小元，$\Longrightarrow$ 存在非 0 的 $\lambda \succ_{K*} 0$ 使得 $x$ minimizes $\lambda^Tz$ over $S$</li>
</ul>
</blockquote>
<p>我们来看充分条件，只需要存在某一个 $\lambda\in K^{\star}$，使得 $x$ 为对应支撑超平面的支撑点就可以了。比如下面这幅图，蓝色的点，我们可以找到这样一个蓝色的支撑超平面，使其为支撑点，所以它就是一个极小元；而对于红色的点来说，无论如何不可能找到一个支撑超平面，使其为支撑点，因此他就有可能不是极小元，因为这只是充分条件（对这个例子来说他就不是极小元）。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-minimum.PNG" alt="minimum"></p>
<p>简单总结一下：</p>
<ul>
<li>最小元：无论沿着锥 $K^{*}$ 里边哪一个方向走，$x$ 都是最小值点，那么他就是最小元；</li>
<li>极小元：如果沿着其中某一个方向走，$x$ 是最小值点，那么他就是极小元。</li>
</ul>
</blockquote>
<p>复习完了最小元和极小元，不要忘了正事。我们要考虑向量约束优化问题中的最优解</p>
<script type="math/tex; mode=display">
\mathcal{O}=\{f_0(x)|x \text{ feasible}\}</script><p>这是一个集合，如果 $f_0(x)$ 是关于锥 $K$ 的最小元，那么对应的 $x$ 就被称为最优解(optimal)；如果 $f_0(x)$ 是关于锥 $K$ 的极小元，那么对应的 $x$ 则被称为 Pareto optimal。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/10-vec-optim.PNG" alt="optimal vs Pareto optimal"></p>
<p><strong><em>例子</em></strong>：假如我们取 $K=R_+^q$，其中</p>
<script type="math/tex; mode=display">
f_0(x)=(F_1(x),...,F_q(x))</script><p>相当于有 $q$ 个不同的目标 $F_i$，最好的情况当然是希望 $F_i$ 都是最小的。</p>
<p>若 $x^{\star}$ 为最小元（存在）就说明任意其他可行解 $y$ 都有 $f_0(x^{\star})\le f(y)$，正是我们希望的；</p>
<p>而如果只能得到极小元 $x^{po}$，就有对任意可行解 $y$，$f_0(y)\preceq f_0(x^{po})\Longrightarrow f_0(x^{po})=f(y)$。这是什么意思呢？</p>
<ol>
<li>首先不可能存在另一个 $y$ 使得每一个元素都有 $F_i(y)\le F_i(x^{po})$，要不然 $x^{po}$ 出现在这里的意义是什么？我们直接选择 $y$ 作为极小元不就好了吗？如果存在那也顶多是 $F_i(y)=F_i(x^{po}),\forall i$，这种情况下 $y$ 跟 $x^{po}$ 其实没什么区别了。</li>
<li>第二点，有可能存在某些 $y$，满足对一些 $i$ 有 $F_i(y)&gt;F_i(x^{po})$，而对另一些 $i$ 则有 $F_i(y)&lt;F_i(x^{po})$，这意味着 $y$ 在某些方面表现得比 $x^{po}$ 差，但在另一些方面则表现得更好，这实际上体现了我们在不同因素之间的一种权衡。</li>
</ol>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>SDP</tag>
        <tag>LP</tag>
        <tag>SOCP</tag>
        <tag>凸优化问题</tag>
        <tag>QP</tag>
        <tag>GP</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 6：模糊综合评判</title>
    <url>/2020/03/13/fuzzy/ch6-evaluate/</url>
    <content><![CDATA[<p>假如我们现在设计了一种服装，想要调研一下这种服装的受欢迎程度，该怎么办呢？</p>
<p>首先是怎么表示受欢迎程度呢？我们可以简单分为三个等级：受欢迎、一般欢迎、不受欢迎，由于不同的人感受可能不同，结合前面模糊集的概念，我们可以引入隶属度，比如调查后得到 50% 的人喜欢，30%一般，20%不喜欢，我们就可以取隶属度分别为 0.5，0.3，0.2。</p>
<p>当然这样太粗糙了，我们知道评价一个衣服的因素有很多，比如样式、耐穿性、价格等，而不同因素即使对同一个人的值观感受也是不一样的，比如有的人很喜欢这个样式，但是摸了摸钱包，发现这个衣服是这么的难看！所以要想更好的评价，我们需要对每一个指标分别评估一下受欢迎程度，然后把各种因素综合起来。这就是模糊综合评判要做的事情。</p>
<a id="more"></a>
<h2 id="1-一级模糊综合评判"><a href="#1-一级模糊综合评判" class="headerlink" title="1. 一级模糊综合评判"></a>1. 一级模糊综合评判</h2><p>现在我们把上面描述的过程规范化。我们有<strong>因素集</strong> $U=\{u_1,…,u_n\}$，<strong>评语集</strong> $V=\{v_1,…,v_m\}$。</p>
<p>综合评判的步骤是什么呢？</p>
<ol>
<li>首先要对每个因素分别进行单因素评判，也就是获得 $r_i = (r_{i1},\cdots,r_{im})$</li>
<li>然后把 $n$ 个因素的评判指标拼接成一个 $n\times m$ 的矩阵，可以获得综合评判矩阵 $R=(r_1^T,\cdots,r_n^T)^T$</li>
<li>之后需要对所有因素进行综合评判，因此可以设置一个权重 $A=(a_1,…,a_n)$，再根据算子 $M(\cdot,\cdot)$ 计算 $B=A\circ R=M(A,R)$，这个过程实际上就类似于一个加权的过程</li>
<li>上一步结束后实际上得到的还是一个向量，要想最后给一个总的评价指标，可以对该向量再进行一次加权或者其他操作，获得一个总的指标。</li>
</ol>
<p>下面我对第 3，4 条再详细解释一下。</p>
<p>算子 $M(\cdot,\cdot)$ 可以取很多种形式，比如</p>
<ul>
<li>$M(\wedge,\vee) \Longrightarrow \boldsymbol{b}_{j}=\max \left\{\left(\boldsymbol{a}_{i} \wedge \boldsymbol{r}_{i j}\right), \mathbf{1} \leq \boldsymbol{i} \leq \boldsymbol{n}\right\}(\boldsymbol{j}=\mathbf{1}, \boldsymbol{2}, \cdots, \boldsymbol{m})$</li>
<li>$M(\cdot,\vee) \Longrightarrow \boldsymbol{b}_{j}=\max \left\{\left(\boldsymbol{a}_{i} \cdot \boldsymbol{r}_{i j}\right), \mathbf{1} \leq \boldsymbol{i} \leq \boldsymbol{n}\right\}(\boldsymbol{j}=\mathbf{1}, \boldsymbol{2}, \cdots, \boldsymbol{m})$</li>
<li>$M(\cdot,+) \Longrightarrow \boldsymbol{b}_{j}=\sum_i \left(\boldsymbol{a}_{i} \cdot\boldsymbol{r}_{i j}\right), (\boldsymbol{j}=\mathbf{1}, \boldsymbol{2}, \cdots, \boldsymbol{m})$</li>
<li>$M(\wedge,\bigoplus)=\boldsymbol{b}_{j}=\min \left\{1, \sum_{i=1}^{n}\left(a_{i} \wedge r_{i j}\right)\right\}(j=1,2, \cdots, m)$</li>
<li>$M(\wedge,+) \Longrightarrow \boldsymbol{b}_{j}=\sum_i \left(\boldsymbol{a}_{i} \wedge\frac{\boldsymbol{r}_{i j}}{r_0}\right), (\boldsymbol{j}=\mathbf{1}, \boldsymbol{2}, \cdots, \boldsymbol{m}),r_0=\sum_k r_{kj}$</li>
</ul>
<p>不同的算子形式有不同特点，也适用于不同情况。</p>
<p>第 3 步经过 $M(,)$ 算子之后，我们得到了模糊评判向量 $B$，要获得综合结论也可以有不同的方式</p>
<ul>
<li>$u=\max{(b_1,…,b_n)}$</li>
<li>$u=\sum_i \mu_i b_i / \sum_i b_i$</li>
</ul>
<h2 id="2-多级模糊综合评判"><a href="#2-多级模糊综合评判" class="headerlink" title="2. 多级模糊综合评判"></a>2. 多级模糊综合评判</h2><p>有时候对一件事物的评价有很多指标，这些指标又可以分为几大类，比如对高校的评分，可以包括</p>
<ul>
<li>教学：教学下面又可以分为师资力量、教学设施、学生质量等等</li>
<li>科研：…… 可以有很多指标</li>
<li>图书馆、后勤 ……</li>
</ul>
<p>这个时候我们可以划分为多个层次依此评判综合，其步骤可以描述为以下形式：</p>
<ol>
<li>将原始因素集 $U=\{u_1,…,u_n\}$ 划分为若干组 $U=\bigcup_i U_i,U_i\cap U_j=\varnothing$</li>
<li>对每组 $U_i$ 分别进行模糊评判，得到 $B_i$</li>
<li>对总的评判矩阵 $R=(B_1^T,…,B_k^T)^T$ 再次进行综合评判，得到 $B=A\circ R$，然后可以得到总的综合评语</li>
</ol>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>模糊综合评判</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 9：广义凸函数</title>
    <url>/2020/03/11/optimization/ch9-gene-cvx/</url>
    <content><![CDATA[<p>有时候函数 $f$ 为向量，此时怎么定义凸函数呢？根据广义不等式引入。</p>
<a id="more"></a>
<h2 id="1-广义单调函数"><a href="#1-广义单调函数" class="headerlink" title="1. 广义单调函数"></a>1. 广义单调函数</h2><p>对于 $f:R^n\to R$，定义其单调性为存在正常锥 $K\subseteq R^n$，有</p>
<ul>
<li><strong>K-nondecreasing</strong> $\iff \forall x\preceq_K y,\quad f(x)\le f(y)$</li>
<li><strong>K-nonincreasing</strong> $\iff $</li>
</ul>
<p>实际上相当于在定义域中规定了一个序关系，但是注意这种序关系并不是完全的，也就是说有可能有 $x\preceq_K y, y\preceq_K x$ 都不成立，意味着有可能定义域中的两个元素之间是不可“比大小的”。</p>
<p>对于广义单调函数的判定有以下性质</p>
<blockquote>
<p>$f$ K-nondecreasing $\iff \nabla f(x)\succeq_{K^*}0,\forall x\in \text{dom}f$</p>
</blockquote>
<p>证明只需要将其转化为一维情形即可，可以用反证法。</p>
<h2 id="2-广义凸函数"><a href="#2-广义凸函数" class="headerlink" title="2. 广义凸函数"></a>2. 广义凸函数</h2><p>对于 $f:R^n\to R^m$，凸函数定义为关于正常锥 $K\subseteq R^m$</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y) \preceq_K \theta f(x)+(1-\theta)f(y)</script><p><strong><em>例子</em></strong></p>
<ul>
<li>$f:S^n_+\to S^n_+,f(X)=X^2$</li>
</ul>
<p>广义凸函数有下面的性质</p>
<blockquote>
<p>$f$ 为 K-convex $\iff \omega^T f(x)$ convex，对任意 $\omega\succeq_{K^*}0$</p>
<p><strong>Remarks</strong>：只需要考虑 $g(x)=\omega^T f(x)$ 就转化为了普通凸函数，实际上相当于 $\omega\in K^*$ 确定了沿某个方向上的序关系。</p>
</blockquote>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>广义凸函数</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 8：对数凹函数 Log-concave Function</title>
    <url>/2020/03/11/optimization/ch8-log-cvx/</url>
    <content><![CDATA[<p>对数凹函数，顾名思义即取完对数以后 $\log f(x)$ 是凹函数，其应用比如在求最大后验 MAP 时，往往会对联合概率密度函数取对数。</p>
<a id="more"></a>
<h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h2><p>函数 $f$ 被称为<strong>对数凹函数(log-concave)</strong>，如果 $\log f$ 是凹的，也即</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y)\ge f(x)^\theta f(y)^{1-\theta}</script><p><strong><em>对数凹函数的例子</em></strong>：</p>
<ul>
<li>幂函数 $x^\alpha$</li>
<li>正态分布</li>
<li>高斯分布的累积分布 $\Phi(x)=1/\sqrt{2\pi}\int_{-\infty}^{x}e^{-u^2/2}du$</li>
<li><strong>指示函数</strong> $I(x)=1_{x\in C}$，其中 $C$ 为凸集</li>
<li>行列式 $\det X$ 是 log-concave，关于 $S^n_{++}$</li>
<li>$\det X/\text{tr}X$ on $S^n_{++}$</li>
</ul>
<p>根据前面提到的复合函数凹凸性，由于 $\log$ 本身是凹函数，且为单调递增，因此可以有 $f$ concave $\Longrightarrow \log f$ concave。</p>
<p>另外，指数函数取对数以后，变成了线性函数，而高斯分布取对数以后变成了二次函数，是凸的。外面这一层取对数的操作可以直观的想想为把原始函数向下掰弯(划掉)了，比如高斯分布就把接近于 0 的值经过 $\log$ 操作掰到了 $-\infty$，从而变成了二次函数。</p>
<h2 id="2-性质"><a href="#2-性质" class="headerlink" title="2. 性质"></a>2. 性质</h2><blockquote>
<p>函数 $f$ 的定义域为凸集，$f$ 为 log-concave <strong>当且仅当</strong></p>
<script type="math/tex; mode=display">
f(x)\nabla^2 f(x)\preceq \nabla f(x) \nabla f(x)^T,\forall x\in \text{dom}f</script></blockquote>
<p>另外</p>
<ul>
<li>两个 log-concave 函数的<strong>乘积</strong>仍然是 log-concave</li>
<li>若 $f(x,y)$ 是 log-concave，则 $g(x)=\int f(x,y)dy$ 也是 log-concave</li>
<li>若 $f(x)$ 为 log-concave，则 $f(x-y)$ 关于 $(x,y)$ 都是 log-concave 的</li>
</ul>
<p>但是注意，两个 log-concave 函数的<strong>和不一定</strong>是 log-concave，反例比如 $f_1=\lambda_1 \exp(-\lambda_1 x),f_2=\lambda_2 \exp(-\lambda_2 x)$。</p>
<p>类似的，对于<strong>对数凸函数(log-convex)</strong>也有一些性质：</p>
<ul>
<li>$\log f$ convex $\Longrightarrow f$ convex（since $f=\exp(\log f)$）</li>
<li>对数凸函数的和也是对数凸函数，即 $f_1,f_2$ log-convex $\Longrightarrow \log(f_1+f_2)$ convex（注意对数凹函数并没有这个性质）</li>
<li>给定 $y$，$f(x,y)$ 关于 $x$ 是 log-convex，则 $g(x)=\int f(x,y)dy$ 是 log-convex</li>
</ul>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>对数凹函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo NexT 7.7 主题美化</title>
    <url>/2020/03/11/tools/next-beautify/</url>
    <content><![CDATA[<p>网上很多关于主题美化的教程都是老版本 next 5.1 的，最近更新到 next 7 之后摸索了好久才找到简单有效的自定义主题方式，下面是具体的操作。</p>
<a id="more"></a>
<p>修改主题下 <code>_config.yml</code> 文件，找到下面这一部分，也即注释掉最后一行</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">  #head: <span class="keyword">source</span><span class="regexp">/_data/</span>head.swig</span><br><span class="line">  #header: <span class="keyword">source</span><span class="regexp">/_data/</span>header.swig</span><br><span class="line">  #sidebar: <span class="keyword">source</span><span class="regexp">/_data/</span>sidebar.swig</span><br><span class="line">  #postMeta: <span class="keyword">source</span><span class="regexp">/_data/</span>post-meta.swig</span><br><span class="line">  #postBodyEnd: <span class="keyword">source</span><span class="regexp">/_data/</span>post-body-end.swig</span><br><span class="line">  #footer: <span class="keyword">source</span><span class="regexp">/_data/</span>footer.swig</span><br><span class="line">  #bodyEnd: <span class="keyword">source</span><span class="regexp">/_data/</span>body-end.swig</span><br><span class="line">  #variable: <span class="keyword">source</span><span class="regexp">/_data/</span>variables.styl</span><br><span class="line">  #mixin: <span class="keyword">source</span><span class="regexp">/_data/mi</span>xins.styl</span><br><span class="line">  style: <span class="keyword">source</span><span class="regexp">/_data/</span>styles.styl</span><br></pre></td></tr></table></figure>
<p>然后在<strong>博客根目录</strong>下创建文件 <code>blog/source/_data/styles.styl</code>，注意是博客根目录而不是主题下的目录，然后我们就可以在这个文件里边添加自定义配置。</p>
<blockquote>
<p>注意想自定义博客外观的话尽量都在这个文件里修改，不要修改其他原始文件，毕竟这个修改坏了删掉就是了，后面所有的修改都是在这一个文件里边添加内容，是不是很简答呢？</p>
</blockquote>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 修改背景图片</span></span><br><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: url(/images/background/blue.jpg);</span><br><span class="line">    <span class="attribute">background-size</span>: cover;</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    <span class="attribute">background-attachment</span>: fixed;</span><br><span class="line">    <span class="attribute">background-position</span>: <span class="number">50%</span> <span class="number">50%</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改主体透明度</span></span><br><span class="line"><span class="comment">// 这个设置并不好使，会使整个页面蒙上一层不透明白色图层，不建议使用</span></span><br><span class="line"><span class="comment">// 建议使用下面的 .post-block 与 .comments</span></span><br><span class="line"><span class="comment">//.main-inner &#123;</span></span><br><span class="line"><span class="comment">//    background: #fff;</span></span><br><span class="line"><span class="comment">//    opacity: 0.8;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改文章块透明度</span></span><br><span class="line"><span class="selector-class">.post-block</span> &#123;</span><br><span class="line">	<span class="attribute">padding</span>: <span class="variable">$content-desktop-padding</span>;</span><br><span class="line">	<span class="attribute">background</span>: rgba(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.9</span>);   <span class="comment">//white;</span></span><br><span class="line">	<span class="attribute">box-shadow</span>: <span class="variable">$box-shadow-inner</span>;</span><br><span class="line">	<span class="attribute">border-radius</span>: <span class="variable">$border-radius-inner</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Comments blocks.</span></span><br><span class="line"><span class="selector-class">.comments</span> &#123;</span><br><span class="line">	<span class="attribute">padding</span>: <span class="variable">$content-desktop-padding</span>;</span><br><span class="line">	<span class="attribute">margin</span>: initial;</span><br><span class="line">	<span class="attribute">margin-top</span>: sboffset;</span><br><span class="line">	<span class="attribute">background</span>: rgba(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0.9</span>);   <span class="comment">//white;</span></span><br><span class="line">	<span class="attribute">box-shadow</span>: <span class="variable">$box-shadow</span>;</span><br><span class="line">	<span class="attribute">border-radius</span>: <span class="variable">$border-radius</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改菜单栏透明度</span></span><br><span class="line"><span class="selector-class">.header-inner</span> &#123;</span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">0.8</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置主页面宽度</span></span><br><span class="line"><span class="selector-class">.header</span>&#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">90%</span>;</span><br><span class="line">    +tablet() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    +mobile() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.container</span> <span class="selector-class">.main-inner</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">90%</span>;</span><br><span class="line">    +tablet() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    +mobile() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: calc(<span class="number">100%</span> - <span class="number">260px</span>);</span><br><span class="line">    +tablet() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    +mobile() &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 5：模糊聚类</title>
    <url>/2020/03/07/fuzzy/ch5-clustering/</url>
    <content><![CDATA[<p>现在想要对 $n$ 个目标 $U=\{x_1,…,x_n\}$ 分类，并且每个对象都有多个指标，也即 $x_i=\{x_{i1},…,x_{im}\}$。</p>
<p>模糊聚类通常包括三个步骤：</p>
<ol>
<li>建立模糊矩阵</li>
<li>建立模糊等价矩阵</li>
<li>进行聚类</li>
</ol>
<a id="more"></a>
<h2 id="1-数据标准化"><a href="#1-数据标准化" class="headerlink" title="1. 数据标准化"></a>1. 数据标准化</h2><p>实际中各个指标的量纲与数量级很可能相差很大，比如高校评价指标中，可能包括科研经费、论文数量、获奖数量等，数量级差别很大，这个时候就需要先对各项数据进行标准化。常用方法有</p>
<p><strong>标准差标准化</strong></p>
<script type="math/tex; mode=display">
x_{ij}'=\frac{x_{ij}-\bar{x}_j}{\sigma_j}</script><p><strong>极差正规化</strong></p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{i j}^{\prime \prime}=\frac{\boldsymbol{x}_{i j}^{\prime}-\min _{1 \leq i \leq n}\left\{\boldsymbol{x}_{i j}^{\prime}\right\}}{\max _{1}\left\{\boldsymbol{x}_{i j}^{\prime}\right\}-\min _{1: i \leqslant n}\left\{\boldsymbol{x}_{i j}^{\prime}\right\}}</script><p><strong>极差标准化</strong></p>
<script type="math/tex; mode=display">
x_{i j}^{\prime}=\frac{x_{i j}-\bar{x}_{i}}{\max \left\{x_{i j}\right\}-\min \left\{x_{i j}\right\}}</script><p><strong>最大值规格化</strong></p>
<script type="math/tex; mode=display">
x_{ij}'=\frac{x_{ij}}{\max{(x_{1j},x_{2j},...,x_{nj})}}</script><h2 id="2-建立模糊相似矩阵"><a href="#2-建立模糊相似矩阵" class="headerlink" title="2. 建立模糊相似矩阵"></a>2. 建立模糊相似矩阵</h2><p>接下来需要确定不同对象之间的相似度，相似度的确定也有几种常用度量：</p>
<h3 id="1-1-相关系数类"><a href="#1-1-相关系数类" class="headerlink" title="1.1 相关系数类"></a>1.1 相关系数类</h3><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-relation.PNG" alt="correlation"></p>
<h3 id="1-2-距离类"><a href="#1-2-距离类" class="headerlink" title="1.2 距离类"></a>1.2 距离类</h3><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-distance.PNG" alt="distance"></p>
<h3 id="1-3-贴近度类"><a href="#1-3-贴近度类" class="headerlink" title="1.3 贴近度类"></a>1.3 贴近度类</h3><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-nearness.PNG" alt="nearness"></p>
<h2 id="3-聚类"><a href="#3-聚类" class="headerlink" title="3. 聚类"></a>3. 聚类</h2><p>获得模糊相似矩阵以后，需要首先求出传递闭包 $t(R)$，以获得模糊等价矩阵，然后根据不同的阈值 $\lambda$ 进行聚类，然后再画出动态聚类图。</p>
<p><strong><em>举个栗子</em></strong></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg1.PNG" alt="exercise"></p>
<p>求解过程如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>0. 特性指标矩阵</th>
<th>1. 数据标准化</th>
<th>2. 求模糊相似矩阵</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg2.PNG" alt=""></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg3.PNG" alt=""></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg4.PNG" alt=""></td>
</tr>
<tr>
<td><strong>3. 求传递闭包</strong></td>
<td><strong>4. 根据不同阈值聚类</strong></td>
<td><strong>5. 画动态聚类图</strong></td>
</tr>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg5.PNG" alt=""></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg6.PNG" alt=""></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg7.PNG" alt=""></td>
</tr>
</tbody>
</table>
</div>
<h2 id="4-其他问题"><a href="#4-其他问题" class="headerlink" title="4. 其他问题"></a>4. 其他问题</h2><p>在实际应用过程中，还会出现一些其他问题，比如：</p>
<ul>
<li><em>在实际应用中，如何选择适当的 $\lambda$？从而给出一个较明确的聚类。</em></li>
</ul>
<p>实际中最佳阈值的确定，可以由专家给出值；也可以应用 F-统计量确定最佳阈值。</p>
<ul>
<li><em>如果不用传递闭包，直接对相似矩阵进行聚类会怎么样？</em></li>
</ul>
<p>直接应用模糊相似矩阵进行聚类时，可以首先确定一个阈值 $\lambda$，然后根据截集获得一系列聚类结果，由于模糊相似矩阵并不是等价矩阵，因此此时的聚类结果是不严谨的，后面需要对交集非空的集合进行合并（这里可能表述的不清楚，看下面的例子就明白了）</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg8.PNG" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg9.PNG" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-eg10.PNG" alt=""></p>
<ul>
<li><em>当样本点很多时（几百万甚至上千万个像素），例如需 要对一张图片上的样本点进行聚类，该怎么办？</em></li>
</ul>
<p>可以应用<strong>模糊C均值法(FCM)</strong></p>
<h2 id="5-模糊C均值法-FCM"><a href="#5-模糊C均值法-FCM" class="headerlink" title="5. 模糊C均值法(FCM)"></a>5. 模糊C均值法(FCM)</h2><p>设 $x_i(i=1,2,…,n)$ 是n个样本组成的样本集合，$c$ 为预定的类别数目， $u_{ij}$ 是第 $i$ 个样本对于第 $j$ 类的隶属度函数。用隶属度函数定义的聚类损失函数可以写为</p>
<script type="math/tex; mode=display">
\sum_{j=1}^{c} \sum_{i=1}^{n} u_{i j}^{m} d_{i j}^{2}=\sum_{j=1}^{c} \sum_{i=1}^{n} u_{i j}^{m}\left\|\mathbf{p}_{j}-\mathbf{x}_{i}\right\|^{2}</script><p>其中 $m&gt;1$，$P_j$ 是第 $j$ 类的聚类中心，后面会给出公式。通常也会假设 $\sum_j u_{ij}=1$。</p>
<p>下面则给出该算法的推导：为了最小化损失函数，则可以应用 Lagrange 乘子法</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{n} \sum_{j=1}^{c} u_{i j}^{m} d_{i j}^{2}-\sum_{i=1}^{n} \lambda_{i}\left(\left(\sum_{j=1}^{c} u_{i j}\right)-1\right)</script><p>对 $\lambda_i,u_{ij}$ 求偏导等于 0，则可以得到迭代公式为</p>
<script type="math/tex; mode=display">
u_{i j}=\frac{\left(\frac{1}{d_{i j}}\right)^{2 /(m-1)}}{\sum_{k=1}^{c}\left(\frac{1}{d_{i k}}\right)^{2 /(m-1)}} \\\mathbf{p}_{j}=\frac{\sum_{i=1}^{n} u_{i j}^{m} \mathbf{x}_{i}}{\sum_{i=1}^{n} u_{i j}^{m}}</script><p>由此就可以给出 FCM 算法的框架</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-fcm.PNG" alt="FCM"></p>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>模糊聚类</tag>
        <tag>模糊矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 7：拟凸函数 Quasiconvex Function</title>
    <url>/2020/03/06/optimization/ch7-quasicvx/</url>
    <content><![CDATA[<h2 id="1-拟凸函数定义"><a href="#1-拟凸函数定义" class="headerlink" title="1. 拟凸函数定义"></a>1. 拟凸函数定义</h2><p><strong>拟凸函数(quasiconvex function)</strong>的定义为：若 $\text{dom}f$ 为凸集，且对任意的 $\alpha$，其下水平集 </p>
<script type="math/tex; mode=display">
S_\alpha = \{x\in\text{dom}f | f(x)\le\alpha\}</script><p>都是凸集，则 $f$ 为拟凸函数。</p>
<a id="more"></a>
<p>类似的有<strong>拟凹函数(quasiconcave)</strong>的定义。如果一个函数既是拟凸的，又是拟凹的，那么它是<strong>拟线性(quasilinear)</strong>的。</p>
<blockquote>
<p><strong>Reamrks</strong>：对于拟线性函数，要求其上水平集和下水平集同时是凸集，因此简单理解，其在某种意义上具有“单调性”。比如 $e^x,\log x$ 等。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>拟凸函数</th>
<th>各种函数的关系</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-quasicvx.PNG" alt="quasiconvex function"></td>
<td><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-quasi.PNG" alt="functions"></td>
</tr>
</tbody>
</table>
</div>
<p>一些常见的拟凸/拟凹/拟线性函数比如</p>
<p><strong><em>拟凸函数</em></strong></p>
<ul>
<li>$f(x)=\sqrt{|x|}$</li>
<li>$f(x)=\frac{\Vert x-a\Vert_2}{\Vert x-b\Vert_2},\text{dom}f=\{x|\Vert x-a\Vert_2 \le \Vert x-b\Vert_2\}$ </li>
</ul>
<p><strong><em>拟凹函数</em></strong></p>
<ul>
<li>$f(x)=x_1x_2$ on $R^2$</li>
</ul>
<p><strong><em>拟线性函数</em></strong></p>
<ul>
<li>$\text{ceil}(x)=\inf\{z\in Z|z\ge x\}$</li>
<li>$\log x$ on $R_{++}$</li>
<li>$f(x)=\frac{a^Tx+b}{c^Tx+d},\text{dom}f=\{c^Tx+d &gt;0\}$ (注意 $f$ 本身不是凸函数，但是是一个保凸变换)</li>
</ul>
<h2 id="2-拟凸函数的判定-等价定义"><a href="#2-拟凸函数的判定-等价定义" class="headerlink" title="2. 拟凸函数的判定/等价定义"></a>2. 拟凸函数的判定/等价定义</h2><p>对于普通凸函数，有一些等价定义，比如</p>
<ul>
<li>Jensen 不等式 / 凸函数原生定义：$f(\theta x+(1-\theta)y)\le \theta f(x)+(1-\theta)f(y)$</li>
<li>“降维打击”：$g(t)=f(x+tv)$ convex $\iff f(x)$ convex</li>
<li>一阶条件：$f(x)$ convex $\iff f(y)\ge \nabla f^T(x)(y-x)$</li>
<li>二阶条件：$f(x)$ convex $\iff \nabla f^2(x)\succeq 0$</li>
<li>epigraph：$f(x)$ convex  $\iff \text{epi}f$ convex </li>
</ul>
<h3 id="2-1-Jensen-不等式"><a href="#2-1-Jensen-不等式" class="headerlink" title="2.1 Jensen 不等式"></a>2.1 Jensen 不等式</h3><blockquote>
<p><strong>等价定义 1</strong>：函数 $f$ 为拟凸的<strong>等价于</strong>：定义域为凸集，且</p>
<script type="math/tex; mode=display">
0\le\theta\le1 \Longrightarrow f(\theta x+(1-\theta)y)\le\max\{f(x),f(y)\}</script><p>证明可以直接用定义。</p>
</blockquote>
<h3 id="2-1-“降维打击”"><a href="#2-1-“降维打击”" class="headerlink" title="2.1 “降维打击”"></a>2.1 “降维打击”</h3><blockquote>
<p><strong>等价定义 2</strong>：$g(t)=f(x+tv)$ quasiconvex $\iff f(x)$ quasiconvex</p>
<p>证明可以直接用定义。</p>
<p><strong>Reamrks</strong>：这种“降维打击”的定义方式实际上就是要求 $f$ 沿着任意一个方向都是(拟)凸的，对于一些高维空间中难以判定凸性，而其一维形式又比较简单的函数来说较适用，比如下面的二阶条件的证明。</p>
</blockquote>
<h3 id="2-2-一阶条件"><a href="#2-2-一阶条件" class="headerlink" title="2.2 一阶条件"></a>2.2 一阶条件</h3><blockquote>
<p><strong>等价定义 3</strong>：$f(x)$ quasiconvex <strong>等价于</strong>：定义域为凸集，且</p>
<script type="math/tex; mode=display">
f(y)\le f(x) \Longrightarrow \nabla f^T(x)(y-x)\le0</script><p><strong>Remarks</strong>：该定义实际上是指 $\nabla f(x)$ 定义了一个 $R^n$ 空间中的超平面(作为法向量)，该超平面就是某一个下水平集的支撑超平面</p>
<script type="math/tex; mode=display">
S_\alpha = \{y| f(y)\le f(x)=\alpha \}\subseteq R^n</script><p>需要注意的是，前面讲的对于凸函数来说 $\left[\nabla f^T(x)\ \ -1\right]^T$ 定义了一个 $R^{n+1}$ 空间中的支撑超平面。注意二者的不同！！！前者是<strong>下水平集的支撑超平面</strong>，后者是<strong>凸函数 surface 的支撑超平面</strong>，二者相差一个维度。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/7-quasi_support.PNG" alt="support hyperplane"></p>
<p>上图中三条封闭曲线代表三个水平集，而 $\nabla f(x)$ 就是其中一个水平集的支撑超平面。这可以联想梯度下降法（牛顿下山法），想象这是一个山谷，每一条线都是一个等高线，$\left[\nabla f^T(x)\ \ -1\right]^T$是山坡地面的法向量，指向空中，而$\nabla f(x)$则在这个等高线所在的平面内，且指向山体内部，与等高线相切。。</p>
</blockquote>
<h3 id="2-3-二阶条件"><a href="#2-3-二阶条件" class="headerlink" title="2.3 二阶条件"></a>2.3 二阶条件</h3><p>对于拟凸函数来说，没有二阶的充分必要条件，有充分条件和必要条件。</p>
<blockquote>
<p><strong>必要条件</strong>：$f(x)$ quasiconvex $\Longrightarrow$ 对任意 $x\in\text{dom}f,y\in R^n$</p>
<script type="math/tex; mode=display">
\text{if }\quad  y^T \nabla f(x)=0 \Longrightarrow y^T\nabla^2f(x)y\ge0</script><p>对于一维函数，只需要 $f’(x)=0 \Longrightarrow f’’(x)\ge0$</p>
<p><strong>充分条件</strong>：$f(x)$ quasiconvex $\Longleftarrow$ 对任意 $x\in\text{dom}f,y\in R^n,y\ne0$</p>
<script type="math/tex; mode=display">
\text{if }\quad  y^T \nabla f(x)=0 \Longrightarrow y^T\nabla^2f(x)y>0</script><p>证明：注意这里对于一维函数 $f:R\to R$ 较简单，因此可以应用“降维打击”的等价定义进行证明。</p>
</blockquote>
<h2 id="3-拟凸函数的保凸变换"><a href="#3-拟凸函数的保凸变换" class="headerlink" title="3. 拟凸函数的保凸变换"></a>3. 拟凸函数的保凸变换</h2><p>先来复习一下凸函数的保凸变换</p>
<ul>
<li>正权重求和</li>
<li>与仿射变换复合</li>
<li>最大值/上确界</li>
<li>单调凸函数与凸函数的复合</li>
<li>下确界</li>
<li>透射变换</li>
</ul>
<p>拟凸函数的也是类似的，主要少了第一个和最后一个，也即拟凸函数的正权重求和不一定是拟凸的，也没有透射变换的定义。</p>
<h3 id="3-1-与仿射变换复合"><a href="#3-1-与仿射变换复合" class="headerlink" title="3.1 与仿射变换复合"></a>3.1 与仿射变换复合</h3><blockquote>
<p>若 $f$ 拟凸，则 $g(x)=f(Ax+b)$ 也是拟凸的</p>
</blockquote>
<p>这个也很简单，因为 $g(x)$ 的下水平集就是 $f(y)$ 的下水平集外加仿射变换 $y=Ax+b$，仿射变换不改变凸性。</p>
<h3 id="3-2-最大值-上确界"><a href="#3-2-最大值-上确界" class="headerlink" title="3.2 最大值/上确界"></a>3.2 最大值/上确界</h3><blockquote>
<p><strong>离散情况</strong>：$f=\max\{\omega_1 f_1,…,\omega_m f_m\},\omega_i\ge0$</p>
<p><strong>连续情况</strong>：$g(x)=\sup\{w(y)f(x,y),\omega(y)\ge0\}$ 是拟凸的，如果 $f(\cdot,y)$ 是拟凸的</p>
</blockquote>
<p>证明很简单，因为导出函数的下水平集就是多个拟凸函数下水平集的交集，当然也是凸集。</p>
<h3 id="3-3-复合函数"><a href="#3-3-复合函数" class="headerlink" title="3.3 复合函数"></a>3.3 复合函数</h3><blockquote>
<p>如果 $g$ 为拟凸函数，且 $h$ 单调递增，则 $f=h\circ g$ 是拟凸的</p>
</blockquote>
<p>证明的话也可以从下水平集的角度理解。</p>
<p><strong><em>例</em></strong>：若 $f$ 是拟凸的，则 $g(x)=f(Ax+b)$ 是拟凸的，且 $\tilde{g}(x)=f((Ax+b)/(c^Tx+d))$ 也是拟凸的，其中 </p>
<script type="math/tex; mode=display">
\{x|c^Tx+d>0,(Ax+b)/(c^Tx+d)\in\text{dom}f\}</script><p>原因很简单，$\tilde{g}(x)$ 的下水平集就是 $f(y)$ 的下水平集外加一个线性分式变换 $y=(Ax+b)/(c^Tx+d)$，而线性分式函数也不改变凸性。</p>
<h3 id="3-4-下确界"><a href="#3-4-下确界" class="headerlink" title="3.4 下确界"></a>3.4 下确界</h3><blockquote>
<p>$f(x,y)$ 对于 $(x,y)$ 是拟凸的，则 $g(x)=\inf_{y\in C}f(x,y)$ 是拟凸的</p>
</blockquote>
<p>证明可以应用 Jensen 不等式。</p>
<blockquote>
<p><strong>Reamrks</strong>：这些保凸变换，前三个都可以直接从下水平集的角度来理解和证明，变换后函数的下水平集都是原始函数下水平集外加一个集合保凸变换，最后一个则不太直观，不过也可以由 Jensen 不等式直接导出。看来要理解拟凸函数，还是要多从下水平集的角度来看，并且集合的保凸变换也是很重要的！</p>
</blockquote>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>拟凸函数</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 6：共轭函数 Conjugate Function</title>
    <url>/2020/03/04/optimization/ch6-conjugate-func/</url>
    <content><![CDATA[<h2 id="1-共轭函数"><a href="#1-共轭函数" class="headerlink" title="1. 共轭函数"></a>1. 共轭函数</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>一个函数 $f$ 的<strong>共轭函数(conjugate function)</strong> 定义为</p>
<script type="math/tex; mode=display">
f^*(y)=\sup_{x\in\text{dom}f}(y^Tx-f(x))</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/6-conjugate.PNG" alt="conjugate function"></p>
<a id="more"></a>
<p>$f^*$ 是凸函数，证明也很简单，可以看成是一系列关于 $y$ 的凸函数取上确界。</p>
<p><strong>Remarks</strong>：实际上共轭函数与前面讲的一系列支撑超平面包围 $f$ 很类似，通过 $y$ 取不同的值，也就获得了不同斜率的支撑超平面，最后把 $f$ 包围起来，就好像是得到了 $\text{epi }f$ 的一个闭包，如下图所示</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/6-conjugate2.PNG" alt="conjugate function"></p>
<h3 id="1-2-性质"><a href="#1-2-性质" class="headerlink" title="1.2 性质"></a>1.2 性质</h3><p>关于共轭函数有以下性质</p>
<ol>
<li>若 $f$ 为凸的且是闭的($\text{epi }f$ 为闭集)，则 $f^{**}=f$ (可以联系上面提到一系列支撑超平面)</li>
<li>(Fenchel’s inequality) $f(x)+f^*(y)\ge x^Ty$，这可以类比均值不等式</li>
<li>(Legendre transform)如果 $f\in C^1$，且为凸的、闭的，设 $x^<em>=\arg\max\{y^Tx-f(x)\}$，那么有 $x^</em>=\nabla f^<em>(y)\iff y=\nabla f(x^</em>)$。这可以用来求极值，比如 $\min f(x)\Longrightarrow 0=\nabla f(x)\iff x=\nabla f^*(0)$</li>
</ol>
<h3 id="1-3-例子"><a href="#1-3-例子" class="headerlink" title="1.3 例子"></a>1.3 例子</h3><p>常用的共轭函数的例子有</p>
<p><strong>负对数函数</strong> $f(x)=-\log x$</p>
<script type="math/tex; mode=display">
\begin{aligned}
f^{*}(y) &=\sup _{x>0}(x y+\log x) \\
&=\left\{\begin{array}{ll}
-1-\log (-y) & y<0 \\
\infty & \text { otherwise }
\end{array}\right.
\end{aligned}</script><p><strong>凸二次函数</strong> $f(x)=(1 / 2) x^{T} Q x$ with $Q \in \mathbf{S}_{++}^{n}$</p>
<script type="math/tex; mode=display">
\begin{aligned}
f^{*}(y) &=\sup _{x}\left(y^{T} x-(1 / 2) x^{T} Q x\right) \\
&=\frac{1}{2} y^{T} Q^{-1} y
\end{aligned}</script><p><strong>指示函数</strong> $I_S(x)=0$ on $\text{dom}I_S=S$</p>
<script type="math/tex; mode=display">
I_S^*(y)=\sup\{y^Tx|x\in S\}</script><p><strong>log-sum-exp 函数</strong> $f(x)=\log\sum\exp x_i$</p>
<script type="math/tex; mode=display">
f^{*}(y)=\left\{\begin{array}{ll}
\sum_{i=1}^{n} y_{i} \log y_{i} & \text { if } y \succeq 0 \text { and } \mathbf{1}^{T} y=1 \\
\infty & \text { otherwise. }
\end{array}\right.</script><p><strong>范数</strong> $f(x)=\Vert x\Vert$</p>
<script type="math/tex; mode=display">
f^{*}(y)=\left\{\begin{array}{ll}
0 & \|y\|_{*} \leq 1 \\
\infty & \text { otherwise }
\end{array}\right.</script><p><strong>范数平方</strong> $f(x)=(1/2)\Vert x\Vert^2$</p>
<script type="math/tex; mode=display">
f^*(y)=(1/2)\Vert y\Vert_*^2</script><p><strong>负熵</strong> $f(x)=\sum x_i\log x_i$</p>
<script type="math/tex; mode=display">
f^*(y)=\sum e^{y_i-1}</script>]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>共轭函数</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 5：保凸变换</title>
    <url>/2020/03/04/optimization/ch5-preserve-cvx/</url>
    <content><![CDATA[<h2 id="1-保凸变换"><a href="#1-保凸变换" class="headerlink" title="1. 保凸变换"></a>1. 保凸变换</h2><p>前面提到过一次保凸变换，前面针对的是集合，凸集经过一定的保凸变换，映射后的集合仍然是凸集。这里复习一下</p>
<ol>
<li>任意个凸集的交集</li>
<li>仿射变换</li>
<li>透视变换</li>
<li>分式线性函数</li>
</ol>
<p>而这里要讲的是对函数经过操作以后，得到的仍然是凸函数。</p>
<a id="more"></a>
<h3 id="1-1-正权重求和"><a href="#1-1-正权重求和" class="headerlink" title="1.1 正权重求和"></a>1.1 正权重求和</h3><blockquote>
<p>离散情况：$f=\sum\omega_i f_i,\omega_i\ge0$</p>
<p>连续情况：$f=\int\omega(y)f(y)dy,\omega(y)\ge0$</p>
</blockquote>
<h3 id="1-2-与仿射变换的复合函数"><a href="#1-2-与仿射变换的复合函数" class="headerlink" title="1.2 与仿射变换的复合函数"></a>1.2 与仿射变换的复合函数</h3><blockquote>
<p>若 $f$ 为凸函数，则 $f(Ax+b)$ 也为凸函数。</p>
</blockquote>
<p><strong>Remarks</strong>：反之则不一定成立，若想成立（根据后面复合函数的原理）则仿射变换应具有一定的单调性。</p>
<h3 id="1-3-逐元素最大值-amp-上确界"><a href="#1-3-逐元素最大值-amp-上确界" class="headerlink" title="1.3 逐元素最大值&amp;上确界"></a>1.3 逐元素最大值&amp;上确界</h3><blockquote>
<p>若 $f_i$ 均为凸函数，则 $f(x)=\max_i\{f_1(x),…,f_n(x)\}$ 也为凸函数。</p>
</blockquote>
<p><strong>Remarks</strong>：这实际上可以看成是 $\text{epi} f=\bigcap \text{epi}f_i$，多个凸集的交集仍然是凸集。</p>
<blockquote>
<p>若与仿射变换相结合，则可以得到 $f(x)=\max_i\{a_1^Tx+b_1,…,a_n^Tx+b_n\}$ 也是凸函数。</p>
</blockquote>
<p><strong><em>例</em></strong>：根据上述结论，可以推广得到，$n$ 个元素中最大的 $r$ 的求和也是凸函数，证明很简单。</p>
<blockquote>
<p>若 $f(x,y)$ 关于 $x$ 是凸的，对任意 $y\in\mathcal{A}$，则 $g(x)=\sup_{y\in\mathcal{A}}f(x,y)$ 也是凸的。</p>
</blockquote>
<p><strong>Remarks</strong>：上述情况跟逐元素最大值是类似的，可以看成是无穷个 epigraph 的交集。</p>
<blockquote>
<p>由上述结论，可以得到一个重要性质</p>
<p><strong>Property</strong>：若 $f$ 为凸函数，则 $f(x)=\sup\{g(x)| g\ \text{is affine},g(z)\le f(z),\forall z\}$</p>
<p><strong>Remarks</strong>：上述性质所描述的事情实际上就是 $f$ 被很多个支撑超平面(以及更靠下的平面)紧紧的围起来了。证明过程实际上就是找到每个 $x$ 对应的(支撑)超平面。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/5-support.jpg" alt="support hyperplane"></p>
<p>通过上面的证明，可以得到的一个结论就是 $\forall x\in \text{int dom}f$，都存在一个 $y$ 使得 </p>
<script type="math/tex; mode=display">
f(z)\ge f(x)+y^T(z-x),\forall z\in\text{dom}f</script><p>由此可以引出<strong>次梯度(subgradient)</strong>的概念</p>
<script type="math/tex; mode=display">
\partial f=\{g| f(z)\ge f(x)+g^T(z-x),\forall z\in\text{dom}f \}</script><p>注意这里得到的是一系列梯度值的集合，这个集合有以下性质</p>
<ol>
<li>$\partial f(x)\ne \varnothing, \text{if}\ x\in\text{int dom}f$</li>
<li>$\partial f(x)$ convex and closed</li>
<li>$\partial f(x)$ bounded if $x\in\text{int dom}f$</li>
</ol>
</blockquote>
<p><strong><em>例</em></strong>：应用上述结论，可验证下面这些函数是凸的</p>
<ol>
<li>集合 $C$ 的支撑函数(support function)定义为：$S_C(x)=\sup_{y\in C}y^Tx$ (实际上可以将 $x$ 看作某个支撑超平面的法向量)</li>
<li>到集合 $C$ 的最远距离：$f(x)=\sup_{y\in C}\Vert x-y\Vert$ (距离关于 $x,y$ 都是凸的)</li>
<li>矩阵范数：$\Vert X\Vert_{a,b}=\sup\left\{\Vert Xv\Vert_a / \Vert v\Vert_b\right\}$ (证明过程用到了范数的等价定义，可参考 Boyd 的书)</li>
</ol>
<h3 id="1-4-下确界"><a href="#1-4-下确界" class="headerlink" title="1.4 下确界"></a>1.4 下确界</h3><p>前面提到了逐元素上确界，实际上就是 epigraph 的交集，而取下确界呢？是类似的，只不过对 $f(x,y)$ 的要求更严了</p>
<blockquote>
<p>若 $f(x,y)$ 关于 $(x,y)$ 是凸的，<strong>$C$ 是一个凸集</strong>，则 $g(x)=\inf_{y\in C}f(x,y)$ 是凸的。</p>
</blockquote>
<p>上述性质可应用下确性质定义来证明，也可以从 epigraph 角度来理解：$\forall(x,t)\in\text{epi }g$，都有 $(x,y,t)\in \text{epi }f,\text{ for some }y\in C$，所以 $\text{epi }g$ 实际上可以看作 $\text{epi }f$ 向低维空间中的一个投影，也是一个仿射变换/线性变换，因此 $\text{epi }g$ 也是凸的。</p>
<p><strong>Remarks</strong>：注意上面还要求 $C$ 是一个凸集，因为凸函数要求其定义域也为凸集。</p>
<p><strong><em>例</em></strong>：到集合 $C$ 最近距离：$\text{dist}(x,S)=\inf_{y\in S}\Vert x-y\Vert$ 是凸的，如果 $S$ 是凸的。</p>
<h3 id="1-5-复合函数"><a href="#1-5-复合函数" class="headerlink" title="1.5 复合函数"></a>1.5 复合函数</h3><p>两个凸函数的复合函数不一定是凸的，比如 $f(x)=-x,g(x)=x^2$，那么 $f(g(x))=-x^3$ 非凸</p>
<blockquote>
<h4 id="1-标量复合函数"><a href="#1-标量复合函数" class="headerlink" title="1. 标量复合函数"></a>1. 标量复合函数</h4><p>有函数 $g:R^n\to R,\ h:R\to R$，对于复合函数 $f(x)= h(g(x))$ </p>
<ol>
<li>$f(x)$ 为凸函数，若 $g$ convex，$h$ convex，$h$ <strong>单调不减</strong></li>
<li>$f(x)$ 为凸函数，若 $g$ concave，$h$ convex，$h$ <strong>单调增</strong></li>
</ol>
<h4 id="2-向量复合函数"><a href="#2-向量复合函数" class="headerlink" title="2. 向量复合函数"></a>2. 向量复合函数</h4><p>有函数 $g:R^n\to R^k,\ h:R^k\to R$，对于复合函数 $f(x)= h(g(x))=h(g_1(x),…,g_n(x))$ </p>
<ol>
<li>$f(x)$ 为凸函数，若 $g_i$ convex，$h$ convex，$h$ <strong>关于每个元素</strong>都单调不减</li>
<li>$f(x)$ 为凸函数，若 $g_i$ concave，$h$ convex，$h$ <strong>关于每个元素</strong>都单调增</li>
</ol>
</blockquote>
<p>证明：标量函数 $f^{\prime \prime}(x)=h^{\prime \prime}(g(x)) g^{\prime}(x)^{2}+h^{\prime}(g(x)) g^{\prime \prime}(x)$</p>
<p>向量复合函数 $f^{\prime \prime}(x)=g^{\prime}(x)^{T} \nabla^{2} h(g(x)) g^{\prime}(x)+\nabla h(g(x))^{T} g^{\prime \prime}(x)$</p>
<p><strong>Remarks</strong>：</p>
<ol>
<li>回到刚开始提到仿射变换与凸函数的复合 $f(Ax+b)$ 是凸的，但是 $Affine(f(x))$ 就不一定了，若是凸函数则需要仿射变换具有一定的单调性</li>
</ol>
<p><strong><em>例</em></strong>：常见的例子有</p>
<ol>
<li>$\exp g(x)$ 是凸的，如果 $g(x)$ 是凸的</li>
<li>$1/g(x)$ 是凸的，如果 $g(x)$ 是凸的且为正值</li>
<li>$\sum\log g_i(x)$ 为凹的，如果 $g_i$ 是凹的且为正值</li>
<li>$\log\sum\exp g_i(x)$ 为凸的，如果 $g_i$ 为凸的</li>
</ol>
<h3 id="1-6-透射变换"><a href="#1-6-透射变换" class="headerlink" title="1.6 透射变换"></a>1.6 透射变换</h3><p>函数 $f:R^n\to R$ 的透射变换 $g:R^n\times R\to R$ 定义为</p>
<script type="math/tex; mode=display">
g(x,t)=tf(x/t),\text{dom }g=\{(x,t)|x/t \in\text{dom }f,t>0 \}</script><blockquote>
<p>若 $f$ 是凸的，则 $g$ 是凸的。</p>
</blockquote>
<p><strong>Remarks</strong>：上述变换在一些问题中应该能够对应一些物理意义，不过我暂时还没想起来。证明也可以用 epigraph 来证明。</p>
<p><strong>例</strong>：对负熵 $f(x)=-\log x$，相对熵为 $g(x,t)=t\log t-t\log x$</p>
<p>类似的，对向量函数 $KL(u,v)=-\sum(u_i\log(u_i/v_i)-u_i+v_i)$ 也是凸的，这实际上就就是 KL-divergence。</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>保凸变换</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 4：模糊关系</title>
    <url>/2020/03/01/fuzzy/ch4-relation/</url>
    <content><![CDATA[<h2 id="1-模糊关系"><a href="#1-模糊关系" class="headerlink" title="1. 模糊关系"></a>1. 模糊关系</h2><p><strong>定义</strong>：模糊关系 $R$ 的隶属函数 $\mu_R:U\times V\to[0,1]$，其中 $\mu_R(x,y)$ 表示 $(x,y)$ 具有关系 $R$ 的程度</p>
<blockquote>
<p><strong>Remarks</strong>：实际上模糊关系 $R$ 就是定义在一个笛卡尔积的论域 $U\times V$ 上的模糊关系，与之前介绍的普通的模糊关系并无太大差别。</p>
</blockquote>
<a id="more"></a>
<p>基本运算定义为：</p>
<ul>
<li><strong>并</strong>：$\boldsymbol{\mu}_{R \cup S}(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{\mu}_{R}(\boldsymbol{x}, \boldsymbol{y}) \vee \boldsymbol{\mu}_{S}(\boldsymbol{x}, \boldsymbol{y})$</li>
<li><strong>交</strong>：$\boldsymbol{\mu}_{R \cap S}(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{\mu}_{R}(\boldsymbol{x}, \boldsymbol{y}) \wedge \boldsymbol{\mu}_{S}(\boldsymbol{x}, \boldsymbol{y})$</li>
<li><strong>补</strong>：$\mu_\bar{R}(x,y)=1-\mu_R(x,y)$</li>
<li><strong>包含</strong>：$\boldsymbol{R} \subseteq \boldsymbol{S} \Rightarrow \boldsymbol{\mu}_{R}(\boldsymbol{x}, \boldsymbol{y}) \leq \boldsymbol{\mu}_{S}(\boldsymbol{x}, \boldsymbol{y})$</li>
<li><strong>相等</strong>：$\boldsymbol{R} = \boldsymbol{S} \Rightarrow \boldsymbol{\mu}_{R}(\boldsymbol{x}, \boldsymbol{y}) = \boldsymbol{\mu}_{S}(\boldsymbol{x}, \boldsymbol{y})$</li>
</ul>
<p>一些模糊关系有：</p>
<ul>
<li><strong>恒等模糊关系</strong>：$R(x,y)=\mathbb{I}_{x=y}$</li>
<li><strong>零模糊关系</strong>：$O(x,y)=0$</li>
<li><strong>全称模糊关系</strong>：$E(x,y)=1$</li>
</ul>
<h2 id="2-模糊矩阵"><a href="#2-模糊矩阵" class="headerlink" title="2. 模糊矩阵"></a>2. 模糊矩阵</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p>对于有限论域 $U,V$，模糊矩阵的定义很容易可以获得 $R_{ij}=\mu_R(x_i,y_j)$</p>
<p>当 $R$ 的对角元素全部为 1 时，称为<strong>模糊自反矩阵</strong></p>
<p>模糊矩阵对应于集合的运算定义为：</p>
<ul>
<li><strong>并</strong>：$\boldsymbol{R} \cup \boldsymbol{S} \Leftrightarrow \boldsymbol{R} \cup \boldsymbol{S}=\left(\boldsymbol{r}_{i j} \vee \boldsymbol{s}_{i j}\right)$</li>
<li><strong>交</strong>：$\boldsymbol{R} \cap \boldsymbol{S} \Leftrightarrow \boldsymbol{R} \cup \boldsymbol{S}=\left(\boldsymbol{r}_{i j} \wedge \boldsymbol{s}_{i j}\right)$</li>
<li><strong>补</strong>：$R^c=(1-r_{ij})$</li>
<li><strong>包含</strong>：$\boldsymbol{R} \subseteq \boldsymbol{S} \Leftrightarrow\left(\boldsymbol{r}_{i j}\right) \leq\left(\boldsymbol{s}_{i j}\right)$</li>
<li><strong>相等</strong>：$\boldsymbol{R} = \boldsymbol{S} \Leftrightarrow\left(\boldsymbol{r}_{i j}\right) =\left(\boldsymbol{s}_{i j}\right)$</li>
</ul>
<h3 id="2-2-运算性质"><a href="#2-2-运算性质" class="headerlink" title="2.2 运算性质"></a>2.2 运算性质</h3><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/4-prop1.PNG" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/4-prop2.png" alt=""></p>
<h3 id="2-3-截矩阵"><a href="#2-3-截矩阵" class="headerlink" title="2.3 截矩阵"></a>2.3 截矩阵</h3><p><strong>截矩阵</strong>的定义为 $R_\lambda=(r_{ij}(\lambda))$，其中 $r_{ij}(\lambda)=\mathbb{I}_{r_{ij}\ge\lambda}$</p>
<blockquote>
<p><strong>Remarks</strong>：截矩阵的定义对应着截集的概念，截集得到的是普通集合，响应的截矩阵也是布尔矩阵，完全没有不确定度。</p>
</blockquote>
<h3 id="2-4-模糊关系合成"><a href="#2-4-模糊关系合成" class="headerlink" title="2.4 模糊关系合成"></a>2.4 模糊关系合成</h3><p><strong>转置</strong>：略</p>
<p><strong>模糊乘积</strong>：设 $Q=(q_{ij})_{n\times m},R=(r_{ij})_{m\times t}$，定义 $S=QR\in\mathcal{F}_{n\times t}$，有 $S_{ik}=\vee_{j=1}^m(q_{ij}\wedge r_{jk})$</p>
<blockquote>
<p><strong>Remarks</strong>：模糊乘积实际上表示了两个模糊关系的复合，即 $Q\in\mathcal{F}(U\times V),R\in\mathcal{F}(V\times W)$，最后合成了模糊关系 $S\in\mathcal{F}(U\times W)$。从公式上来看，模糊矩阵的乘积跟普通矩阵的乘积很像，只不过乘法换成了 $\wedge$，加法换成了 $\vee$。</p>
</blockquote>
<p>模糊关系的合成具有以下性质：</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/4-prop3.png" alt=""></p>
<h2 id="3-模糊关系性质"><a href="#3-模糊关系性质" class="headerlink" title="3. 模糊关系性质"></a>3. 模糊关系性质</h2><h3 id="3-1-自反性、对称性、传递性"><a href="#3-1-自反性、对称性、传递性" class="headerlink" title="3.1 自反性、对称性、传递性"></a>3.1 自反性、对称性、传递性</h3><p>就像普通集合的关系一样，模糊集合有三个重要性质：自反性、对称性、传递性。</p>
<p><strong>自反性</strong>：若 $\forall x\in U,\mu_R(x,x)=1$，则称 $R$ 满足<strong>自反性</strong>，相应的有模糊矩阵 $I\subseteq R$</p>
<p><strong>定理 1</strong>：若 $A$ 为自反矩阵，则有</p>
<script type="math/tex; mode=display">
I\subseteq A \subseteq A^2 \subseteq \cdots \subseteq A^n \subseteq \cdots</script><p><strong>对称性</strong>：若 $\forall x,y\in U,\mu_R(x,y)=\mu_R(y,x)$，则称 $R$ 满足<strong>对称性</strong>，相应的有模糊矩阵 $R^T=R$</p>
<p><strong>传递性</strong>：$\mu_R(x,z)\ge\vee_y (\mu_R(x,y)\wedge\mu_R(y,z))$，则称 $R$ 满足<strong>传递性</strong>，相应的有模糊矩阵 $R^2\subseteq R$</p>
<p><strong>定理 2</strong>：若 $Q$ 为传递矩阵，则有</p>
<script type="math/tex; mode=display">
Q \supseteq Q^{2} \supseteq Q^{3} \supseteq \cdots \supseteq Q^{\mathbf{n}-1} \supseteq Q^{\mathbf{n}} \supseteq \cdots</script><h3 id="3-2-模糊相似关系与等价关系"><a href="#3-2-模糊相似关系与等价关系" class="headerlink" title="3.2 模糊相似关系与等价关系"></a>3.2 模糊相似关系与等价关系</h3><p><strong>模糊相似关系</strong>：$R\in F(U\times U)$，满足自反性和对称性 $\Longrightarrow I\subseteq R\subseteq R^2\subseteq \cdots\subseteq R^n\subseteq \cdots$</p>
<p><strong>模糊等价关系</strong>：$R\in F(U\times U)$，满足自反性、对称性和传递性 $\Longrightarrow R=R^2=\cdots=R^n=\cdots$</p>
<blockquote>
<p><strong>定理</strong>：$R$ 为等价关系 $\iff R_\lambda$ 为等价关系 $\forall \lambda\in[0,1]$</p>
<p><strong>Proof</strong>：若 $R_\lambda$ 为等价关系，则意味着 $\forall i,j,k$，若 $r_{ij}(\lambda)=1,r_{jk}(\lambda)=1 \Longrightarrow r_{ik}(\lambda)=1$。因此对于模糊矩阵来说，应有 $r_{ij}\ge\lambda,r_{jk}\ge\lambda \Longrightarrow r_{ik}\ge\lambda$。在此基础上易证充分必要性。</p>
<p><strong>Remarks</strong>：这个定理将模糊等价关系转化为普通等价关系，而普通等价关系可以很容易分类。</p>
</blockquote>
<h3 id="3-3-对称闭包与传递闭包"><a href="#3-3-对称闭包与传递闭包" class="headerlink" title="3.3 对称闭包与传递闭包"></a>3.3 对称闭包与传递闭包</h3><p><strong>对称闭包</strong>：设 $A,\hat{A},B\in\mathcal{F}(U\times U)$，若 $A\subseteq\hat{A},A^T\subseteq\hat{A}$，且对任意包含 $A$ 的对称关系 $B$，都有 $\hat{A}\subseteq B$，则 $\hat{A}$ 为 $A$ 的对称闭包，记为 $s(A)=\hat{A}$。</p>
<blockquote>
<p>实际上对称闭包就是包含 $A$ 的<strong>最小的</strong>对称关系，很容易的有 $s(A)=A\cup A^T$</p>
</blockquote>
<p><strong>传递闭包</strong>：$A\subseteq\hat{A},A^2\subseteq\hat{A}$，且任意包含 $A$ 的传递关系 $B$ 都有 $\hat{A}\subseteq B$，则 $\hat{A}$ 为 $A$ 的传递闭包，记为 $t(A)=\hat{A}$。</p>
<p><strong>传递闭包定理 1</strong>：$t(A)=A\cup A^2 \cup\cdots\cup A^n\cup\cdots=\bigcup_{k=1}^\infty A^k$</p>
<p><strong>传递闭包定理 2</strong>：$t(A)=\bigcup_{k=1}^n A^k$ （可以使用鸽巢原理，证明 $A^{n+1}\subseteq A^m,m\le n$）</p>
<p><strong>传递闭包定理 3</strong>：相似矩阵 $R\in U_{n\times n}$ 的传递闭包是等价矩阵，且 $t(R)=R^n$</p>
<p><strong>传递闭包定理 4</strong>：相似矩阵 $R\in U_{n\times n}$，则 $\forall m\ge n,t(R)=R^m$</p>
<p><strong>传递闭包定理 5</strong>：相似矩阵 $R\in U_{n\times n}$，则 $\exist k\le n,t(R)=R^k$</p>
<blockquote>
<p><strong>Remarks</strong>：</p>
<ul>
<li>定理 2 证明了传递闭包在实际中是可计算的</li>
<li>定理 3-5 中对相似矩阵求传递闭包就得到了等价矩阵，对后面的模糊据类很有用，因为模糊等价矩阵可以与普通等价矩阵联系起来，而若想进行分类，则必须依托于等价关系。</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>模糊矩阵</tag>
        <tag>模糊关系</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 3：扩展原理</title>
    <url>/2020/03/01/fuzzy/ch3-extension/</url>
    <content><![CDATA[<p>扩展原理实际上是将普通集合中的<strong>函数映射</strong>的概念扩展到了模糊集合上。</p>
<a id="more"></a>
<h2 id="1-映射的象"><a href="#1-映射的象" class="headerlink" title="1. 映射的象"></a>1. 映射的象</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>对于普通集合，设 $f:X\to Y,A\subseteq X$，$f(A)=\{f(x)|x\in A\}$ 称为 $A$ 的象</p>
<p>映射 $f$ 诱导了一个从 $P(X)$ 到 $P(Y)$ 的新映射</p>
<h3 id="1-2-性质"><a href="#1-2-性质" class="headerlink" title="1.2 性质"></a>1.2 性质</h3><p>该映射有以下性质：</p>
<ul>
<li>$A\subseteq B\Longrightarrow f(A)\subseteq f(B)$</li>
<li>$f(\bigcup_{t\in T}A_t)=\bigcup_{t\in T}f(A_t)$</li>
<li>$f(\bigcap_{t\in T}A_t)=\bigcap_{t\in T}f(A_t)$</li>
</ul>
<h2 id="2-扩展原理"><a href="#2-扩展原理" class="headerlink" title="2. 扩展原理"></a>2. 扩展原理</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p>对于模糊集合，设 $f:X\to Y,A\in \mathcal{F}(X)$，定义 $f(A)\in\mathcal{F}(Y)$ 为</p>
<script type="math/tex; mode=display">
\forall y\in Y, \quad (f(A))(y)=\vee_{f(x)=y}A(x)</script><p>映射 $f$ 诱导了一个从 $\mathcal{F}(X)$ 到 $\mathcal{F}(Y)$ 的新映射</p>
<h3 id="2-2-性质"><a href="#2-2-性质" class="headerlink" title="2.2 性质"></a>2.2 性质</h3><p>该映射有以下性质：</p>
<ul>
<li>$A,B\in \mathcal{F}(X),A\subseteq B\Longrightarrow f(A)\subseteq f(B)$</li>
<li>$A_t\in\mathcal{F}(X),f(\bigcup_{t\in T}A_t)=\bigcup_{t\in T}f(A_t)$</li>
<li>$A_t\in\mathcal{F}(X),f(\bigcap_{t\in T}A_t)=\bigcap_{t\in T}f(A_t)$</li>
<li>$A\in\mathcal{F}(X),\alpha\in[0,1]$，则 $f(A_{\bar\alpha})=(f(A))_\bar\alpha$</li>
<li>$A\in\mathcal{F}(X),\alpha\in[0,1]$，则 $f(A_{\alpha})\subseteq(f(A))_\alpha$</li>
</ul>
<blockquote>
<p><strong>Remarks</strong>：从 $X$ 到 $Y$ 的映射，只是对元素“换了个名称”，并没有改变对应的隶属度，因此映射 $f$ 与对模糊集的操作比如截集是可以交换顺序的</p>
</blockquote>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
  </entry>
  <entry>
    <title>模糊数学笔记 2：贴近度</title>
    <url>/2020/03/01/fuzzy/ch2-nearness/</url>
    <content><![CDATA[<h2 id="1-贴近度"><a href="#1-贴近度" class="headerlink" title="1. 贴近度"></a>1. 贴近度</h2><p>给定 $A,B,C\in \mathcal{F}(U)$，$\sigma(<em>,</em>)$ 满足以下几个条件时，被称为<strong>贴近度</strong></p>
<ol>
<li>$\sigma(A,A)=1$</li>
<li>$\sigma(A,B)=\sigma(B,A)$</li>
<li>若 $A\subset B\subset C$，则 $\sigma(A,B)\ge\sigma(A,C),\sigma(B,C)\ge\sigma(A,C)$</li>
<li>$\sigma(U,\varnothing)=0$</li>
</ol>
<a id="more"></a>
<p><strong>严格贴近度</strong>的定义为</p>
<ol>
<li>$\sigma(A,B)=1 \iff A=B$</li>
<li>上述 2.-4. 条</li>
</ol>
<p>贴近度的例子：</p>
<ul>
<li>严格贴近度：$\sigma(A, B)=\frac{\sum_{n} a_{n} \wedge b_{n}}{\sum_n a_{n} \vee b_{n}}$</li>
<li>$\sigma(A, B)=1-t\left(\sum_{1}^{n}\left|a_{k}-b_{k}\right|^{p}\right)^{q}$</li>
<li>$\sigma(A, B)=\frac{\sum_{n} a_{n} \wedge b_{n}}{\sum_n (a_{n} + b_{n})/2}$</li>
<li>$\sigma(A, B)=\exp\left(-t\left(\sum_{1}^{n}\left|a_{k}-b_{k}\right|^{p}\right)^{q}\right)$</li>
</ul>
<h2 id="2-内外积"><a href="#2-内外积" class="headerlink" title="2. 内外积"></a>2. 内外积</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><p><strong>内积</strong>：$A,B\in\mathcal{F}(U)$，称 $A\circ B=\underset{u \in U}{\vee}(A(u) \wedge B(u))$ 为 $A,B$ 的内积</p>
<p><strong>外积</strong>：$A,B\in\mathcal{F}(U)$，称 $A \hat{\circ} B=\underset{u \in U}{\wedge}(A(u) \vee B(u))$ 为 $A,B$ 的外积</p>
<blockquote>
<p><strong>Remarks</strong>：内外积本身并不是用来表述两个集合的相似程度的，就像向量的内外积，还与向量自身的模长有关。</p>
</blockquote>
<h3 id="2-2-性质"><a href="#2-2-性质" class="headerlink" title="2.2 性质"></a>2.2 性质</h3><ul>
<li>$A \hat{\circ} B = A^c\circ B^c,(A\circ B)^c=A^c \hat{\circ} B^c$</li>
<li>$A {\circ} B\le \bar{a}\wedge\bar{b},A \hat{\circ} B\ge\underline{a}\vee\underline{b}$</li>
<li>$A\circ A=\bar{a},A \hat{\circ} A=\underline{a}$</li>
<li>$\underset{B \in \mathcal{F}(U)}{\vee}(A \circ B)=\bar{a}, \quad \underset{B \in \mathcal{F}(U)}{\wedge}\left(A {\hat{\circ}} B\right)=\underline{a}$</li>
<li>$A \subseteq B \Rightarrow A \circ B=\bar{a}, A{\hat{\circ}} B=\underline{b}$</li>
<li>$A \circ A^{c} \leq \frac{1}{2}, \quad A{\hat{\circ}} B \geq \frac{1}{2}$</li>
<li>$A \subseteq B \Rightarrow A \circ C \leq B \circ C, A{\hat{\circ}} C \leq B{\hat{o}} C$</li>
</ul>
<h2 id="3-格贴近度"><a href="#3-格贴近度" class="headerlink" title="3. 格贴近度"></a>3. 格贴近度</h2><p>给定F集A，让F集B靠近A，会使内积增大而外积减少。即当内积较大且外积较小时，A与B比较贴近。 所以，以内外积相结合的“格贴近度”来刻 划两个F集的贴近程度。</p>
<p><strong>格贴近度</strong>：$N_{1}(A, B)=(A \circ B) \wedge\left(A\hat{\circ} B\right)^{c}$</p>
<p>格贴近度有以下性质</p>
<ul>
<li>$0\le N_1(A,B)\le1$</li>
<li>$N_1(A,B)=N_1(B,A)$</li>
<li>$N_1(A,A)=\bar{a}\wedge(1-\underline{a})$</li>
<li>$A \subseteq B \subseteq C \Rightarrow N_1(A, C) \leq N_1(A, B) \wedge N_1(B, C)$</li>
</ul>
<blockquote>
<p><strong>Remarks</strong>：注意根据第 3 条性质可知，格贴近度并不适合描述两个模糊集的相似程度，比如 $N_1(U,U)=0$</p>
</blockquote>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>贴近度</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 4：凸函数 Convex Function</title>
    <url>/2020/02/29/optimization/ch4-cvx-function/</url>
    <content><![CDATA[<h2 id="1-凸函数"><a href="#1-凸函数" class="headerlink" title="1. 凸函数"></a>1. 凸函数</h2><h3 id="1-1-凸函数定义"><a href="#1-1-凸函数定义" class="headerlink" title="1.1 凸函数定义"></a>1.1 凸函数定义</h3><p><strong>凸函数(convex function)</strong>的定义：</p>
<script type="math/tex; mode=display">
f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y),\quad \forall x,y\in\text{dom}f,\theta\in[0,1]</script><p>函数 $f$ 的<strong>扩展函数(extended-value extension)</strong> $\tilde{f}$ 定义为</p>
<script type="math/tex; mode=display">
\tilde{f}(x)=\begin{cases}f(x),&x\in\text{dom}f\\\infty,&x\notin\text{dom}f\end{cases}</script><p>相当于对原来函数 $f$ 的定义域进行了扩展。</p>
<a id="more"></a>
<h3 id="1-2-常见凸函数"><a href="#1-2-常见凸函数" class="headerlink" title="1.2 常见凸函数"></a>1.2 常见凸函数</h3><h4 id="1-2-1-R"><a href="#1-2-1-R" class="headerlink" title="1.2.1 $R$"></a>1.2.1 $R$</h4><p>凸函数(convex)</p>
<ul>
<li>仿射函数 $ax+b$，for any $a,b\in R$</li>
<li>指数函数 $e^{ax}$，for any $a\in R$</li>
<li>幂函数 $x^\alpha,x\in R_{++}$，for $\alpha\ge1$ or $\alpha\le0$</li>
<li>绝对值幂函数 $\vert x\vert^p,x\in R$，for $p\ge 1$</li>
<li>负熵 $x\log x,x\in R_{++}$</li>
</ul>
<p>凹函数(concave)</p>
<ul>
<li>仿射函数 $ax+b$，for any $a,b\in R$</li>
<li>幂函数 $x^\alpha,x\in R_{++}$，for $0\le\alpha\le1$</li>
<li>对数函数 $\log x,x\in R_{++}$</li>
</ul>
<h4 id="1-2-2-R-n"><a href="#1-2-2-R-n" class="headerlink" title="1.2.2 $R^n$"></a>1.2.2 $R^n$</h4><ul>
<li>仿射函数 $a^Tx+b$</li>
<li>范数 $\Vert x\Vert_p,p\ge 1$</li>
</ul>
<h4 id="1-2-3-R-m-times-n"><a href="#1-2-3-R-m-times-n" class="headerlink" title="1.2.3 $R^{m\times n}$"></a>1.2.3 $R^{m\times n}$</h4><ul>
<li>仿射函数 $f(X)=\text{tr}(A^TX)+b$</li>
<li>谱范数 $f(X)=\Vert X\Vert_2=\sigma_{\max}(X)=(\lambda_\max(X^TX))^{1/2}$</li>
</ul>
<h2 id="2-凸函数判定"><a href="#2-凸函数判定" class="headerlink" title="2. 凸函数判定"></a>2. 凸函数判定</h2><h3 id="2-1-“降维打击”"><a href="#2-1-“降维打击”" class="headerlink" title="2.1 “降维打击”"></a>2.1 “降维打击”</h3><p>“降维打击”是指对于函数 $f:R^n\to R$ 限制在某一个方向上观察，若对于任意一个方向上都是凸函数，则 $f$ 就是凸函数。准确的定义如下。</p>
<blockquote>
<p>设 $f:R^n\to R$，有映射 $g:R\to R$</p>
<script type="math/tex; mode=display">
g(t)=f(x+tv),\quad \text{dom}\ g=\{t|x+tv\in\text{dom}\ f\}</script><p>则 $f$ 为凸函数<strong>当且仅当</strong> $g(t)$ 对任意 $x\in\text{dom}f,v\in R^n$ 都是凸函数。</p>
</blockquote>
<p><strong><em>例</em></strong>：函数 $f:S^n\to R$，有 $f(X)=\log\det X,\text{dom}f=S^n_{++}$</p>
<script type="math/tex; mode=display">
\begin{align}g(t)=\log\det(X+tV)&=\log\det X + \log\det(I+tX^{-1/2}VX^{-1/2})\\&=\log\det X + \sum_{i=1}^n \log(1+t\lambda_i)\end{align}</script><p>由于 $g$ 关于 $t$ 是凹函数，因此 $f$ 是凹函数。</p>
<h3 id="2-2-一阶条件"><a href="#2-2-一阶条件" class="headerlink" title="2.2 一阶条件"></a>2.2 一阶条件</h3><blockquote>
<p>函数 $f$ 为凸函数<strong>当且仅当</strong></p>
<script type="math/tex; mode=display">
f(y)\ge f(x)+\nabla f^T(x)(y-x) \quad \forall x,y\in\text{dom}f</script><p>证明：略。用定义即可。</p>
</blockquote>
<h3 id="2-3-二阶条件"><a href="#2-3-二阶条件" class="headerlink" title="2.3 二阶条件"></a>2.3 二阶条件</h3><blockquote>
<p>函数 $f$ 为凸函数<strong>当且仅当</strong>海森矩阵(Hessian matrix)（若二阶可微）</p>
<script type="math/tex; mode=display">
\nabla^2 f(x)\succeq0 \quad \forall x\in\text{dom}f</script></blockquote>
<p>可用海森矩阵验证为凸函数的例子</p>
<ul>
<li>二次函数 $f(x)=(1/2)x^TPx+q^Tx+r$ (with $P\in S^n$)</li>
<li>最小二乘目标函数 $f(x)=\Vert Ax-b\Vert_2^2$</li>
<li>二次函数 $f(x,y)=x^2/y$</li>
<li>log-sum-exp $f(x)=\log\sum_k\exp x_k$</li>
<li>几何均值 $f(x)=(\Pi_k x_k)^{1/n}$ on $R^n_{++}$</li>
</ul>
<h2 id="3-Sublevel-set-amp-Epigraph"><a href="#3-Sublevel-set-amp-Epigraph" class="headerlink" title="3. Sublevel set &amp; Epigraph"></a>3. Sublevel set &amp; Epigraph</h2><p>函数 $f:R^n\to R$ 的 $\alpha$ <strong>下水平集($\alpha$-sublevel set)</strong> 的定义为</p>
<script type="math/tex; mode=display">
C_\alpha=\{x\in\text{dom}f|f(x)\le\alpha\}</script><p><strong>凸函数的下水平集也是凸集，但是反之不一定成立</strong>。</p>
<p>针对函数 $f:R^n\to R$ 定义的 <strong>epigraph</strong> 为</p>
<script type="math/tex; mode=display">
\text{epi}f=\{(x,t)\in R^{n+1}|x\in\text{dom}f,f(x)\le t\}</script><blockquote>
<p>函数 $f$ 为凸函数<strong>当且仅当</strong>其 epigraph 为凸集。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/4-epigraph.PNG" alt="epigraph"></p>
<p><strong>Remarks</strong>：对于 epigraph 内的点，有</p>
<script type="math/tex; mode=display">
\begin{align}
t\ge f(y)\ge f(x)+\nabla f^T(x)(y-x) \\
\iff 
\left[\begin{array}{cc}\nabla f^T(x)\\-1\end{array}\right]
\left(\left[\begin{array}{cc}y\\t\end{array}\right] - \left[\begin{array}{cc}x\\f(x)\end{array}\right]\right)\le0
\end{align}</script><p>上面的式子实际上就是找到了一个在 $(x,f(x))$ 处的<strong>支撑超平面</strong>，<strong>法向量</strong>即为 $[\begin{array}{cc}\nabla f^T(x) &amp;-1\end{array}]^T$</p>
</blockquote>
<h2 id="4-Jensen’s-Inequality"><a href="#4-Jensen’s-Inequality" class="headerlink" title="4. Jensen’s Inequality"></a>4. Jensen’s Inequality</h2><p>对于凸函数 $f$，有</p>
<script type="math/tex; mode=display">
f(\mathbb{E}z)=\mathbb{E}f(z)</script><p>证明：离散情况易证，连续情况可以用一阶条件证明。</p>
<p><strong>Lemma</strong>：$f:R^n\to R$ 为凸函数，那么 $f$ 在任一<strong>内点</strong>处都<strong>连续</strong>(连续但不一定可导)。</p>
<p><strong>Remarks</strong>：往往绝大部分点都是可微的，但是我们经常会遇到那些不可微的点，比如 ReLU 函数。</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>凸函数</tag>
      </tags>
  </entry>
  <entry>
    <title>模糊数学笔记 1：模糊集 Fuzzy set</title>
    <url>/2020/02/26/fuzzy/ch1-set/</url>
    <content><![CDATA[<p>传统集合中元素要么属于集合，要么不属于集合。但假如我们给出一个“所有年轻人组成的集合”，那么这个时候年龄多大才算年轻人呢？30岁算年轻人嘛？有的人觉得算，有的人觉得不算，这是一个比较模糊的界定，所以就引入了模糊集的概念。</p>
<a id="more"></a>
<h2 id="1-传统集合的定义"><a href="#1-传统集合的定义" class="headerlink" title="1. 传统集合的定义"></a>1. 传统集合的定义</h2><p><strong>论域</strong> U，<strong>集合</strong> A，这可以用一个映射来表示</p>
<script type="math/tex; mode=display">
\begin{aligned}
\chi_{A}: \boldsymbol{U} \rightarrow &\{\mathbf{0}, \mathbf{1}\} \\
\boldsymbol{u} \mapsto & \chi_{A}(\boldsymbol{u})
\end{aligned}</script><p>也可以用一个分段函数来表示</p>
<script type="math/tex; mode=display">
\chi_{A}(u)=\left\{\begin{array}{ll}
{1,} & {u \in A} \\
{0,} & {u \notin A}
\end{array}\right.</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-set.png" alt="set"></p>
<h2 id="2-模糊集合的定义"><a href="#2-模糊集合的定义" class="headerlink" title="2. 模糊集合的定义"></a>2. 模糊集合的定义</h2><p>模糊集的含义表示其中的元素 $x$ “有一定的可能性”属于集合 $A$，或者说“一定程度上”属于集合 $A$，那么这个属于的程度就被称为<strong>隶属度</strong> $\mu_A(x)\in [0,1]$。与传统集合相比，传统集合中元素的隶属程度非 0 即 1，也即要么属于，要么不属于，是确定的，模糊集里则引入了一定的不确定性。也用一个映射表示为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu_{A}: &\boldsymbol{U} \rightarrow[0,1] \\
&\boldsymbol{x} \mapsto \mu_{A}(x) \in[0,1]
\end{aligned}</script><p>其中映射 $\mu_A$ 称为 $A$ 的<strong>隶属函数</strong>，$\mu_A(x)$ 为 $x$ 对 $A$ 的<strong>隶属度</strong>。注意 $\mu_A(x)=0.5$ 时表示最具有模糊性。</p>
<h2 id="3-模糊集的表示方法"><a href="#3-模糊集的表示方法" class="headerlink" title="3. 模糊集的表示方法"></a>3. 模糊集的表示方法</h2><h3 id="3-1-Zadeh-表示法"><a href="#3-1-Zadeh-表示法" class="headerlink" title="3.1 Zadeh 表示法"></a>3.1 Zadeh 表示法</h3><script type="math/tex; mode=display">
A=\frac{A\left(x_{1}\right)}{x_{1}}+\frac{A\left(x_{2}\right)}{x_{2}}+\cdots+\frac{A\left(x_{n}\right)}{x_{n}}</script><p>这里 $\frac{A\left(x_{i}\right)}{x_{i}}$ 表示 $x_i$ 对模糊集 $A$ 的隶属度为 $A(x_i)$。若论域 $U$ 为无限集，则模糊集表示为</p>
<script type="math/tex; mode=display">
A=\int_{x\in U} \frac{A(x)}{x}</script><h3 id="3-2-序偶表示法"><a href="#3-2-序偶表示法" class="headerlink" title="3.2 序偶表示法"></a>3.2 序偶表示法</h3><script type="math/tex; mode=display">
A=\left\{\left(x_{1}, A\left(x_{1}\right)\right),\left(x_{2}, A\left(x_{2}\right)\right), \cdots,\left(x_{n}, A\left(x_{n}\right)\right)\right\}</script><h3 id="3-3-向量表示法"><a href="#3-3-向量表示法" class="headerlink" title="3.3 向量表示法"></a>3.3 向量表示法</h3><script type="math/tex; mode=display">
A=(A(x_1),...,A(x_n))</script><h2 id="4-模糊集的运算"><a href="#4-模糊集的运算" class="headerlink" title="4. 模糊集的运算"></a>4. 模糊集的运算</h2><h3 id="4-1-集合基本运算"><a href="#4-1-集合基本运算" class="headerlink" title="4.1 集合基本运算"></a>4.1 集合基本运算</h3><ul>
<li><strong>相等</strong>：$A=B \iff A(x)=B(x), \forall x\in U$</li>
<li><strong>包含</strong>：$A\subset B \iff A(x)\le B(x), \forall x\in U$</li>
<li><strong>交集</strong>：$(A\cap B)(x) = A(x)\wedge B(x),\forall x\in U$</li>
<li><strong>并集</strong>：$(A\cup B)(x) = A(x)\vee B(x),\forall x\in U$</li>
<li><strong>补集</strong>：$A^c(x)=1-A(x),\forall x\in U$</li>
</ul>
<p>其中记号 $a\wedge b=\min\{a,b\},a\vee b=\max\{a,b\}$</p>
<h3 id="4-2-计算性质"><a href="#4-2-计算性质" class="headerlink" title="4.2 计算性质"></a>4.2 计算性质</h3><p>很多计算性质都和普通集合差不多</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-prpo1.png" alt="properties 1"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-prpo2.png" alt="properties 2"></p>
<p>需要注意的是<strong>无穷个</strong>集合的交集与并集的定义</p>
<script type="math/tex; mode=display">
\bigcup_{t \in T} A_{t}(a)=\sup_{t \in T} A_{t}(a) \\ 
\bigcap_{t \in T} A_{t}(a)=\inf_{t \in T} A_{t}(a)</script><h2 id="5-隶属度的确定"><a href="#5-隶属度的确定" class="headerlink" title="5. 隶属度的确定"></a>5. 隶属度的确定</h2><h3 id="5-1-实验统计法"><a href="#5-1-实验统计法" class="headerlink" title="5.1 实验统计法"></a>5.1 实验统计法</h3><h3 id="5-2-半-解析法"><a href="#5-2-半-解析法" class="headerlink" title="5.2 (半)解析法"></a>5.2 (半)解析法</h3><p>根据问题性质套用现有模糊分布，然后根据测量数据确定分布中的参数。</p>
<p>模糊分布大致分为：<strong>偏大型、偏小型、中间型</strong></p>
<h3 id="5-3-专家打分法"><a href="#5-3-专家打分法" class="headerlink" title="5.3 专家打分法"></a>5.3 专家打分法</h3><p>根据专家的反馈意见进行统计</p>
<h2 id="6-截集与分解定理"><a href="#6-截集与分解定理" class="headerlink" title="6. 截集与分解定理"></a>6. 截集与分解定理</h2><h3 id="6-1-截集的定义"><a href="#6-1-截集的定义" class="headerlink" title="6.1 截集的定义"></a>6.1 截集的定义</h3><p><strong>定义</strong>：若 $A$ 为 $U$ 上的任一模糊集，对 $\forall \lambda\in [0,1]$，记</p>
<script type="math/tex; mode=display">
A_\lambda = \{x|A(x)\ge\lambda,x\in U\}</script><p>称为 $A$ 的<strong>$\lambda$-截集</strong>，其中 $\lambda$ 称为阈值或置信水平。类似的，<strong>强截集(开截集)</strong>定义为</p>
<script type="math/tex; mode=display">
A_\lambda = \{x|A(x)>\lambda,x\in U\}</script><p>注意：<strong>截集为普通集合，不是模糊集</strong>！</p>
<h3 id="6-2-截集运算性质"><a href="#6-2-截集运算性质" class="headerlink" title="6.2 截集运算性质"></a>6.2 截集运算性质</h3><p>大部分性质都很简单</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-prpo3.png" alt="properties 3"></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/1-prpo4.png" alt="properties 4"></p>
<p>但注意其中第 3 和第 6 条注意并不是相等！！</p>
<h3 id="6-3-一些定义"><a href="#6-3-一些定义" class="headerlink" title="6.3 一些定义"></a>6.3 一些定义</h3><ul>
<li><strong>核</strong>：$ker(A)=A_1$</li>
<li><strong>支集</strong>：$supp(A)=A_{\bar{0}}$</li>
<li><strong>边界</strong>：$A_{\bar{0}}\backslash A_1$</li>
<li><strong>数乘</strong>：$\lambda A(u) = \lambda \wedge A(u),u\in U$<ul>
<li>$A\subset B \Rightarrow \lambda A \subset \lambda B$</li>
<li>$\lambda_1\le\lambda_2\Rightarrow \lambda_1 A \subset \lambda_2 A$</li>
</ul>
</li>
</ul>
<h3 id="6-4-分解定理"><a href="#6-4-分解定理" class="headerlink" title="6.4 分解定理"></a>6.4 分解定理</h3><p><strong>分解定理 1</strong>：$A\in \mathcal{F}(U)$，则 $A=\bigcup_{\lambda\in[0,1]}\lambda A_\lambda$</p>
<p><strong>分解定理 2</strong>：$A\in \mathcal{F}(U)$，则 </p>
]]></content>
      <categories>
        <category>Fuzzy Mathematics</category>
      </categories>
      <tags>
        <tag>模糊集</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 3：广义不等式</title>
    <url>/2020/02/26/optimization/ch3-gene-ineq/</url>
    <content><![CDATA[<p>将普通的不等式 $1\le2$ 推广到更加复杂的情况，例如两个向量 $x\le y$ 该如何定义？这就是广义不等式(generalized inequality)</p>
<a id="more"></a>
<h2 id="1-正常锥"><a href="#1-正常锥" class="headerlink" title="1. 正常锥"></a>1. 正常锥</h2><p><strong>正常锥(proper cone)</strong> $K\subseteq R^n$ 需要满足</p>
<ul>
<li>$K$ is closed (contains its boundary) </li>
<li>$K$ is solid (has nonempty interior) </li>
<li>$K$ is pointed (contains no line)</li>
</ul>
<h2 id="2-广义不等式"><a href="#2-广义不等式" class="headerlink" title="2. 广义不等式"></a>2. 广义不等式</h2><p>基于正常锥 $K$ 定义的不等式(generalized inequality)表示为</p>
<script type="math/tex; mode=display">
x \preceq_{K} y \quad \Longleftrightarrow \quad y-x \in K\\ 
x \prec_{K} y \quad \Longleftrightarrow \quad y-x \in \operatorname{int} K</script><h2 id="3-最小元与极小元"><a href="#3-最小元与极小元" class="headerlink" title="3. 最小元与极小元"></a>3. 最小元与极小元</h2><h3 id="3-1-最小元-minimum"><a href="#3-1-最小元-minimum" class="headerlink" title="3.1 最小元(minimum)"></a>3.1 最小元(minimum)</h3><p>$x\in S$ 为集合 $S$ 关于 $\preceq_{K}$ 的<strong>最小元(minimum)</strong>，如果有</p>
<script type="math/tex; mode=display">
y\in S \Longrightarrow x\preceq_{K}y</script><p>也等价于 $S\subseteq x+K$</p>
<p><strong>注意</strong>：1. 最小元可能<strong>不存在</strong>；2. 若存在则<strong>唯一</strong></p>
<h3 id="3-2-极小元-minimal"><a href="#3-2-极小元-minimal" class="headerlink" title="3.2 极小元(minimal)"></a>3.2 极小元(minimal)</h3><p> $x\in S$ 为集合 $S$ 关于 $\preceq_{K}$ 的<strong>极小元(minimum)</strong>，如果有</p>
<script type="math/tex; mode=display">
y\in S\quad y\preceq_{K}x \Longrightarrow y=x</script><p><strong>注意</strong>：极小元不唯一</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/3-minimum.png" alt="minimum and minimal"></p>
<h2 id="4-对偶锥"><a href="#4-对偶锥" class="headerlink" title="4. 对偶锥"></a>4. 对偶锥</h2><p>锥 $K$ 的<strong>对偶锥(dual cone)</strong> 定义为</p>
<script type="math/tex; mode=display">
K^{*}=\left\{y | y^{T} x \geq 0 \text { for all } x \in K\right\}</script><p>有一些性质</p>
<ul>
<li>$K^*$ is convex, closed</li>
<li>$K^{**}=K$ if $K$ is closed convex</li>
<li>$K$ is proper cone $\Longrightarrow K^{*}$ is proper</li>
</ul>
<p>一些例子</p>
<ul>
<li>$K=\mathbf{R}_{+}^{n}: K^{*}=\mathbf{R}_{+}^{n}$</li>
<li>$K=\mathbf{S}_{+}^{n}: K^{*}=\mathbf{S}_{+}^{n}$</li>
<li>$K=\left\{(x, t) ||x|_{2} \leq t\right\}: K^{*}=\left\{(x, t) ||x|_{2} \leq t\right\}$</li>
<li>$K=\left\{(x, t) ||x|_{1} \leq t\right\}: K^{*}=\left\{(x, t) ||x|_{\infty} \leq t\right\}$</li>
</ul>
<h2 id="5-最小元与极小元-应用对偶锥定义"><a href="#5-最小元与极小元-应用对偶锥定义" class="headerlink" title="5. 最小元与极小元(应用对偶锥定义)"></a>5. 最小元与极小元(应用对偶锥定义)</h2><h3 id="5-1-最小元"><a href="#5-1-最小元" class="headerlink" title="5.1 最小元"></a>5.1 最小元</h3><p>$x$ 是集合 $S$ 关于 $\preceq_{K}$ 的最小元 $\iff$ 对任意的 $\lambda \succ_{K*} 0$，$x$ 为 $\lambda^Tz$ 在集合 $S$ 上的唯一最小解</p>
<blockquote>
<p><strong>Remarks</strong>：实际上这点可以理解为对任意 $\lambda\in K^*$，其定义了一个支撑超平面，$x$ 就是支撑点</p>
</blockquote>
<h3 id="5-2-极小元"><a href="#5-2-极小元" class="headerlink" title="5.2 极小元"></a>5.2 极小元</h3><ul>
<li>充分条件：若对于某些 $\lambda \succ_{K*} 0$，$x$ minimizes $\lambda^Tz$ over $S$，$\Longrightarrow$ $x$ 为极小元</li>
<li>必要条件：$x$ 为<strong>凸集</strong> $S$ 的极小元，$\Longrightarrow$ 存在非 0 的 $\lambda \succ_{K*} 0$ 使得 $x$ minimizes $\lambda^Tz$ over $S$</li>
</ul>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>锥</tag>
        <tag>广义不等式</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 2：超平面分离定理</title>
    <url>/2020/02/26/optimization/ch2-separa-hyper/</url>
    <content><![CDATA[<p>简单理解就是：没有交集的两个凸集，可以被一个超平面完全分隔开。</p>
<a id="more"></a>
<h2 id="1-超平面分离定理"><a href="#1-超平面分离定理" class="headerlink" title="1. 超平面分离定理"></a>1. 超平面分离定理</h2><p><strong>超平面分离定理(Separating hyperplane theorem)</strong>：若 $C,D$ 为非空凸集，且 $C\cap D=\varnothing$，则存在 $a\ne 0,b$，使得</p>
<script type="math/tex; mode=display">
a^Tx\le b\quad\text{for}\quad x\in C,\quad a^Tx\ge b \quad\text{for}\quad x\in D</script><p>也可以等价表示为 $\inf_{x\in D}a^Tx \ge \sup_{x\in C}a^Tx$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/2-seperate.png" alt="Separating hyperplane theorem"></p>
<p><strong>Lemma 1</strong>：$C$ <strong>closed，convex</strong>，$y\notin C$，那么存在<strong>唯一的</strong> $x\in C$，使得 $\Vert y-x\Vert=\inf\{\Vert y-z\Vert|z\in C\}=d(y,C)$</p>
<p>Proof：omit…</p>
<p><strong>Lemma 2</strong>：$C$ <strong>closed，convex</strong>，$y\notin C$，那么存在 $a\ne 0,b$，使得</p>
<script type="math/tex; mode=display">
a^Ty<b,\quad a^Tx\ge b\quad\forall x\in C</script><p>Proof：omit…</p>
<blockquote>
<p><strong>Remark</strong>：上述定理表明存在超平面可以严格分开 $y$ 与 $C$。</p>
</blockquote>
<p><strong>Lemma 3</strong>：$C$ <strong>convex</strong>，$y\notin C$，那么存在 $a\ne 0,b$，使得</p>
<script type="math/tex; mode=display">
a^Ty\le b,\quad a^Tx\ge b\quad\forall x\in C</script><p>Proof：omit…</p>
<p><strong>超平面分离定理逆定理</strong>：若 $C$ 为开集，且存在超平面分离 $C,D$，则 $C\cap D=\varnothing$</p>
<h2 id="2-支撑超平面定理"><a href="#2-支撑超平面定理" class="headerlink" title="2. 支撑超平面定理"></a>2. 支撑超平面定理</h2><p><strong>支撑超平面</strong>：对于集合 $C$ 的边界点 $x_0$，支撑超平面为 $\{x|a^Tx=a^Tx_0\}$，其满足 $a\ne0$ 且 $a^Tx\le a^Tx_0,\ \forall x\in C$</p>
<p><strong>支撑超平面定理</strong>：如果 $C$ 为凸集，那么 $C$ 的每个边界点都存在一个支撑超平面</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>凸集分离定理</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 1：Convex Sets</title>
    <url>/2020/02/22/optimization/ch1-cvx-sets/</url>
    <content><![CDATA[<h2 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1. 凸集"></a>1. 凸集</h2><p>区分两种集合的定义（下面的描述并不是严格的数学语言，领会意思就行了）：</p>
<ul>
<li><strong>仿射集(Affine set)</strong>：$x=\theta x_1 + (1-\theta)x_2,\quad \theta\in\mathbb{R}$</li>
<li><strong>凸集(Convex set)</strong>：$x=\theta x_1 + (1-\theta)x_2,\quad \theta\in[0,1]$</li>
</ul>
<p>主要的区别就在于后面 $\theta$ 的取值范围，简单理解就是说仿射集类似<strong>直线</strong>，凸集类似<strong>线段</strong>。</p>
<p>更一般的，仿射集都可以表示为线性方程组的解集，也即 $\{x|Ax=b\}$</p>
<a id="more"></a>
<h2 id="2-常见凸集"><a href="#2-常见凸集" class="headerlink" title="2. 常见凸集"></a>2. 常见凸集</h2><h3 id="2-1-凸包-Convec-hull"><a href="#2-1-凸包-Convec-hull" class="headerlink" title="2.1 凸包(Convec hull)"></a>2.1 凸包(Convec hull)</h3><p>假如集合 $S=\{x_1,…,x_k\}$，则其<strong>凸包</strong>可以表示为</p>
<script type="math/tex; mode=display">
\left\{\sum_{i=1}^k\theta_i x_i \vert \sum\theta_i=1, \theta_i\ge0\right\}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/cvx_hull.PNG" alt="cvx hull"></p>
<h3 id="2-2-超平面-Hyperplanes"><a href="#2-2-超平面-Hyperplanes" class="headerlink" title="2.2 超平面(Hyperplanes)"></a>2.2 超平面(Hyperplanes)</h3><p>类比三维空间中的平面，可以有<strong>超平面</strong>的定义</p>
<script type="math/tex; mode=display">
\left\{x\vert a^Tx=b\right\}(a\ne0)</script><p>其中 $a$ 就是该平面的法向量。</p>
<h3 id="2-3-半空间-Halfspaces"><a href="#2-3-半空间-Halfspaces" class="headerlink" title="2.3 半空间(Halfspaces)"></a>2.3 半空间(Halfspaces)</h3><p>类似的，有<strong>半空间</strong>定义为</p>
<script type="math/tex; mode=display">
\left\{x\vert a^Tx\le b\right\}(a\ne0)</script><h3 id="2-4-多面体-Polyhedra"><a href="#2-4-多面体-Polyhedra" class="headerlink" title="2.4 多面体(Polyhedra)"></a>2.4 多面体(Polyhedra)</h3><p>高维空间中的<strong>多面体</strong>定义为</p>
<script type="math/tex; mode=display">
\left\{x\vert A x \preceq b, \quad C x=d \right\}</script><p>其中 $\preceq$ 表示对每个元素都作比较。实际上就是求很多个半空间以及半平面的交集，与三维空间是类似的。</p>
<h3 id="2-5-欧几里得球与椭球-Euclidean-balls-and-ellipsoids"><a href="#2-5-欧几里得球与椭球-Euclidean-balls-and-ellipsoids" class="headerlink" title="2.5 欧几里得球与椭球(Euclidean balls and ellipsoids)"></a>2.5 欧几里得球与椭球(Euclidean balls and ellipsoids)</h3><p>高维空间中的<strong>欧几里得球</strong>的定义为</p>
<script type="math/tex; mode=display">
B\left(x_{c}, r\right)=\left\{x |\left\|x-x_{c}\right\|_{2} \leq r\right\}=\left\{x_{c}+r u |\|u\|_{2} \leq 1\right\}</script><p><strong>椭球</strong>的定义为</p>
<script type="math/tex; mode=display">
\left\{x |\left(x-x_{c}\right)^{T} P^{-1}\left(x-x_{c}\right) \leq 1\right\} = \left\{x_{c}+A u |\|u\|_{2} \leq 1\right\}</script><p>其中 $P \in \mathbf{S}_{++}^{n}$ (也即 $P$ 为对称正定矩阵)。中间的矩阵 $P$ 的作用就相当于在各个特征向量方向上进行了放缩。</p>
<blockquote>
<p><strong>Remarks</strong>：关于矩阵性质，可以参考我的矩阵论学习笔记，这里复习一个知识点。</p>
<ul>
<li><strong>正规矩阵</strong>的定义为满足 $A^HA=AA^H$ 的矩阵 $A$ 即为正规矩阵，因此<strong>对称矩阵、Hermit矩阵、酉矩阵</strong>都是正规矩阵。而正规矩阵有什么性质呢？正规矩阵可以<strong>对角化</strong>，且<strong>存在一组正交的特征向量</strong>！</li>
<li><strong>正定矩阵</strong>的定义为满足 $x^TAx&gt;0$ 的矩阵 $A$，实际上也就是说矩阵 $A$ 的<strong>特征值均大于 0</strong>！</li>
<li>因此对称正定矩阵的性质有：所有特征向量正交，所有特征值大于 0。</li>
</ul>
</blockquote>
<h3 id="2-6-范数球-norm-balls"><a href="#2-6-范数球-norm-balls" class="headerlink" title="2.6 范数球(norm balls)"></a>2.6 范数球(norm balls)</h3><p><strong>范数</strong> $\Vert\cdot\Vert$ 需要满足以下性质</p>
<ul>
<li>$\Vert x \Vert\ge0;\ \Vert x\Vert=0 \iff x=0$</li>
<li>$|t x|=|t||x|$ for $t \in \mathbf{R}$</li>
<li>$|x+y| \leq|x|+|y|$</li>
</ul>
<p>向量范数如 $\Vert x\Vert_0, \Vert x\Vert_1, \Vert x\Vert_2, \Vert x\Vert_p, \Vert x\Vert_\infty$</p>
<p>矩阵范数如 $\Vert X\Vert_2, \Vert X\Vert_p$</p>
<p><strong>范数球</strong>的定义为</p>
<script type="math/tex; mode=display">
\left\{x |\left\|x-x_{c}\right\| \leq r\right\}</script><h3 id="2-7-凸锥-Convex-cone"><a href="#2-7-凸锥-Convex-cone" class="headerlink" title="2.7 凸锥(Convex cone)"></a>2.7 凸锥(Convex cone)</h3><p>我们先来看看锥的定义</p>
<ul>
<li><strong>锥(cone)</strong>：$x\in C\Rightarrow \theta x\in C, \forall \theta\ge0$</li>
<li><strong>凸锥(Convex cone)</strong>：$x_1,x_2\in C \Rightarrow x=\theta_1 x_1+\theta_2 x_2 \in C,\forall \theta_1,\theta_2\ge0$</li>
</ul>
<p>注意锥一定包含原点 0。锥不一定是凸的，反例如下，这是一个锥，但不是凸锥</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/cone.PNG" alt="cone"></p>
<h3 id="2-8-范数锥-norm-cone"><a href="#2-8-范数锥-norm-cone" class="headerlink" title="2.8 范数锥(norm cone)"></a>2.8 范数锥(norm cone)</h3><p><strong>范数锥</strong>定义如下</p>
<script type="math/tex; mode=display">
\{(x, t) |\|x\| \leq t\}</script><p>也被称为 Ice cream cone。其中欧几里得范数锥被称为<strong>二阶锥</strong>(second-order cone)</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/norm_cone.PNG" alt="norm cone"></p>
<h3 id="2-9-半正定锥-Positive-semidefinite-cone"><a href="#2-9-半正定锥-Positive-semidefinite-cone" class="headerlink" title="2.9 半正定锥(Positive semidefinite cone)"></a>2.9 半正定锥(Positive semidefinite cone)</h3><p>定义几个符号</p>
<ul>
<li>$\mathbf{S}^{n}$ 为 $n$ 阶对称矩阵</li>
<li>$\mathbf{S}_{+}^{n}=\left\{X \in \mathbf{S}^{n} | X \succeq 0\right\}$ 为对称半正定矩阵，为凸锥</li>
<li>$\mathbf{S}_{++}^{n}=\left\{X \in \mathbf{S}^{n} | X \succ 0\right\}$ 为对称正定矩阵</li>
</ul>
<p>例如给定二阶矩阵</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
{x} & {y} \\
{y} & {z}
\end{array}\right] \in \mathrm{S}_{+}^{2}</script><p>其坐标满足如下图</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/semidef_cone.PNG" alt="positive semidefinite cone"></p>
<h2 id="3-保凸变换"><a href="#3-保凸变换" class="headerlink" title="3. 保凸变换"></a>3. 保凸变换</h2><p>上面是一些常见的凸集，对于更复杂的情况，怎么判断是否为凸集呢？</p>
<ul>
<li>根据定义 $x_{1}, x_{2} \in C, \quad 0 \leq \theta \leq 1 \quad \Longrightarrow \quad \theta x_{1}+(1-\theta) x_{2} \in C$</li>
<li>凸集经过<strong>保凸变换</strong>以后仍然是凸集，如<ul>
<li>凸集的交集</li>
<li>仿射变换</li>
<li>投影变换</li>
<li>分式线性映射</li>
</ul>
</li>
</ul>
<h3 id="3-1-凸集的交集"><a href="#3-1-凸集的交集" class="headerlink" title="3.1 凸集的交集"></a>3.1 凸集的交集</h3><p>任意个(可以是无数个)凸集的交集仍然是凸集</p>
<p><strong><em>例子 1</em></strong>：$S=\left\{x \in \mathbf{R}^{m}|| p(t) | \leq 1 \text { for }|t| \leq \pi / 3\right\}$，其中 $p(t)=x_{1} \cos t+x_{2} \cos 2 t+\cdots+x_{m} \cos m t$</p>
<h3 id="3-2-仿射变换"><a href="#3-2-仿射变换" class="headerlink" title="3.2 仿射变换"></a>3.2 仿射变换</h3><p>若映射 $f: \mathbf{R}^{n} \rightarrow \mathbf{R}^{m}$ 是仿射变换</p>
<script type="math/tex; mode=display">
f(x)=A x+b \text { with } A \in \mathbf{R}^{m \times n}, b \in \mathbf{R}^{m}</script><p>则有</p>
<ul>
<li>$S \subseteq \mathbf{R}^{n}$ convex $\Longrightarrow f(S)=\{f(x) | x \in S\}$ convex</li>
<li>$C \subseteq \mathbf{R}^{m}$ convex $\Longrightarrow f^{-1}(C)=\left\{x \in \mathbf{R}^{n} | f(x) \in C\right\}$ convex</li>
</ul>
<p><strong><em>例子 2</em></strong>：双曲锥 $\left\{x | x^{T} P x \leq\left(c^{T} x\right)^{2}, c^{T} x \geq 0\right\}\left(\text { with } P \in \mathbf{S}_{+}^{n}\right)$，因为其可以转化为二阶锥</p>
<p><strong><em>例子 3</em></strong>：$\left\{x | x_{1} A_{1}+\cdots+x_{m} A_{m} \preceq B\right\}$(with $A_i,P\in S^p$)<strong>？？？</strong></p>
<h3 id="3-3-投影变换"><a href="#3-3-投影变换" class="headerlink" title="3.3 投影变换"></a>3.3 投影变换</h3><p><strong>投影函数</strong> $P: \mathbf{R}^{n+1} \rightarrow \mathbf{R}^{n}$</p>
<script type="math/tex; mode=display">
P(x, t)=x / t, \quad \operatorname{dom} P=\{(x, t) | t>0\}</script><p><strong>Proof</strong>：略。应用凸集定义</p>
<h3 id="3-4-分式线性函数"><a href="#3-4-分式线性函数" class="headerlink" title="3.4 分式线性函数"></a>3.4 分式线性函数</h3><p>分式线性映射 $f: \mathbf{R}^{n} \rightarrow \mathbf{R}^{m}$ 为</p>
<script type="math/tex; mode=display">
f(x)=\frac{A x+b}{c^{T} x+d}, \quad \text { dom } f=\left\{x | c^{T} x+d>0\right\}</script><p>其可以看作先仿射变换再投影变换。</p>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>凸集</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化笔记 0：绪论</title>
    <url>/2020/02/22/optimization/ch0-intro/</url>
    <content><![CDATA[<h2 id="0-绪论"><a href="#0-绪论" class="headerlink" title="0. 绪论"></a>0. 绪论</h2><p>首先明确凸优化这门课的主要目的：</p>
<ol>
<li>判断一个问题是否为凸的</li>
<li>将一个问题转化为凸的</li>
<li>求解凸优化问题，给出算法性能</li>
</ol>
<a id="more"></a>
<h2 id="1-优化问题与凸优化"><a href="#1-优化问题与凸优化" class="headerlink" title="1. 优化问题与凸优化"></a>1. 优化问题与凸优化</h2><h3 id="1-1-一般优化问题"><a href="#1-1-一般优化问题" class="headerlink" title="1.1 一般优化问题"></a>1.1 一般优化问题</h3><p>一般优化问题的形式为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{min}\quad & f_{0}(x)\\
\text { s.t. } \quad & f_{i}(x) \leq b_{i}, \quad i=1, \dots, m
\end{aligned}</script><p>其中 $f_0$ 为优化目标，$f_i$ 为约束函数。</p>
<p>一般的优化问题都很难求解，可能无法给出一个解析解，甚至数值解法也不能给出最优解。而在众多复杂的优化问题中，有一些问题则较为容易：</p>
<ul>
<li>最小二乘问题(least-squares problems)：有解析解</li>
<li>线性规划问题(linear programming problems )</li>
<li>凸优化问题(convex optimization problems)：通常没有解析解，但是有有效的数值解法</li>
</ul>
<h3 id="1-2-凸优化问题"><a href="#1-2-凸优化问题" class="headerlink" title="1.2 凸优化问题"></a>1.2 凸优化问题</h3><p>如果目标函数和约束函数都是<strong>凸函数</strong>，则被称为凸优化问题。满足以下条件的函数称为凸函数</p>
<script type="math/tex; mode=display">
f(\alpha x+\beta y) \leq \alpha f(x)+\beta f(y),\quad \alpha+\beta=1,\alpha>0,\beta>0</script><p>凸优化中经常遇到的困难为：</p>
<ul>
<li>难以判断一个问题是否为凸的</li>
<li>将一个问题转化为凸问题需要很多的技巧(tricks)</li>
</ul>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
  </entry>
  <entry>
    <title>PID 控制器</title>
    <url>/2020/02/21/system/pid/</url>
    <content><![CDATA[<p>在过程控制中，按偏差的比例（P）、积分（I）和微分（D）进行控制的PID控制器（亦称PID调节器）是应用最为广泛的一种自动控制器。它具有原理简单，易于实现，适用面广，控制参数相互独立，参数的选定比较简单等优点.</p>
<a id="more"></a>
<h2 id="1-基本原理"><a href="#1-基本原理" class="headerlink" title="1. 基本原理"></a>1. 基本原理</h2><p><strong>PID控制器</strong>（比例-积分-微分控制器），由比例单元（P）、积分单元（I）和微分单元（D）组成。可以透过调整这三个单元的增益 $K_p$，$K_i$ 和 $K_d$ 来调定其特性。PID控制器主要适用于基本上线性，且动态特性不随时间变化的系统。</p>
<p>PID控制器的比例单元(P)、积分单元(I)和微分单元(D)分别对应目前误差、过去累计误差及未来误差。若是不知道受控系统的特性，一般认为PID控制器是最适用的控制器。</p>
<h2 id="2-控制算法"><a href="#2-控制算法" class="headerlink" title="2. 控制算法"></a>2. 控制算法</h2><p>若定义 $u(t)$ 为控制输出，PID算法可以用下式表示</p>
<script type="math/tex; mode=display">
\mathrm{u}(t)=\mathrm{MV}(t)=K_{p} e(t)+K_{i} \int_{0}^{t} e(\tau) d \tau+K_{d} \frac{d}{d t} e(t)</script><p>其中 $e(t)$ 表示误差。</p>
<h3 id="2-1-比例控件"><a href="#2-1-比例控件" class="headerlink" title="2.1 比例控件"></a>2.1 比例控件</h3><p>比例控制考虑当前误差，误差值和一个正值的常数 $K_p$（表示比例）相乘。$K_p$ 只是在控制器的输出和系统的误差成比例的时候成立。</p>
<script type="math/tex; mode=display">
P_{\text{out}} = K_p e(t)</script><p>若比例增益大，在相同误差量下，会有较大的输出，但若比例增益太大，会使系统不稳定。相反的，若比例增益小，若在相同误差量下，其输出较小，因此控制器会较不敏感的。若比例增益太小，当有干扰出现时，其控制信号可能不够大，无法修正干扰的影响。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/PID_varyingP.jpg" alt="Change K_p"></p>
<p>上图即为改变 $K_p$ 对应的控制器。</p>
<h3 id="2-2-积分控件"><a href="#2-2-积分控件" class="headerlink" title="2.2 积分控件"></a>2.2 积分控件</h3><p>积分控制考虑过去误差，将误差值过去一段时间和（误差和）乘以一个正值的常数 $K_i$。</p>
<script type="math/tex; mode=display">
I_{\mathrm{out}}=K_{i} \int_{0}^{t} e(\tau) d \tau</script><p>积分控制会<strong>加速</strong>系统趋近设定值的过程，并且消除纯比例控制器会出现的稳态误差。积分增益越大，趋近设定值的速度越快，不过因为积分控制会累计过去所有的误差，可能会使回授值出现<strong>过冲</strong>的情形。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/Change_with_Ki.png" alt="Change K_i"></p>
<p>上图即为改变 $K_i$ 对应的控制器。</p>
<h3 id="2-3-微分控件"><a href="#2-3-微分控件" class="headerlink" title="2.3 微分控件"></a>2.3 微分控件</h3><p>微分控制考虑将来误差，计算误差的一阶导，并和一个正值的常数 $K_d$ 相乘。这个导数的控制会对系统的改变作出反应。导数的结果越大，那么控制系统就对输出结果作出更快速的反应。这个 $K_d$ 参数也是PID被称为可预测的控制器的原因。$K_d$ 参数对减少控制器短期的改变很有帮助。一些实际中的速度缓慢的系统可以不需要 $K_d$ 参数。</p>
<script type="math/tex; mode=display">
D_{\mathrm{out}}=K_{d} \frac{d}{d t} e(t)</script><p>微分控制可以提升整定时间及系统<strong>稳定性</strong>。不过因为纯微分器不是因果系统，因此在PID系统实现时，一般会为微分控制加上一个低通滤波器以限制高频增益及噪声。实际上较少用到微分控制，估计PID控制器中只有约20%有用到微分控制。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/Change_with_Kd.png" alt="Change K_d"></p>
<p>上图即为改变 $K_d$ 对应的控制器。</p>
<h2 id="3-位置式和增量式PID算法"><a href="#3-位置式和增量式PID算法" class="headerlink" title="3. 位置式和增量式PID算法"></a>3. 位置式和增量式PID算法</h2><p>PID 一般有两种：<strong>位置式 PID</strong> 和<strong>增量式 PID</strong>。在小车里一般用增量式，因为位置式 PID 的输出与过去的所有状态有关，计算时要对历史上每一次的控制误差进行累加，这个计算量非常大，而且没有必要。增量式则子需要计算每次控制量的变化量。</p>
<p>将 PID 公式离散化，可以推导出位置式 PID 公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
u(k)&=K_p\left[e_{k}+\frac{1}{T_i} \sum_{j=0}^{k} e_{j}+T_d \frac{e_{k}-e_{k-1}}{T}\right] \\
&=K_p \cdot e_{k}+\frac{K_p}{T_i} \sum_{j=0}^{k} e_{j}+K_p \cdot T_d \frac{e_{k}-e_{k-1}}{T} \\
&=A e_{k}+B\left(\sum_{j=0}^{k-1} e_{j}+e_{k}\right)+C\left(e_{k}-e_{k-1}\right)
\end{aligned}</script><p>其中把 $K_i,K_d$ 表示为 $K_p/T_i,K_p\cdot T_d$。</p>
<p>在此基础上可以推导出增量式 PID 公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta u_{k}&=u_{k}-u_{k-1}=K_p\left[e_{k}-e_{k-1}+\frac{T}{T_i} e_{k}+T_d \frac{e_{k}-2 e_{k-1}+e_{k-2}}{T}\right] \\
&=K_p\left(1+\frac{T}{T_i}+\frac{T_d}{T}\right) e_{k}-K_p\left(1+\frac{2 T_d}{T}\right) e_{k-1}+K_p \frac{T_d}{T} e_{k-2} \\
&=A e_{k}+B e_{k-1}+C e_{k-2}
\end{aligned}</script><p>其中误差 $e_k = y_0(kT) - y(kT)$，$y_0$ 表示目标值，$y(kT)$ 表示第 $k$ 个时刻的值。</p>
]]></content>
      <categories>
        <category>Control</category>
      </categories>
      <tags>
        <tag>PID</tag>
      </tags>
  </entry>
  <entry>
    <title>LaSalle&#39;s invariance principle 拉萨尔不变性原理</title>
    <url>/2020/02/20/system/lasalle/</url>
    <content><![CDATA[<p>拉萨尔不变原理是李亚普诺夫第二方法的推广。这种观察给出了李亚普诺夫理论的统一认识，且极大地推广了李亚普诺夫第二方法，现在人们称这一推广为拉萨尔不变原理。</p>
<a id="more"></a>
<h2 id="1-系统模型"><a href="#1-系统模型" class="headerlink" title="1. 系统模型"></a>1. 系统模型</h2><p>考虑一个控制系统</p>
<script type="math/tex; mode=display">
\dot{x}(t)=f(x(t))</script><p>其中 $f(0)=0$。</p>
<h2 id="2-基本定义"><a href="#2-基本定义" class="headerlink" title="2. 基本定义"></a>2. 基本定义</h2><p><strong>正极限点(positive limit point)</strong>：p 被称为 $x(t)$ 的正极限点，如果存在一个时间序列 $\{t_n\}$，有 $n\to\infty$ 时 $t_n\to\infty$，且使得 $x(t_n)\to\infty$ 随着 $n\to\infty$。</p>
<p><strong>正极限集(positive limit set)</strong>：$x(t)$ 的所有正极限点的集合即为正极限集。</p>
<blockquote>
<p><strong>Remarks</strong>：这里举个例子，序列 $x(n)=1,-1,1,-1,…$，那么取奇数项时极限为 1，偶数项时极限为 -1.但是对于完整的序列 $x(n)$ 则极限不存在，而 $x(n)$ 的正极限集则为 $\{1,-1\}$。</p>
<p>为什么这里会引入<strong>集合</strong>呢？因为控制系统中最终的稳定状态可能不是一个孤立的点，而是在很多个状态之间循环转换，比如一个单位圆。</p>
</blockquote>
<p><strong>不变集(invariant set)</strong>：集合 $M$ 是关于系统 (1) 的不变集，如果有 $x(0)\in M \Rightarrow x(t)\in M, \forall t\in \mathbb{R}$。如果有 $x(0)\in M \Rightarrow x(t)\in M, \forall t\ge0$ 则称为正不变集(positive invariant set)。</p>
<h2 id="3-拉萨尔不变性原理"><a href="#3-拉萨尔不变性原理" class="headerlink" title="3. 拉萨尔不变性原理"></a>3. 拉萨尔不变性原理</h2><p><strong>LaSalle’ Theorem</strong>：令 $\Omega\in D$ 是一个紧致集，且是关于系统 $\dot{x}(t)=f(x(t))$ 的不变集。令 $V:D\to\mathbb{R}$ 是一个连续函数，且满足 $\dot{V}(x)\le0\ \ in\ \ \Omega$。令 $M$ 为 $\Omega$ 中所有满足 $\dot{V}(x)=0$ 的点的集合，令 $E$ 为 $M$ 中的最大不变集，那么从 $\Omega$ 中出发的所有解都将趋于 $E$ 随着 $t\to\infty$。</p>
<blockquote>
<p><strong>Remarks</strong>：这里的 $M$ 和 $E$ 有什么不同吗？二者不等价吗？不一定等价！因为 $\Omega$ 本身是一个不变集，而 $M$ 又是他的一个子集，如下图所示，那么任意一个起始于 $M$ 的轨迹都有可能跑出 $M$ 而进入 $\Omega\backslash M$，因此 $M$ 并不是一个不变集。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/lip.png" alt=""></p>
</blockquote>
]]></content>
      <categories>
        <category>Control</category>
      </categories>
      <tags>
        <tag>Lyapunov</tag>
        <tag>系统稳定性</tag>
      </tags>
  </entry>
  <entry>
    <title>Second-Order Cone Programming(SOCP) 二次锥规划</title>
    <url>/2020/02/17/optimization/socp/</url>
    <content><![CDATA[<p>二次锥规划是凸优化的一部分，可以应用<strong>内点法</strong>快速求解，相比于<strong>半正定规划</strong>更有效。</p>
<a id="more"></a>
<h2 id="1-二阶锥"><a href="#1-二阶锥" class="headerlink" title="1. 二阶锥"></a>1. 二阶锥</h2><h3 id="1-1-二阶锥定义"><a href="#1-1-二阶锥定义" class="headerlink" title="1.1 二阶锥定义"></a>1.1 二阶锥定义</h3><p>在此之前，先给出<strong>二阶锥</strong>的定义。</p>
<p>在 $k$ 维空间中二阶锥 (Second-order cone) 的定义为</p>
<script type="math/tex; mode=display">
\mathcal{C}_{k}=\left\{\left[\begin{array}{l}
{u} \\
{t}
\end{array}\right] | u \in \mathbb{R}^{k-1}, t \in \mathbb{R},\|u\| \leq t\right\} \notag</script><p>其也被称为 quadratic，ice-cream，Lorentz cone。</p>
<h3 id="1-2-二阶锥约束"><a href="#1-2-二阶锥约束" class="headerlink" title="1.2 二阶锥约束"></a>1.2 二阶锥约束</h3><p>在此基础上，<strong>二阶锥约束</strong>即为</p>
<script type="math/tex; mode=display">
\|A x+b\| \leq c^{T} x+d \Longleftrightarrow\left[\begin{array}{c}
{A} \\
{c^{T}}
\end{array}\right] x+\left[\begin{array}{l}
{b} \\
{d}
\end{array}\right] \in \mathcal{C}_{k}</script><p>其中 $x\in \mathbb{R}^{n}, A\in\mathbb{R}^{(k-1)\times n}, b\in\mathbb{R}^{k-1},c\in\mathbb{R}^{n},\mathbb{R}$。实际上是对 $x$ 进行了仿射变换，由于仿射变换不改变凹凸性，因此二阶锥也是凸锥。</p>
<h2 id="2-优化问题建模"><a href="#2-优化问题建模" class="headerlink" title="2. 优化问题建模"></a>2. 优化问题建模</h2><p>优化目标如下，其中 $f \in \mathbb{R}^{n}, A_{i} \in \mathbb{R}^{n_{i} \times n}, b_{i} \in \mathbb{R}^{n_{i}}, c_{i} \in \mathbb{R}^{n}, d_{i} \in \mathbb{R}, F \in \mathbb{R}^{p \times n},$ and $g \in \mathbb{R}^{p}, x \in \mathbb{R}^{n}$</p>
<script type="math/tex; mode=display">
\begin{align}
\text{minize}\quad& f^{T} x \notag\\
\text{subject to}\quad& {\left\|A_{i} x+b_{i}\right\|_{2} \leq c_{i}^{T} x+d_{i}, \quad i=1, \ldots, m} \notag\\
&{F x=g}\notag
\end{align}\notag</script><p>上述问题被称为<strong>二次锥规划</strong>是因为其约束，要求仿射函数 $(Ax+b,c^T x+d)$ 为 $\mathbb{R}^{k+1}$ 空间中的二阶锥。</p>
<h2 id="3-类似问题转化"><a href="#3-类似问题转化" class="headerlink" title="3. 类似问题转化"></a>3. 类似问题转化</h2><p>一些其他优化问题也可以转化为 SOCP，例如</p>
<h3 id="3-1-二次规划"><a href="#3-1-二次规划" class="headerlink" title="3.1 二次规划"></a>3.1 二次规划</h3><p>考虑二次约束</p>
<script type="math/tex; mode=display">
x^{T} A^{T} A x+b^{T} x+c \leq 0  \notag</script><p>可以等价转化为 SOC 约束</p>
<script type="math/tex; mode=display">
\left\|\begin{array}{c}\left(1+b^{T} x+c\right) / 2 \\ Ax\end{array}\right\|_{2} 
\leq\left(1-b^{T} x-c\right) / 2 \notag</script><h3 id="3-2-随机线性规划"><a href="#3-2-随机线性规划" class="headerlink" title="3.2 随机线性规划"></a>3.2 随机线性规划</h3><p>问题模型为</p>
<script type="math/tex; mode=display">
\begin{align}
\text{minize}\quad& c^{T} x \notag\\
\text{subject to}\quad& \mathbb{P}\left(a_{i}^{T} x \leq b_{i}\right) \geq p, \quad i=1, \ldots, m \notag
\end{align}\notag</script><p>问题转化可参考<a href="https://en.wikipedia.org/wiki/Second-order_cone_programming" target="_blank" rel="noopener">维基百科</a></p>
<h2 id="4-问题求解"><a href="#4-问题求解" class="headerlink" title="4. 问题求解"></a>4. 问题求解</h2><p>二阶锥规划可以应用<strong>内点法</strong>快速求解，且比<strong>半正定规划</strong>(semidefinite programming)更有效。</p>
<p>Matlab 有专门的凸优化工具包，<a href="http://cvxr.com/cvx/" target="_blank" rel="noopener">下载地址</a>在这里，安装教程在官网上有。使用方法如下，只需要修改优化目标和约束条件即可</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">m = <span class="number">20</span>; n = <span class="number">10</span>; p = <span class="number">4</span>;</span><br><span class="line">A = <span class="built_in">randn</span>(m,n); b = <span class="built_in">randn</span>(m,<span class="number">1</span>);</span><br><span class="line">C = <span class="built_in">randn</span>(p,n); d = <span class="built_in">randn</span>(p,<span class="number">1</span>); e = <span class="built_in">rand</span>;</span><br><span class="line">cvx_begin</span><br><span class="line">    variable x(n)</span><br><span class="line">    minimize( norm( A * x - b, <span class="number">2</span> ) )</span><br><span class="line">    subject to</span><br><span class="line">        C * x == d</span><br><span class="line">        norm( x, Inf ) &lt;= e</span><br><span class="line">cvx_end</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Convex Optimization</category>
      </categories>
      <tags>
        <tag>SOCP</tag>
      </tags>
  </entry>
  <entry>
    <title>笔记本选购指南</title>
    <url>/2020/02/09/tools/laptop/</url>
    <content><![CDATA[<p>根据网上的资料总结了一些笔记本选购过程中需要注意的一些参数。</p>
<a id="more"></a>
<h2 id="1-确定需求类型"><a href="#1-确定需求类型" class="headerlink" title="1. 确定需求类型"></a>1. 确定需求类型</h2><p>笔记本大致可以分为三类：</p>
<ul>
<li><strong>游戏本</strong>：性能高、厚重。</li>
<li><strong>轻薄本</strong>：性能略低，轻薄，适合平时主要需求为办公的人群。</li>
<li><strong>全能本</strong>：兼顾重量与性能，但是为了降低重量可能导致散热效果等不好。</li>
</ul>
<h2 id="2-主要关注参数"><a href="#2-主要关注参数" class="headerlink" title="2. 主要关注参数"></a>2. 主要关注参数</h2><ul>
<li><p><strong>处理器</strong></p>
<ul>
<li><strong>核数</strong>：</li>
<li><strong>主频</strong>：</li>
</ul>
</li>
<li><p><strong>内存</strong></p>
</li>
<li><p><strong>硬盘</strong></p>
<ul>
<li><strong>机械硬盘</strong>（HDD）：<strong>容量大</strong>，读写速度慢，有噪音，抗震能力弱，功耗大，但是<strong>相对便宜</strong>。</li>
<li><strong>固态硬盘</strong>（SSD）：容量小，<strong>读写速度快</strong>，<strong>无噪音</strong>，<strong>抗震能力强</strong>，<strong>功耗小</strong>，但是相对要贵。</li>
<li><strong>混合硬盘</strong></li>
</ul>
</li>
<li><p><strong>显卡</strong></p>
<ul>
<li><strong>集成显卡</strong>：</li>
<li><strong>独立显卡</strong>：</li>
</ul>
</li>
<li><p><strong>屏幕</strong></p>
<ul>
<li><strong>类型</strong>：主要是TN屏与IPS屏。能不选TN屏就<strong>不选**</strong>TN<strong>**屏</strong>。IPS的优点：亮度高，对比度高，视角广，缺点：响应时间长（目前可以控制在5ms）。TN屏优点：响应时间短（一般都2ms），缺点：亮度中，对比度中，视角小</li>
<li><strong>分辨率</strong>：分辨率越高，显示效果就越精细和细腻。大部分至少为1080P（1920*1080）</li>
<li><strong>色域</strong>：屏幕所能显示的色彩范围区域。色域越高，屏幕呈现出的色彩越丰富、艳丽。平面设计、服装设计等，<strong>对色彩有要求</strong>的同学需要特别注意，一定要选择<strong>高色域屏幕（72%NTSC/100%sRGB以上）</strong>。</li>
<li><strong>刷新率</strong>：刷新率越高，所显示的图象（画面）稳定性就越好。常见的屏幕多为60Hz。</li>
</ul>
</li>
<li><p><strong>电池</strong></p>
</li>
<li><p><strong>笔记本接口</strong></p>
<ul>
<li>外接设备，如USB、Type-C、网线、HDMI等</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>电子产品</category>
      </categories>
      <tags>
        <tag>电脑</tag>
        <tag>电子产品</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分析学习笔记</title>
    <url>/2020/02/03/linear-algebra/matrix/</url>
    <content><![CDATA[<p>在网上自学矩阵分析的一些笔记，主要是总结一些结论性的东西，并没有太多证明。对于非数学专业的学生，笔者认为抛开证明的细节，从更加具象的角度理解矩阵可能会有更清晰的理解。</p>
<p>未完待续，更新中 …</p>
<p>参考资料：<a href="https://zhuanlan.zhihu.com/matrix-learning" target="_blank" rel="noopener">知乎专栏</a></p>
<a id="more"></a>
<hr>
<h2 id="1-线性代数基础——空间"><a href="#1-线性代数基础——空间" class="headerlink" title="1. 线性代数基础——空间"></a>1. 线性代数基础——空间</h2><ul>
<li><p>几个基本的概念</p>
<ul>
<li><p><strong>数域</strong>：对<strong>加减乘除</strong>四则基本<strong>运算封闭</strong>的<strong>数集</strong></p>
<ul>
<li>注意：首先<strong>数域</strong>的概念针对的是<strong>数集</strong>，不是向量也不是矩阵；其次要求对四则基本运算封闭。</li>
</ul>
</li>
<li><p><strong>线性空间</strong>：需满足以下条件</p>
<script type="math/tex; mode=display">
\begin{alignat}{1}
&1)\ \alpha+\beta=\beta+\alpha     &5)\ 1 a=\alpha\notag\\
&2)\ (\alpha+\beta)+\gamma=\alpha+(\beta+\gamma)   &6)\ k(l \alpha)=(k l) \alpha\notag\\
&3)\ \exists 0 \in V, \forall \alpha \in V, 有 \alpha+0=\alpha &7)\ (k+l) \alpha=k \alpha+l \alpha\notag\\
&4)\ \forall \alpha \in V, \exists \beta \in V, s.t.\  \alpha+\beta=0 \qquad &8)\ k(\alpha+\beta)=k \alpha+l \beta\notag\\
\end{alignat}\notag</script></li>
<li><p><strong>子空间</strong>：</p>
</li>
<li><p>空间的<strong>维数</strong>：基的个数</p>
</li>
<li><p><strong>平凡子空间</strong>：V 空间的子空间只有 0 空间和 V 空间本身</p>
</li>
<li><p><strong>非平凡子空间</strong>：除了平凡子空间，其他所有子空间</p>
</li>
<li><p>子空间的<strong>直和</strong>：$V_1 \cap V_2=\{0\}$ 时，直和可定义为 $V_1 \bigoplus V_2$，主要是为了保证<strong>分解的唯一性</strong>。可以推广到多个子空间 $V_i (\sum_{j\ne i}V_j) = \{0\}$</p>
<ul>
<li>注：$V_1,V_2$ 相互可能不是正交的，比如二维平面中不正交的两个基</li>
</ul>
</li>
<li><p><strong>酉空间</strong>：欧几里得空间推广到<strong>复数域</strong></p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-投影"><a href="#2-投影" class="headerlink" title="2. 投影"></a>2. 投影</h2><ul>
<li><strong>变换</strong>：线性空间到自身的映射 $T:V(C)\to V(C)$</li>
<li><strong>线性变换</strong>：<ul>
<li>$T(\alpha+\beta) = T(\alpha)+T(\beta)$</li>
<li>$T(k\alpha) = kT(\alpha)$</li>
</ul>
</li>
<li><strong>投影</strong>：$T$ 是 $V(C)$ 上的投影， $\iff T^2=T$</li>
</ul>
<blockquote>
<p><strong>定理 1</strong>：设 $T$ 是 $V(C)$ 上的投影，则 $V(C) = R(T)\bigoplus N(T)$</p>
<p><strong>定理 2</strong>：设 $V(C) = V_1\bigoplus V_2$，则存在投影 $T$ 使得 $R(T)=V_1, N(T)=V_2$</p>
<p><strong>Remark</strong>：根据投影的定义 $T^2=T$，可以形象理解为<strong>降维</strong>操作，也即投影过程不可逆，投影一次后即进入<strong>值域</strong> $R(T)$，也即是 $V(C)$ 的一个低维子空间。</p>
</blockquote>
<ul>
<li><p><strong>投影矩阵</strong>：投影 $T$ 为线性变换，可以用矩阵 $A$ 表示<br><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/proj.jpg" alt="线性变换"></p>
</li>
<li><p><strong>幂等矩阵</strong>：满足 $A^2=A$，有如下性质</p>
<ul>
<li>$A^H$ 与 $(E-A)$ 也是幂等矩阵</li>
<li>$A$ 的特征值只有 0 和 1，且可以对角化</li>
<li>$rank(A)=tr(A)$</li>
<li>$A(E-A)=(E-A)A$</li>
<li>$Aa = a, \iff a\in R(A)$</li>
<li>$N(A)=R(E-A), R(A)=N(E-A)$</li>
</ul>
<blockquote>
<p>上面的性质均可由<strong>幂等矩阵</strong>的性质导出</p>
</blockquote>
</li>
<li><p><strong>正交投影</strong>：$\iff R^{\perp}(T) = N(T) \iff A^H=A$</p>
</li>
</ul>
<blockquote>
<p><strong>Remark</strong>：</p>
<ul>
<li>实际上对于正交投影 $A$，可以写成以下形式</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/decom.jpg" alt="正交投影分解"></p>
<ul>
<li>是否存在<strong>非正交投影</strong>呢？非正交投影又是什么形式呢？<br>只需要将中间的对角阵换成Jordan标准型的形式？</li>
</ul>
</blockquote>
<hr>
<h2 id="3-Jordan标准型"><a href="#3-Jordan标准型" class="headerlink" title="3. Jordan标准型"></a>3. Jordan标准型</h2><p>注：此部分是矩阵论的基本定理之一，非常重要！！！</p>
<blockquote>
<p><strong>定理 1</strong>：任意 n 阶矩阵 $A$，一定存在 n 阶<strong>可逆矩阵</strong> P 使得</p>
<script type="math/tex; mode=display">
P^{-1} A P=\left(\begin{array}{cccc}
{J_{1}} & {} & {} & {} \\
{} & {J_{2}} & {} & {} \\
{} & {} & {\ddots} & {} \\
{} & {} & {} & {J_{k}}
\end{array}\right)=J \notag</script><p>其中 $J_i$ 为 Jordan 块。有以下几个结论</p>
<ol>
<li>Jordan 块的个数是<strong>线性无关特征向量的个数</strong></li>
<li>矩阵可<strong>对角化</strong>当且仅当 $k=n$</li>
<li>对于某个特征值，Jordan 块个数为<strong>几何重数</strong>，所有 Jordan 块的阶数之和为<strong>代数重数</strong>（特征值多项式根的阶数即为代数重数，永远有几何重数不大于代数重数）</li>
<li>特征值的几何重数不大于代数重数</li>
<li>矩阵不同特征值对应的<strong>特征向量线性无关</strong></li>
</ol>
</blockquote>
<hr>
<h2 id="4-初等矩阵与酉矩阵"><a href="#4-初等矩阵与酉矩阵" class="headerlink" title="4. 初等矩阵与酉矩阵"></a>4. 初等矩阵与酉矩阵</h2><h3 id="4-1-初等变换矩阵"><a href="#4-1-初等变换矩阵" class="headerlink" title="4.1 初等变换矩阵"></a>4.1 初等变换矩阵</h3><blockquote>
<p><strong>定义</strong>：设 $\boldsymbol{u,v}\in \mathbb{C}^n,\sigma\in \mathbb{C}$，则称 $E(\boldsymbol{u,v},\sigma)=E-\sigma\boldsymbol{uv}^H$ 为<strong>初等变换矩阵</strong></p>
</blockquote>
<ul>
<li><p><strong>初等变换</strong>矩阵性质</p>
<ul>
<li>特征向量<ul>
<li>若 $\boldsymbol{u\in v^{\perp}}$，设 $\boldsymbol{u_1,…,u_{n-1}}$ 是 $v^\perp$ 的一组基，则 $E(\boldsymbol{u,v},\sigma)$ 的一组<strong>线性无关</strong>的特征向量为 $\boldsymbol{u_1,…,u_{n-1}}$</li>
<li>若 $\boldsymbol{u\notin v^{\perp}}$，设 $\boldsymbol{u_1,…,u_{n-1}}$ 是 $v^\perp$ 的一组基，则 $E(\boldsymbol{u,v},\sigma)$ 的一组<strong>线性无关</strong>的特征向量为 $\boldsymbol{u,u_1,…,u_{n-1}}$</li>
</ul>
</li>
<li>特征值 $\lambda(E(\boldsymbol{u,v},\sigma))=\{1,…,1,1-\sigma v^H u\}$</li>
<li>行列式 $det(E(\boldsymbol{u,v},\sigma))=1-\sigma v^H u$</li>
<li>逆矩阵 $E(u, v, \sigma)^{-1}=E\left(u, v, \frac{\sigma}{\sigma v^{H} u-1}\right),\left(1-\sigma v^{H} u \neq 0\right)$</li>
<li>非零向量 $\boldsymbol{a,b}\in\mathbb{C}^n$，存在 $\boldsymbol{u,v},\sigma$ 使得 $E(u, v, \sigma) a=b,\left(\sigma u=\frac{a-b}{v^{H} a}\right)$</li>
</ul>
<blockquote>
<p><strong>Remarks</strong></p>
<ol>
<li>前两个性质可以根据 $u,v$ 的垂直关系直观想象。当 $u\perp v$ 时，此时 $E$ 对于特征值 $1$ 的代数重数为 $n$，而几何重数为 $n-1$（注意此时出现了代数重数大于几何重数的情况！）；否则，$E$ 对于特征值 $1$ 的代数重数和几何重数为 $n-1$，且有另一个特征值 $1-\sigma v^H u$</li>
</ol>
</blockquote>
</li>
<li><p>所有初等变换可以用上述定义表示</p>
<ul>
<li>置换 ${E_{i j}=E-\left(e_{i}-e_{j}\right)\left(e_{i}-e_{j}\right)^{T}=E\left(e_{i}-e_{j}, e_{i}-e_{j}, 1\right)}$</li>
<li>相消 ${E_{i j}(k)=E+k e_{j} e_{i}^{T}=E\left(e_{j}, e_{i},-k\right)}$</li>
<li>数乘 ${E_{i}(k)=E-(1-k) e_{i} e_{i}^{T}=E\left(e_{i}, e_{i}, 1-k\right)}$</li>
</ul>
</li>
</ul>
<h3 id="4-2-初等酉矩阵"><a href="#4-2-初等酉矩阵" class="headerlink" title="4.2 初等酉矩阵"></a>4.2 初等酉矩阵</h3><blockquote>
<p><strong>定义</strong>：设 $\boldsymbol{u}\in \mathbb{C}^n$ 且 $u^H u =1$，则称 $H(U)=E(\boldsymbol{u,U},2)=E-2\boldsymbol{uu}^H$ 为<strong>初等酉矩阵</strong>，或者<strong>Householder矩阵</strong></p>
</blockquote>
<ul>
<li><strong>Householder</strong>变换性质<ul>
<li>$H^H=H=H^{-1}$</li>
<li>$H(\boldsymbol{u})(\boldsymbol{a}+r\boldsymbol{u})=\boldsymbol{a}-r\boldsymbol{u}, \forall a\in v^\perp, r\in\mathbb{C}$（镜像变换）</li>
<li>范数不变性：$||Hx||=||x||$</li>
<li>保持随机向量的协方差</li>
<li>可用于数值算法构造正交基</li>
</ul>
</li>
</ul>
<h3 id="4-3-酉变换"><a href="#4-3-酉变换" class="headerlink" title="4.3 酉变换"></a>4.3 酉变换</h3><ul>
<li><strong>酉变换与酉矩阵</strong><ol>
<li>保持<strong>内积</strong>不变</li>
<li>保持长度不变</li>
<li>保持夹角不变</li>
<li>保持形状不变</li>
</ol>
</li>
<li>内积的定义，比如连续区间中对连续函数的定义</li>
</ul>
<hr>
<h2 id="5-欧氏空间中的度量（？）"><a href="#5-欧氏空间中的度量（？）" class="headerlink" title="5. 欧氏空间中的度量（？）"></a>5. 欧氏空间中的度量（？）</h2><ul>
<li><p><strong>内积</strong>：满足 4 条性质</p>
<ol>
<li>$(x,x)\ge0,且(x,x)=0\iff x=0$</li>
<li>$(x,y)=\overline{(y,x)},\forall x,y\in V(P)$</li>
<li>$(\lambda x,y)=\bar{\lambda}(x,y),\forall \lambda\in P,\forall x,y\in V(P)$</li>
<li>$(x+y,z)=(x,z)+(y,z),\forall x,y,z\in V(P)$</li>
</ol>
</li>
<li><p><strong>线性流形</strong>：$P=r_{0}+V_{1}=\left\{r_{0}+\alpha | \alpha \in V_{1}\right\}$</p>
<ul>
<li>实际上就是将子空间进行平移</li>
</ul>
</li>
<li><p>n 维空间中的<strong>体积</strong></p>
<ol>
<li>$V(\alpha_1)=||\alpha_1||$</li>
<li>$V\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}\right)=V\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n-1}\right) \bullet h_{n}$，其中 $h_n$ 是 $\alpha_n$ 到 $L(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n-1})$ 的距离</li>
</ol>
</li>
<li><p>Gram 行列式</p>
<script type="math/tex; mode=display">
G\left(\alpha_{1}, \cdots, \alpha_{k}\right)=\left| \begin{array}{cccc}
{\left(\alpha_{1}, \alpha_{1}\right)} & {\left(\alpha_{1}, \alpha_{2}\right)} & {\cdots} & {\left(\alpha_{1}, \alpha_{k}\right)} \\
{\left(\alpha_{2}, \alpha_{1}\right)} & {\left(\alpha_{2}, \alpha_{2}\right)} & {\cdots} & {\left(\alpha_{2}, \alpha_{k}\right)} \\
{\cdots} & {\cdots} & {\cdots} & {\cdots} \\
{\left(\alpha_{k}, \alpha_{1}\right)} & {\left(\alpha_{k}, \alpha_{2}\right)} & {\cdots} & {\left(\alpha_{k}, \alpha_{k}\right)}
\end{array}\right|\notag</script></li>
<li><p>将线性无关向量组 $\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}$ 正交化之后，Gram 行列式不变，即 $G\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right)=G\left(\beta_{1}, \beta_{2}, \cdots, \beta_{k}\right)$</p>
</li>
<li><p>体积 $V\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}\right)=\sqrt{G\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{n}\right)}$</p>
</li>
<li><p>定理 1：设 $\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}$ 是 $V_1$ 的一组基，向量 $\alpha$ 到流形 $P=\alpha_0+V_1$ 的距离为 $d^{2}=\frac{G\left(\alpha_{1}, \cdots, \alpha_{k}, \alpha-\alpha_{0}\right)}{G\left(\alpha_{1}, \cdots, \alpha_{k},\right)}$</p>
</li>
<li><p>定理 2：线性流形 $P_1=\alpha_0+V_1$ 和 $P_2=\alpha_0+V_1$ 之间的距离等于 $\alpha_1-\alpha_2$ 关于线性子空间 $V=V_1+V_2$ 的正交分量长度</p>
</li>
</ul>
<hr>
<h2 id="6-Kronecker积"><a href="#6-Kronecker积" class="headerlink" title="6. Kronecker积"></a>6. Kronecker积</h2><ul>
<li>性质<ul>
<li>$E_m\bigotimes E_n = E_{mn}$</li>
<li></li>
</ul>
</li>
</ul>
<hr>
<h2 id="7-范数"><a href="#7-范数" class="headerlink" title="7. 范数"></a>7. 范数</h2><h3 id="7-1-向量范数"><a href="#7-1-向量范数" class="headerlink" title="7.1 向量范数"></a>7.1 向量范数</h3><ul>
<li>范数：刻画向量大小的度量，需要满足以下三条性质<ol>
<li>正定性：$||x||\ge0,且||x||=0\iff x=0$</li>
<li>齐次性：$||\lambda x||=|\lambda|\cdot ||x||,\lambda\in R,x\in C^n$</li>
<li>三角不等式：$||x+y||\le ||x||+||y||,\forall x,y\in C^n$</li>
</ol>
</li>
<li>范数与内积的关系是什么？</li>
<li>导出性质<ul>
<li>$||0||=0$</li>
<li>$x\ne0时,||\frac{1}{||x||}x||=1$</li>
<li>$||-x||=||x||,\forall x\in C^n$</li>
<li>$\vert \Vert x\Vert-\Vert y\Vert \vert \le \Vert x-y \Vert$</li>
</ul>
</li>
<li>常用范数<ul>
<li>1范数：$|x|_{1}=\sum_{i=1}^{n}\left|x_{i}\right|$</li>
<li>2范数：$|x|_{2}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{2}\right)^{1 / 2}$</li>
<li>$\infty$范数：$|x|_{\infty}=\max _{1 \leq i \leq n}\left|x_{i}\right|$</li>
<li>p范数(Holder范数)：$|x|_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{1 / p} \quad 1 \leq p&lt;\infty$<ul>
<li>p可取<strong>正整数</strong></li>
<li>可验证满足三角不等式，需要用到Young不等式和Holder不等式</li>
</ul>
</li>
</ul>
</li>
<li>向量序列的<strong>收敛性</strong></li>
<li>向量范数的<strong>等价性</strong><ul>
<li><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/norm%20equality.jpg" alt="范数等价性"><br>等价性表示不同范数的量级是相同的，只差一个系数</li>
<li><strong>定理</strong>：$V(P)$ 上的任意两个向量范数均等价</li>
<li><strong>范数等价保证了向量序列的收敛性与范数选取无关</strong>。无穷范数收敛，其他范数一定收敛。其他范数收敛，无穷范数一定收敛。</li>
</ul>
</li>
</ul>
<h3 id="7-2-矩阵范数"><a href="#7-2-矩阵范数" class="headerlink" title="7.2 矩阵范数"></a>7.2 矩阵范数</h3><ul>
<li><p>矩阵可以转化为向量表示</p>
</li>
<li><p>矩阵范数：$A\in P^{m\times n}$，需满足以下条件</p>
<ol>
<li>正定性：$||A||\ge0,且||A||=0\iff A=0$</li>
<li>齐次性：$||\lambda A||=|\lambda|\cdot ||A||,\lambda\in R,A\in P^{m\times n}$</li>
<li>三角不等式：$||A+B||\le ||A||+||B||,\forall A,B\in P^{m\times n}$</li>
<li><strong>相容性</strong>：$\Vert AB \Vert \le \Vert A\Vert\cdot \Vert B\Vert$</li>
</ol>
<blockquote>
<p><strong>Remarks</strong>：这里相容性的定义目的是什么呢？为了放缩方便？</p>
</blockquote>
</li>
<li><p>例如</p>
<ul>
<li>（自相容）$|A|_{m_{1}}=\sum_{j=1}^{n} \sum_{i=1}^{m}\left|a_{i j}\right|$</li>
<li>（不相容）$|A|_{m_{\infty}}=\max _{i, j}\left\{\left|a_{i j}\right|\right\} \quad 1 \leq i \leq m \quad 1 \leq j \leq n$</li>
<li>（自相容）Frobenius范数：$|A|_{m_{2}}=\left(\sum_{j=1}^{n} \sum_{i=1}^{m}\left|a_{i j}\right|^{2}\right)^{\frac{1}{2}}$<ul>
<li>$|\boldsymbol{A}|_{m_{2}}^{2}=\operatorname{tr}\left(\boldsymbol{A}^{\boldsymbol{H}} \boldsymbol{A}\right)=\sum_{i=1}^{n} \lambda_{i}\left(\boldsymbol{A}^{\boldsymbol{H}} \boldsymbol{A}\right)$</li>
<li>对任意酉矩阵$U,V$，$|\boldsymbol{A}|_{m_{2}}^{2}=\left|\boldsymbol{U}^{\boldsymbol{H}} \boldsymbol{A} \boldsymbol{V}\right|_{m_{2}}^{2}=\left|\boldsymbol{U} \boldsymbol{A} \boldsymbol{V}^{\boldsymbol{H}}\right|_{m_{2}}^{2}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-3-算子范数"><a href="#7-3-算子范数" class="headerlink" title="7.3 算子范数"></a>7.3 算子范数</h3><ul>
<li><p>向量范数与矩阵范数的相容性：$|A x|_{m} \leq|A|_{m}|x|_{m}$ 是否成立</p>
<ul>
<li><strong>定义</strong>：设 $|\cdot|_a$ 是 $P^n$ 上的向量范数，$|\cdot|_m$ 是 $P^{n\times n}$ 上的矩阵范数，且<script type="math/tex; mode=display">
\|A x\|_{a} \leq\|A\|_{m}\|x\|_{a}\notag</script>则称 $|\cdot|_m$ 为与向量范数 $|\cdot|_a$ 相容的矩阵范数</li>
</ul>
</li>
<li><p>算子范数</p>
<ul>
<li><p>设 $|\cdot|_a$ 是 $P^n$ 上的向量范数，$A\in P^{n\times n}$，则</p>
<script type="math/tex; mode=display">
\|\boldsymbol{A}\|_{a}=\underset{\boldsymbol{x} \neq \boldsymbol{\theta}}{\max } \frac{\|\boldsymbol{A} \boldsymbol{x}\|_{a}}{\|\boldsymbol{x}\|_{a}}\left(=\max _{\|u\|_{a}=1}\|A u\|_{a}\right) \notag</script><p>是与向量范数 $|\cdot|_a$ 相容的矩阵范数</p>
</li>
<li><p>推论：算子范数也是相容的矩阵范数，即 $|AB|_a\le|A|_a|B|_a$</p>
</li>
</ul>
</li>
<li><p>常用算子范数</p>
<ul>
<li>极大列和范数：$|\boldsymbol{A}|_{\mathbf{1}}=\mathbf{m}_{\boldsymbol{j}} \mathbf{x}\left(\sum_{\boldsymbol{i}=1}^{\boldsymbol{n}}\left|\boldsymbol{a}_{i j}\right|\right)$</li>
<li>极大行和范数：$|A|_{\infty}=\max _{i}\left(\sum_{j=1}^{n}\left|a_{i j}\right|\right)$</li>
<li>谱范数：$|\boldsymbol{A}|_{2}=\sqrt{r\left(\boldsymbol{A}^{\boldsymbol{H}} \boldsymbol{A}\right)}$<ul>
<li>谱半径：$r(A)=\max _{i}\left|\lambda_{i}\right|$</li>
<li>$|A|_{2}=\left|A^{H}\right|_{2}=\left|A^{T}\right|_{2}=|\bar{A}|_{2}$</li>
<li>$\left|A^{H} A\right|_{2}=\left|A A^{H}\right|_{2}=|A|_{2}^{2}$</li>
<li>对任意酉矩阵$U,V$，$|\boldsymbol{U} \boldsymbol{A}|_{2}=|\boldsymbol{A} \boldsymbol{V}|_{2}=|\boldsymbol{U} \boldsymbol{A} \boldsymbol{V}|_{2}=|\boldsymbol{A}|_{2}$</li>
</ul>
</li>
</ul>
</li>
<li><p>定理</p>
<ul>
<li>$|\boldsymbol{A}|_{2}=\max _{|x|_{2}=|y|_{2}=\mathbf{1}}\left|\boldsymbol{y}^{\boldsymbol{H}} \boldsymbol{A} \boldsymbol{x}\right|$</li>
<li>$|\boldsymbol{A}|_{2}^{2} \leq|\boldsymbol{A}|_{1}|\boldsymbol{A}|_{\infty}$</li>
</ul>
</li>
</ul>
<hr>
<h2 id="8-矩阵分解"><a href="#8-矩阵分解" class="headerlink" title="8. 矩阵分解"></a>8. 矩阵分解</h2><h3 id="8-1-三角分解"><a href="#8-1-三角分解" class="headerlink" title="8.1 三角分解"></a>8.1 三角分解</h3><ul>
<li>三角矩阵<ul>
<li>逆矩阵仍然是三角矩阵</li>
<li>三角矩阵的积仍是三角矩阵</li>
</ul>
</li>
</ul>
<blockquote>
<p> <strong>定理(LU分解)</strong>：设  $A\in C^{n\times n}$，则 $A$ 可<strong>唯一的</strong>分解为</p>
<script type="math/tex; mode=display">
A=U_1 R \notag</script><p>其中 $U_1$ 为酉矩阵，$R$ 为正线上三角矩阵；或者 A 可以<strong>唯一的</strong>分解为</p>
<script type="math/tex; mode=display">
A = L U_2 \notag</script><p>其中 $U_2$ 为酉矩阵，$L$ 为正线下三角矩阵。</p>
<p><strong>推论 1</strong>：对于实数域，则有类似的 <strong>QR分解</strong></p>
<p><strong>推论 2.1</strong>：对于<strong>实对称</strong>矩阵，存在唯一上三角实矩阵</p>
<script type="math/tex; mode=display">
A = R^T R \notag</script><p><strong>推论 2.2</strong>：正定 <strong>Hermite</strong> 矩阵，存在唯一上三角复矩阵</p>
<script type="math/tex; mode=display">
A = R^H R \notag</script></blockquote>
<ul>
<li>任意矩阵的三角分解（非方阵）</li>
</ul>
<h3 id="8-2-谱分解"><a href="#8-2-谱分解" class="headerlink" title="8.2 谱分解"></a>8.2 谱分解</h3><ul>
<li><strong>单纯矩阵</strong>：代数重数等于几何重数</li>
</ul>
<blockquote>
<p><strong>定理</strong>：设 $A\in C^{n\times n}$ 是<strong>单纯矩阵</strong>，则 $A$ 可以分解为一系列<strong>幂等矩阵</strong> $A_i$ 的加权和</p>
<script type="math/tex; mode=display">
A = \sum_{i=1}^n \lambda_i A_i \notag</script><p>其中 $\lambda_i$ 是 $A$ 的特征值</p>
<p><strong>证明</strong>：由单纯矩阵可知</p>
<script type="math/tex; mode=display">
A=P\Lambda P^{-1}=\left(v_{1}, v_{2}, \cdots, v_{n}\right)\left[\begin{array}{cccc}{\lambda_{1}} & {0} & {\cdots} & {0} \\{0} & {\lambda_{2}} & {\cdots} & {0} \\{\cdots} & {\cdots} & {\cdots} & {\cdots} \\{0} & {0} & {\cdots} & {\lambda_{n}}\end{array}\right]\left(\begin{array}{c}{\omega_{1}^{T}} \\{\omega_{2}^{T}} \\{\vdots} \\{\omega_{n}^{T}}\end{array}\right) \notag</script><p>取 $A_i = v_i w_i^T$，$A_i$ 的性质：</p>
<ul>
<li>幂等性：$A_i^2=A_i$</li>
<li>分离性：$A_i A_j=0(i\ne0)$</li>
<li>可加性：$\sum_{i=1}^n A_i = E_n$</li>
</ul>
<blockquote>
<p><strong>Remarks</strong></p>
<p>这里的幂等矩阵 $A_i$ 可以看作是正交<strong>基</strong>的概念</p>
<p>由前面投影矩阵的定义可知，<strong>每一个 $A_i$ 都是一个投影矩阵</strong>，将任意一个向量 $x$ 投影到 $v_i$ 张成的子空间 $L(v_i)$ 上。因此上面的幂等矩阵分解实际上可以理解为“<strong>特征空间分解</strong>”（笔者瞎想的名词），如何理解呢？把<strong>每个 $A_i$ 看作是矩阵 $A$ 的一个特征子空间（的投影基）</strong>，$Ax$ 实际上就是把 $x$ 投影到各个特征子空间中，然后根据对应的<strong>特征值</strong>进行伸缩，最后再合成一个作用后的向量，即表示 $A$ 对 $x$ 的线性变换。</p>
</blockquote>
<p><strong>定理</strong>：设 $A\in C^{n\times n}$，有 $k$ 个相异的特征值 $\lambda_i(i=1,…,k)$，则 $A$ 是<strong>单纯矩阵</strong>的充要条件是，存在 $k$ 个矩阵矩阵 $A_i$ 满足</p>
<ol>
<li>$A_{i} A_{j}=\left\{\begin{array}{ll}{A_{i}} &amp; {i=j} \\ {0} &amp; {i \neq j}\end{array}\right.$</li>
<li>$\sum_{i=1}^k A_i = E_n$</li>
<li>$A = \sum_{i=1}^k \lambda_i A_i$</li>
</ol>
</blockquote>
<ul>
<li><strong>正规矩阵</strong>：满足 $A^HA=AA^H$ 的矩阵<br><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/normal%20matrix.jpg" alt="正规矩阵"></li>
</ul>
<blockquote>
<p><strong>引理</strong>：设 $A$ 为正规矩阵，$A$ 与 $B$ <strong>酉相似</strong>，则 $B$ 为正规矩阵</p>
<blockquote>
<p> <strong>定理</strong>：任意矩阵 $A\in C^{n\times n}$，存在酉矩阵 $U$ 使得</p>
<script type="math/tex; mode=display">
A=URU^H \notag</script><p>其中 $R$ 为<strong>上三角矩阵</strong>且主对角线元素为 $A$ 的特征值</p>
</blockquote>
<p><strong>引理</strong>：设 $A$ 为正规矩阵且为三角矩阵，则 $A$ 为对角矩阵</p>
<blockquote>
<p><strong>Remarks</strong>：</p>
<p><strong>任意矩阵 $A$ 都与三角阵 $R$ 酉相似</strong>，因此若矩阵 $A$ 为正规阵，则 $R$ 既是正规阵，又是三角阵，则一定是对角阵。</p>
<p>因此，<strong>正规阵一定可以对角化</strong>，由下面的定理可知，可以<strong>酉对角化</strong>的矩阵一定是正规矩阵。</p>
<p>这与普通的可对角化矩阵的区别是什么呢？普通矩阵可对角化的充要条件是代数重数等于几何重数，也即只需要 <strong>n 个线性无关的特征向量</strong>即可($A=PJP^{-1}$)。而正规矩阵则要求<strong>所有特征向量正交</strong>($A=U\Lambda U^H$)！</p>
<p><strong>Remarks</strong></p>
<p>那么<strong>正定矩阵</strong>与<strong>正规矩阵</strong>的区别是什么呢？先看正定矩阵的定义：特征值全部为正数。区别很明显了，一个是从特征值角度，另一个是从特征向量角度，牢记这一点就不会弄混两者了。</p>
<p>凡是具有 $A^HA$ 形式的矩阵，既是<strong>正规矩阵</strong>，又是<strong>正定矩阵</strong>！</p>
</blockquote>
<p><strong>定理</strong>：$A$ 为正规矩阵的充要条件是存在酉矩阵 $U$ 使</p>
<script type="math/tex; mode=display">
A = U \text{diag}(\lambda_1,...,\lambda_n)U^H \notag</script><p>其中 $\lambda_i$ 是 $A$ 的特征值</p>
<p><strong>定理</strong>：$A$ 有 $k$ 个相异特征值，则 $A$ 是正规矩阵的充要条件是存在 $k$ 个矩阵 $A_i$ 满足</p>
<ol>
<li>$A_{i} A_{j}=\left\{\begin{array}{ll}{A_{i}} &amp; {i=j} \\ {0} &amp; {i \neq j}\end{array}\right.$</li>
<li>$\sum_{i=1}^k A_i = E_n$</li>
<li>$A = \sum_{i=1}^k \lambda_i A_i$</li>
<li>$A_i^H = A_i(i=1,…,k)$</li>
</ol>
</blockquote>
<h3 id="8-3-最大秩分解"><a href="#8-3-最大秩分解" class="headerlink" title="8.3 最大秩分解"></a>8.3 最大秩分解</h3><ul>
<li><strong>定理</strong>：设 $A\in C^{m\times n}_r$，则存在矩阵 $B\in C^{m\times r}_r, D\in C^{r\times n}_r$，使得 $A=BD$<ul>
<li>注：可以理解为 $B$ 取出了 $r$ 线性无关的列向量，或者 $D$ 取出了 $r$ 个线性无关的行向量</li>
<li>$(B^HB)^{-1}B^HB=E_r$，可以用于求 $B$ 的左逆，$D$ 同理</li>
</ul>
</li>
</ul>
<h3 id="8-4-奇异值分解"><a href="#8-4-奇异值分解" class="headerlink" title="8.4 奇异值分解"></a>8.4 奇异值分解</h3><ul>
<li><strong>奇异值</strong>：设 $A\in C^{m\times n}_r$，$A^HA$ 的特征值为 $\lambda_{1} \geq \lambda_{2} \geq \cdots \geq \lambda_{r}&gt;\lambda_{r+1}=\cdots=\lambda_{n}=\mathbf{0}$，则称 $\sigma_{i}=\sqrt{\lambda_{i}}(i=1,2, \cdots, r)$ 为 $A$ 的正奇异值（实际上就相当于 A 的“绝对特征值”）</li>
<li><strong>定理</strong>：设 $A\in C^{m\times n}_r$，则有<ol>
<li>$rank(A)=rank(A^HA)=rank(AA^H)$</li>
<li>$A^HA,AA^H$ 的特征值均为非负实数</li>
<li>$A^HA,AA^H$ 的特征值相同</li>
</ol>
</li>
<li><strong>酉等价</strong>：$A,B\in C^{m\times n}$，存在酉矩阵 $U,V$ 使得 $A=UBV$</li>
<li><strong>定理</strong>：若 $A,B$ 酉等价，则它们有相同的奇异值</li>
</ul>
<blockquote>
<p><strong>定理</strong>：设 $A\in C^{m\times n}_r$，$\sigma_1,…,\sigma_r$ 是 $A$ 的 $r$ 个奇异值，则存在酉矩阵 $U\in C^{m\times m},V\in C{n\times n}$，使得</p>
<script type="math/tex; mode=display">
A=U\left[\begin{array}{ll}{D} & {0} \\ {0} & {0}\end{array}\right] V \notag</script><p>其中 $\boldsymbol{D}=\operatorname{diag}\left(\delta_{1}, \delta_{2}, \cdots, \delta_{r}\right),\left|\delta_{i}\right|=\sigma_{i}$</p>
</blockquote>
<hr>
<h2 id="9-特征值估计"><a href="#9-特征值估计" class="headerlink" title="9. 特征值估计"></a>9. 特征值估计</h2><h3 id="9-1-几个不等式"><a href="#9-1-几个不等式" class="headerlink" title="9.1 几个不等式"></a>9.1 几个不等式</h3><ul>
<li><strong>定理 1(Schur 不等式)</strong>：设 $A\in C^{n\times n}$ 的特征值为 $\lambda_1,…,\lambda_n$，则 $\sum_{i=1}^{n}\left|\lambda_{i}\right|^{2} \leq \sum_{i=1}^{n} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}=|A|_{F}^{2}$，等号成立当且仅当 $A$ 为正规矩阵</li>
<li><strong>定理 2(Hirsch)</strong>：设 $A\in C^{n\times n}$，记 $B=\frac{A+A^H}{2},C=\frac{A-A^H}{2}$，$A,B,C$ 特征值分别为 $\{\lambda_i\},\{\mu_i\},\{i\gamma_i\}$，均从大到小排列。则有<ol>
<li>$\left|\lambda_{i}\right| \leq n \max _{i, j}\left|a_{i j}\right|$</li>
<li>$\left|\mathbf{R e} \lambda_{i}\right| \leq n \max _{i, j}\left|b_{i j}\right|$</li>
<li>$\left|\mathbf{I m} \lambda_{i}\right| \leq \boldsymbol{n} \max _{i, j}\left|\boldsymbol{c}_{i j}\right|$</li>
</ol>
</li>
<li><strong>定理 3(Bendixson)</strong>：设 $A\in R^{n\times n}$，则 $A$ 的任一特征值满足 $\left|\mathbf{I m} \lambda_{i}\right| \leq \sqrt{\frac{n(n-1)}{2}} \max _{i, j}\left|c_{i j}\right|$</li>
</ul>
<h3 id="9-2-盖尔圆盘定理"><a href="#9-2-盖尔圆盘定理" class="headerlink" title="9.2 盖尔圆盘定理"></a>9.2 盖尔圆盘定理</h3><ul>
<li><strong>定义 1</strong>：设 $A\in C^{n\times n}$<ul>
<li>行盖尔圆盘：$S_{i}=\left\{z \in C:\left|z-a_{i i}\right| \leq R_{i}=\sum_{j \neq i}\left|a_{i j}\right|\right\}$</li>
<li>列盖尔圆盘：$G_{i}=\left\{z \in C:\left|z-a_{i i}\right| \leq C_{i}=\sum_{j \neq i}\left|a_{j i}\right|\right\}$</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>定理 1(圆盘定理)</strong>：设 $A\in C^{n\times n}$，则 $A$ 的任一特征值</p>
<script type="math/tex; mode=display">
\lambda_{i} \in \boldsymbol{S}=\bigcup_{j=1}^{n} \boldsymbol{S}_{j} \quad(\boldsymbol{i}=\mathbf{1}, 2, \cdots, \boldsymbol{n}) \notag</script><p>类似的，有</p>
<script type="math/tex; mode=display">
\lambda_{i} \in \left(\bigcup_{j=1}^{n} \boldsymbol{S}_{j}\right) \bigcap \left(\bigcup_{j=1}^{n} \boldsymbol{G}_{j}\right)
\quad(\boldsymbol{i}=\mathbf{1}, 2, \cdots, \boldsymbol{n}) \notag</script><p><strong>定理 2</strong>：设 $n$ 阶方阵 $A$ 的 $n$ 个盖尔圆盘中有 $k$ 个圆盘的并形成一个<strong>连通区域</strong> $G$（圆盘相切也算连通），且它与余下的 $n-k$ 个圆盘都不相交，则在该区域中恰好有 $A$ 的 $k$ 个特征值</p>
<p><strong>证明</strong>：取 $A_{\varepsilon}=D+\varepsilon B,\ \varepsilon \in[0,1]$，而 $A_\varepsilon$ 的特征值 $\lambda_i(A_\varepsilon) = \lambda_i(\varepsilon)$ 时关于 $\varepsilon$ 的<strong>连续函数</strong>，在圆盘随着 $\varepsilon$ 扩大过程中，特征值一直都处于圆盘内部</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2020/gerschgorin.jpg" alt="gerschgorin"></p>
<p><strong>推论 1</strong>：设 $n$ 阶方阵 $A$ 的 $n$ 个盖尔圆盘两两互不相交，则 $A$ 相似于对角阵</p>
<p><strong>推论 2</strong>：设 $n$ 阶<strong>实矩阵</strong> $A$ 的 $n$ 个盖尔圆盘两两互不相交，则 $A$ 的特征值全部为实数</p>
<p><strong>改进</strong>：可以取 $D=diag(p_1,…,p_n),\ \ p_i&gt;0$，则有 $D^{-1}AD$ 与 $A$ <strong>相似</strong>，因此他们有相同的特征值，可以用 $D^{-1}AD$ 的特征值来估计 $A$。此时可以将某些盖尔圆变小，但是代价就是其他盖尔圆会变大。</p>
</blockquote>
<ul>
<li><strong>行对角占优</strong>：$\left|a_{ii}\right| \geq R_{i}=\sum_{j=1, j \ne i}^{n}\left|a_{i j}\right| \quad(i=1,2, \cdots, n)$</li>
<li><strong>列对角占优</strong>：$\left|a_{ii}\right| \geq C_{i}=\sum_{j=1, j \ne i}^{n}\left|a_{ji}\right| \quad(i=1,2, \cdots, n)$</li>
</ul>
<blockquote>
<p><strong>定理 3</strong>：设 $A\in C^{n\times n}$ <strong>严格</strong>行对角占优，则</p>
<ol>
<li>$A$ 可逆</li>
<li>若 $A$ 所有主对角元都为正数，则 $A$ 的特征值都有正实部</li>
<li>若 $A$ 为 Hermite 矩阵，且所有主对角元都为正数，则 $A$ 的特征值都为正数</li>
</ol>
</blockquote>
<h3 id="9-3-Hermite矩阵特征值的变分特性"><a href="#9-3-Hermite矩阵特征值的变分特性" class="headerlink" title="9.3 Hermite矩阵特征值的变分特性"></a>9.3 Hermite矩阵特征值的变分特性</h3><p>因为Hermite矩阵 $A\in C^{n\times n}$ 的特征值均为实数，所以可以把他们记作（按照大小进行排序）：</p>
<script type="math/tex; mode=display">
\lambda_{\min }=\lambda_{n} \leq \lambda_{n-1} \ldots \leq \lambda_{2} \leq \lambda_{1}=\lambda_{\max } \notag</script><ul>
<li><strong>Rayleigh 商</strong>：$R(x)=\frac{x^{H} A x}{x^{H} x} \quad x \neq 0$<ul>
<li>$\lambda_{n} x^{H} x \leq x^{H} A x \leq \lambda_{1} x^{H} x \quad\left(\forall x \in C^{n}\right)$</li>
<li>$\lambda_{\max }=\lambda_{1}=\max _{x \neq 0} R(x)=\max _{x^{H}} x^{H} A x$</li>
<li>$\lambda_{\min }=\lambda_{n}=\min _{x \neq 0} R(x)=\min _{x^{H} x=1} x^{H} A x$</li>
</ul>
</li>
<li><strong>定理(Courant-Fischer)</strong>：设特征值 $\lambda_1 \le \lambda_2 \le \cdots \le \lambda_n$，则<ul>
<li>$\begin{array}{ccc}{\min } &amp; {\max } &amp; {R(x)=\lambda_{k}} \\ {\omega_{1}, \omega_{2}, \cdots, \omega_{n-k} \in C^{n}} &amp; {x \neq 0, x \in C^{n} \atop {x \perp \omega_{1}, \omega_{2}, \cdots, \omega_{n-k}}} &amp; {} \end{array}$</li>
<li>$\begin{array}{ccc}{\max } &amp; {\min } &amp; {R(x)=\lambda_{k}} \\ {\omega_{1}, \omega_{2}, \cdots, \omega_{n-k} \in C^{n}} &amp; {x \neq 0, x \in C^{n} \atop {x \perp \omega_{1}, \omega_{2}, \cdots, \omega_{n-k}}} &amp; {} \end{array}$</li>
</ul>
</li>
<li><strong>定理(Weyl)</strong>：$\lambda_k(A)+\lambda_n(B)\le\lambda_k(A+B)\le\lambda_k(A)+\lambda_1(B)$</li>
</ul>
<hr>
<h2 id="10-矩阵分析"><a href="#10-矩阵分析" class="headerlink" title="10. 矩阵分析"></a>10. 矩阵分析</h2><h3 id="10-1-矩阵序列与矩阵级数"><a href="#10-1-矩阵序列与矩阵级数" class="headerlink" title="10.1 矩阵序列与矩阵级数"></a>10.1 矩阵序列与矩阵级数</h3><ul>
<li>矩阵序列<ul>
<li><strong>定理</strong>：设 $\Vert\cdot\Vert$ 是 $C^{m\times n}$ 上的任一矩阵范数，矩阵序列 $\{A^{(k)}\}$ 收敛于 $A$ 的充要条件是 $\lim _{k \rightarrow+\infty}\left|A^{(k)}-A\right|=0$</li>
<li><strong>定理</strong>：设 $\lim _{k \rightarrow+\infty} A^{(k)}=A, \lim _{k \rightarrow+\infty} B^{(k)}=B . \alpha, \beta \in C$，则<ul>
<li>$\lim _{k \rightarrow+\infty}\left(\alpha A^{(k)}+\beta B^{(k)}\right)=\alpha A+\beta B$</li>
<li>$\lim _{k \rightarrow+\infty} A^{(k)} B^{(k)}=A B$</li>
<li>当 $A^{(k)}$ 与 $A$ 都可逆时，$\lim _{k \rightarrow+\infty}\left(A^{(k)}\right)^{-1}=A^{-1}$</li>
</ul>
</li>
</ul>
</li>
<li><strong>收敛矩阵</strong>：设 $A\in C^{n\times n}$，若 $\lim _{k \rightarrow \infty} A^{k}=0$，则称 $A$ 为收敛矩阵<ul>
<li><strong>定理</strong>：设 $A\in C^{n\times n}$，则 $A$ 为收敛矩阵的充要条件是 $r(A)&lt;1$</li>
</ul>
</li>
<li><strong>矩阵级数</strong>：$\sum_{k=1}^{\infty} A^{(k)}=A^{(1)}+A^{(2)}+\cdots+A^{(k)}+\cdots$，称 $\boldsymbol{S}^{(\boldsymbol{N})}=\sum_{\boldsymbol{k}=1}^{\boldsymbol{N}} \boldsymbol{A}^{(\boldsymbol{k})}$ 为矩阵级数的部分和，若 $\lim _{N \rightarrow \infty} S^{(N)}=S$ 则称级数<strong>收敛</strong><ul>
<li><strong>定理</strong>：在 $C^{n\times n}$ 中，$\sum_{k=1}^{\infty} A^{(k)}$ 绝对收敛的充要条件是正项级数 $\sum_{k=1}^{\infty}\left|A^{(k)}\right|$ 收敛</li>
<li><strong>定理</strong>：方阵 $A$ 的 Neumann 级数 $\sum_{k=0}^{\infty} A^{k}=I+A+A^{2}+\cdots+A^{k}+\cdots$ 收敛的充要条件是 $r(A)&lt;1$，且收敛时，其和为 $(I-A)^{-1}$</li>
</ul>
</li>
</ul>
<h3 id="10-2-矩阵函数"><a href="#10-2-矩阵函数" class="headerlink" title="10.2 矩阵函数"></a>10.2 矩阵函数</h3><ul>
<li><p>幂级数：设幂级数 $\sum_{k=0}^{\infty} c_{k} z^{k}$ 收敛半径为 $r$，且当 $|z|&lt;r$ 时，幂级数收敛于函数 $f(z)$，即 $f(z)=\sum_{k=0}^{\infty} c_{k} z^{k}, \quad|z|&lt;r$</p>
</li>
<li><p>矩阵幂级数：如果 $A\in C^{n\times n}$ 满足 $r(A)&lt;r$，则称收敛矩阵的矩阵幂级数 $\sum_{k=0}^{\infty} a_{k} A^{k}$ 为矩阵函数，记为 $f(A)$，即 $f(A)=\sum_{k=0}^{\infty} c_{k} A^{k}$，考虑参数 $t$，有 $f(At)=\sum_{k=0}^{\infty} c_{k} (At)^{k}$</p>
<ul>
<li>常用矩阵函数：</li>
<li>$e^{A}=\sum_{k=0}^{\infty} \frac{1}{k !} A^{k}, \quad A \in C^{n \times n}$</li>
<li>$\sin A=\sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k+1) !} A^{2 k+1}, \quad A \in C^{n \times n}$</li>
<li>$\cos A=\sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k) !} A^{2 k}, \quad A \in C^{n \times n}$</li>
<li>$(E-A)^{-1}=\sum_{k=0}^{\infty} A^{k}, \quad r(A)&lt;1$</li>
<li>$\ln (E+A)=\sum_{k=0}^{\infty} \frac{(-1)^{k}}{k+1} A^{k+1}, \quad r(A)&lt;1$</li>
</ul>
</li>
<li><p>矩阵函数值计算</p>
<ul>
<li><p>相似对角化：设 $P^{-1}AP=diag(\lambda_1,…,\lambda_n)=D$，则 $f(At) = P\cdot diag(f(\lambda_1 t),…,f(\lambda_n t))\cdot P^{-1}$</p>
</li>
<li><p>Jordan标准型：设 $P^{-1}AP=diag(J_1,…,J_s)$，则 </p>
<script type="math/tex; mode=display">
f(A)=P\left(\begin{array}{ccc}
{f\left(J_{1}\right)} & {} & {} \\
{} & {\ddots} & {} \\
{} & {} & {f\left(J_{s}\right)}
\end{array}\right) P^{-1} \notag</script></li>
</ul>
</li>
<li><p>矩阵函数性质</p>
<ul>
<li>如果 $AB=BA$，则<ul>
<li>$e^{A} e^{B}=e^{B} e^{A}=e^{A+B}$</li>
<li>$\cos (A+B)=\cos A \cos B-\sin A \sin B$</li>
<li>$\sin (A+B)=\sin A \cos B+\cos A \sin B$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="11-矩阵求逆"><a href="#11-矩阵求逆" class="headerlink" title="11 矩阵求逆"></a>11 矩阵求逆</h2><hr>
<h2 id="Hermite矩阵的性质"><a href="#Hermite矩阵的性质" class="headerlink" title="Hermite矩阵的性质"></a>Hermite矩阵的性质</h2><ul>
<li>一般 Hermite 矩阵<ul>
<li>Hermite 矩阵本身就是<strong>正规矩阵</strong>，因此可以对角化(几何重数等于代数重数)，不同特征向量<strong>正交</strong></li>
<li>特征值均为<strong>实数</strong>（反 Hermite 矩阵的特征值全为虚数）</li>
</ul>
</li>
<li>正定 Hermite 矩阵<ul>
<li>主对角线元素全部大于 0</li>
<li>存在正定 Hermite 矩阵 $B$ 使得 $A=B^2$（可以无穷分解）</li>
<li>$A$ 的任意 k 行和对应的 k 列组成的主子阵是正定的 </li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
        <tag>矩阵分析</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(十一)  Sum-product algorithm</title>
    <url>/2020/02/03/statistic/SI_Ch11_Sumproduct/</url>
    <content><![CDATA[<p>和积算法/信念传播算法</p>
<a id="more"></a>
<h2 id="1-Sum-product-Message-passing-on-trees"><a href="#1-Sum-product-Message-passing-on-trees" class="headerlink" title="1. Sum-product(Message passing) on trees"></a>1. Sum-product(Message passing) on trees</h2><ul>
<li><p>目的是为了计算边缘分布，相比于 elimination 的优势在于可以用较少的计算次数计算所有随机变量的边缘分布，关键在于复用 message</p>
</li>
<li><p>algorithm</p>
<ul>
<li><p>Step 1: Compute messages </p>
<script type="math/tex; mode=display">
m_{i \rightarrow j}\left(x_{j}\right)=\sum_{x_{i}} \phi_{i}\left(x_{i}\right) \psi_{i j}\left(x_{i}, x_{j}\right) \prod_{k \in N(i) \backslash\{j\}} m_{k \rightarrow i}\left(x_{i}\right)</script></li>
<li><p>Step 2: Compute marginals </p>
<script type="math/tex; mode=display">
p_{\times i}\left(x_{i}\right) \propto \phi_{i}\left(x_{i}\right) \prod_{j \in N(i)} m_{j \rightarrow i}\left(x_{i}\right)</script></li>
</ul>
</li>
<li><p>Remarks</p>
<ul>
<li><p>什么是 message？</p>
</li>
<li><p>tree 的一枝表示什么？实际上就是一个<strong>条件分布</strong>，如下图中实际上就是 $m_2(x_1)=\sum_{x_2,x_4,x_5}p(x_2,x_4,x_5|x_1)$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/message.PNG" alt="message"></p>
</li>
</ul>
</li>
</ul>
<h2 id="2-Sum-product-algorithm-on-factor-trees"><a href="#2-Sum-product-algorithm-on-factor-trees" class="headerlink" title="2. Sum-product algorithm on factor trees"></a>2. Sum-product algorithm on factor trees</h2><ul>
<li><p>algorithm</p>
<ul>
<li><p>Message from variable to factor </p>
<script type="math/tex; mode=display">
m_{i \rightarrow a}\left(x_{i}\right)=\prod_{b \in N(i) \backslash\{a\}} m_{b \rightarrow i}\left(x_{i}\right)</script></li>
<li><p>Message from factor to variable </p>
<script type="math/tex; mode=display">
m_{a \rightarrow i}\left(x_{i}\right)=\sum_{\mathbf{x}_{N(a) \backslash\{i\}}} f_{a}\left(x_{i}, \mathbf{x}_{N(a) \backslash\{i\}}\right) \prod_{j \in N(a) \backslash\{i\}} m_{j \rightarrow a}\left(x_{j}\right)</script></li>
</ul>
</li>
</ul>
<h2 id="3-Max-Product-for-undirected-tree-factor-tree"><a href="#3-Max-Product-for-undirected-tree-factor-tree" class="headerlink" title="3. Max-Product for undirected tree/factor tree"></a>3. Max-Product for undirected tree/factor tree</h2><h2 id="4-Parallel-Max-Product"><a href="#4-Parallel-Max-Product" class="headerlink" title="4. Parallel Max-Product"></a>4. Parallel Max-Product</h2><ul>
<li><p>所有节点同时运算，至多需要 d(最长path的length) 次迭代即可</p>
</li>
<li><p>trick: 整体的减少乘法次数</p>
<script type="math/tex; mode=display">
\begin{align}
\tilde{m}_{i}^{t}\left(x_{i}\right)&=\prod_{k \in N(i)} m_{k \rightarrow i}^{t}\left(x_{i}\right) \\
m_{i \rightarrow j}^{t+1}\left(x_{j}\right)&=\max _{x_{i} \in \mathcal{X}} \phi_{i}\left(x_{i}\right) \psi_{i j}\left(x_{i}, x_{j}\right) \frac{\tilde{m}_{i}^{t}\left(x_{i}\right)}{m_{j \rightarrow i}^{t}\left(x_{i}\right)}
\end{align}</script></li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>信念传播算法</tag>
        <tag>和积算法</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(十) Elimination algorithm</title>
    <url>/2020/02/03/statistic/SI_Ch10_Elimination/</url>
    <content><![CDATA[<p>消去算法</p>
<a id="more"></a>
<hr>
<h2 id="1-Elimination-algorithm"><a href="#1-Elimination-algorithm" class="headerlink" title="1. Elimination algorithm"></a>1. Elimination algorithm</h2><ul>
<li><p>主要目的是为了计算边缘分布</p>
</li>
<li><p>Reconstituted graph: 若消去的随机变量为 $x_k$，则所有与 $x_k$ 连接的随机变量会形成一个新的 clique</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/elimination_1.PNG" alt="elimination"></p>
</li>
<li><p>复杂度</p>
<ul>
<li>Brute-force marginalization：$O\left(|\mathcal{X}|^N\right)$</li>
<li>Zig-zag elimination：$O\left(|\mathcal{X}|^{\sqrt{N}}\right)$</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/elimination_2.PNG" alt="elimination"></p>
<h2 id="2-MAP-elimination-algorithm"><a href="#2-MAP-elimination-algorithm" class="headerlink" title="2. MAP elimination algorithm"></a>2. MAP elimination algorithm</h2><ul>
<li><p>计算 MAP $\boldsymbol{x}^{*} \in \arg \max _{\boldsymbol{x} \in \mathcal{X}^{N}} p_{\mathbf{x}}(\boldsymbol{x})$</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\max _{x, y} f(x) g(x, y)=\max _{x}\left(f(x) \max _{y} g(x, y)\right)} \\ {\sum_{x, y} f(x) g(x, y)=\sum_{x}\left(f(x) \sum_{y} g(x, y)\right)}\end{array}</script></li>
<li><p>example</p>
<script type="math/tex; mode=display">
p_{\mathbf{x}}(\boldsymbol{x}) \propto \exp \left(x_{1} x_{2}-x_{1} x_{3}-x_{2} x_{4}+x_{3} x_{4}+x_{3} x_{5}\right)</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/map_elimination_1.PNG" alt="map_elimination"></p>
</li>
<li><p>algorithm</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/map_elimination_2.PNG" alt="map_elimination"></p>
</li>
<li><p>complexicity</p>
<script type="math/tex; mode=display">
\text { overall cost } \leq|\mathcal{C}| \sum_{i}|\mathcal{X}|^{\left|S_{i}\right|+1} \leq N|\mathcal{C}||\mathcal{X}|^{\max _{i}\left|S_{i}\right|+1}</script></li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>消去算法</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(九) Graphical models</title>
    <url>/2020/02/03/statistic/SI_Ch9_GraphModels/</url>
    <content><![CDATA[<h2 id="1-Undirected-graphical-models-Markov-random-fields"><a href="#1-Undirected-graphical-models-Markov-random-fields" class="headerlink" title="1. Undirected graphical models(Markov random fields)"></a>1. Undirected graphical models(Markov random fields)</h2><ul>
<li><p>节点表示随机变量，边表示与节点相关的势函数</p>
<script type="math/tex; mode=display">
p_{\mathbf{x}}(\mathbf{x}) \propto \varphi_{12}\left(x_{1}, x_{2}\right) \varphi_{13}\left(x_{1}, x_{3}\right) \varphi_{25}\left(x_{2}, x_{5}\right) \varphi_{345}\left(x_{3}, x_{4}, x_{5}\right)</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/undirected_graph.PNG" alt="undirected_graph"></p>
</li>
<li><p><strong>clique</strong>：全连接的节点集合</p>
</li>
<li><p><strong>maximal clique</strong>：不是其他 clique 的真子集</p>
</li>
</ul>
<blockquote>
<p><strong>Theorem (Hammersley-Cliﬀord) </strong>: A strictly positive distribution $p_{\mathsf{x}}(\mathbf{x})&gt;0$ satisﬁes the graph separation property of undirected graphical models if and only if it can be represented in the factorized form </p>
<script type="math/tex; mode=display">
p_{\mathsf{x}}(\mathbf{x}) \propto \prod_{\mathcal{A} \in \mathcal{C}} \psi_{\mathbf{x}_{\mathcal{A}}}\left(\mathbf{x}_{\mathcal{A}}\right)</script></blockquote>
<ul>
<li><strong>conditional independence</strong>：$\mathbf{x}_{\mathcal{A}_{1}} \perp \mathbf{x}_{\mathcal{A}_{2}} | \mathbf{x}_{\mathcal{A}_{3}}$</li>
</ul>
<a id="more"></a>
<h2 id="2-Directed-graphical-models-Bayesian-network"><a href="#2-Directed-graphical-models-Bayesian-network" class="headerlink" title="2. Directed graphical models(Bayesian network)"></a>2. Directed graphical models(Bayesian network)</h2><ul>
<li><p>节点表示随机变量，有向边表示条件关系</p>
<script type="math/tex; mode=display">
p_{\mathrm{x}_{1}, \ldots, \mathrm{x}_{n}}=p_{\mathrm{x}_{1}}\left(x_{1}\right) p_{\mathrm{x}_{2} | \times_{1}}\left(x_{2} | x_{1}\right) \cdots p_{\mathrm{x}_{n} | x_{1}, \ldots, x_{n-1}}\left(x_{n} | x_{1}, \ldots, x_{n-1}\right)</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/directed_graph.PNG" alt="directed_graph"></p>
</li>
<li><p>Directed acyclic graphs (<strong>DAG</strong>) </p>
</li>
<li><p>Fully-connected DAG </p>
</li>
<li><p><strong>conditional independence</strong>：$\mathbf{x}_{\mathcal{A}_{1}} \perp \mathbf{x}_{\mathcal{A}_{2}} | \mathbf{x}_{\mathcal{A}_{3}}$</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/conditional_independence.PNG" alt="conditional_independence"></p>
</li>
<li><p>Bayes ball algorithm </p>
<ul>
<li>primary shade: $\mathcal{A_3}$ 中的节点</li>
<li>secondary shade: primary shade 的节点，以及 secondary shade 的父节点</li>
</ul>
<p><img src="C:\Users\1\AppData\Roaming\Typora\typora-user-images\1574319393095.png" alt="1574319393095"></p>
</li>
</ul>
<h2 id="3-Factor-graph"><a href="#3-Factor-graph" class="headerlink" title="3. Factor graph"></a>3. Factor graph</h2><ul>
<li><p>有 variable nodes 和 factor nodes，是 bipartitie graph</p>
<script type="math/tex; mode=display">
p_{\mathbf{x}}(\mathbf{x}) \propto \prod_{j} f_{j}\left(\mathbf{x}_{f_{j}}\right)</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/factor_graph.PNG" alt="factor_graph"></p>
</li>
<li><p>因子图比 directed graph 和 undirected graph 的表示能力更强，比如 $p(x)=\frac{1}{Z}\phi_{12}(x_1,x_2)\phi_{13}(x_1,x_3)\phi_{23}(x_2,x_3)$</p>
</li>
<li><p>因子图可以与 DAG 相互转化(根据 $x_1,…,x_n$ 依次根据 conditional independence 决定父节点)，DAG又可以转化为 undirected graph</p>
</li>
</ul>
<h2 id="4-Measuring-goodness-of-graphical-representations"><a href="#4-Measuring-goodness-of-graphical-representations" class="headerlink" title="4. Measuring goodness of graphical representations"></a>4. Measuring goodness of graphical representations</h2><ul>
<li>给定分布 D 和图 G，他们之间没必要有联系</li>
<li>$CI(D)$：the set of conditional independencies satisﬁed by $D$</li>
<li>$CI(G)$： the set of all conditional independencies implied by $G$</li>
<li><strong>I-map</strong>：$C I(\mathcal{G}) \subset C I(D)$</li>
<li><strong>D-map</strong>: ：$C I(\mathcal{G}) \supset C I(D)$</li>
<li><strong>P-map</strong>：$C I(\mathcal{G}) = C I(D)$</li>
<li>minimal I-map: Aminimal I-mapisanI-mapwiththepropertythatremovinganysingle edge would cause the graph to no longer be an I-map.<br><strong>Remarks</strong>: G 中去掉一个边会使该 map 中有更多的 conditional independence，也即 $CI(G)$ 更大，更不易满足 I-map条件。I-map 可以表示分布 D，但是 D-map 不能</li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>图模型</tag>
        <tag>因子图</tag>
        <tag>DAG</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(八) Model Selection</title>
    <url>/2020/02/03/statistic/SI_Ch8_ModelSelection/</url>
    <content><![CDATA[<p>模型选择</p>
<a id="more"></a>
<h2 id="1-Bayesian-Approach"><a href="#1-Bayesian-Approach" class="headerlink" title="1.Bayesian Approach"></a>1.Bayesian Approach</h2><ul>
<li><p>Consider a nested sequence of model classes </p>
<script type="math/tex; mode=display">
\mathcal{P}_{1} \subset \mathcal{P}_{2} \subset \mathcal{P}_{3} \subset \cdots</script></li>
<li><p>ML decision rule: </p>
<script type="math/tex; mode=display">
\hat{m}=\arg \max _{m}\left\{\max _{p \in \mathcal{P}_{m}} p(\boldsymbol{y})\right\}=\arg \max _{m}\left\{\max _{a} p_{y | x, H}\left(\boldsymbol{y} | a, H_{m}\right)\right\}</script></li>
</ul>
<h2 id="2-Laplace’s-Method"><a href="#2-Laplace’s-Method" class="headerlink" title="2. Laplace’s Method"></a>2. Laplace’s Method</h2><ul>
<li><p>连续分布</p>
<script type="math/tex; mode=display">
p_{\times}(x)=\frac{p_{0}(x)}{Z_{p}}</script></li>
<li><p>用 taylor 级数近似似然函数</p>
<script type="math/tex; mode=display">
\ln p_{0}(x) \approx \ln p(\hat{x})+\left.(x-\hat{x}) \frac{\mathrm{d}}{\mathrm{d} x} \ln p_{0}(x)\right|_{x=\hat{x}}+\left.\frac{1}{2}(x-\hat{x})^{2} \frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}} \ln p_{0}(x)\right|_{x=\hat{x}} \\
p_{0}(x) \approx p_{0}(\hat{x}) \exp \left[-\frac{1}{2} J_{\mathbf{y}=\boldsymbol{y}}(\hat{x})(x-\hat{x})^{2}\right]</script></li>
</ul>
<h2 id="3-Bayes-Information-Criterion"><a href="#3-Bayes-Information-Criterion" class="headerlink" title="3. Bayes Information Criterion"></a>3. Bayes Information Criterion</h2><ul>
<li>MAP decision rule: <script type="math/tex; mode=display">
\hat{m}=\arg \max _{m} p_{\mathbf{y} | \mathbf{H}}\left(\boldsymbol{y} | H_{m}\right)</script>其中<script type="math/tex; mode=display">
p_{\mathbf{y} | \mathbf{H}}\left(\boldsymbol{y} | H_{m}\right)=\int p_{\mathbf{y} | \mathbf{x}, \mathbf{H}}\left(\boldsymbol{y} | x, H_{m}\right) p_{\mathbf{x} | \mathbf{H}}\left(x | H_{m}\right) \mathrm{d} x</script>令<script type="math/tex; mode=display">
q_{0}(x)=p_{\mathbf{y} | \mathbf{x}, \mathbf{H}}\left(\boldsymbol{y} | x, H_{m}\right) p_{\mathbf{x} | \mathbf{H}}\left(x | H_{m}\right) \propto p_{\mathbf{x} | \mathbf{y}, \mathbf{H}}\left(x | \boldsymbol{y}, H_{m}\right)</script>可以有<script type="math/tex; mode=display">
p_{\mathrm{y} | \mathrm{H}}(\boldsymbol{y} | H)=\int q_{0}(x) \mathrm{d} x \approx p_{\mathrm{y} | x, \mathrm{H}}(\boldsymbol{y} | \hat{x}, H) p_{\mathrm{x} | \mathrm{H}}(\hat{x} | H) \sqrt{2 \pi J_{\mathrm{y}}^{-1}(\hat{x})}</script>其中最后一项为 <strong>Occam’s razor factor</strong></li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>模型选择</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(七)  Typical Sequence</title>
    <url>/2020/02/03/statistic/SI_Ch7_TypicalSequence/</url>
    <content><![CDATA[<p>典型集</p>
<a id="more"></a>
<h2 id="1-一些定理"><a href="#1-一些定理" class="headerlink" title="1. 一些定理"></a>1. 一些定理</h2><blockquote>
<p><strong>Markov inequality</strong>: $r.v. \ \ \mathsf{x}\ge0$</p>
<script type="math/tex; mode=display">
\mathbb{P}(x\ge\mu)\le \frac{\mathbb{E}[x]}{\mu}</script><p><strong>Proof</strong>: omit…</p>
<p><strong>Weak law of large numbers(WLLN)</strong>: $\vec{y}=[y_1,y_2,…,y_N]^T, \ \ \ \ y_i \sim p \ \ \ i.i.d$</p>
<script type="math/tex; mode=display">
\lim_{N\to\infty}\mathbb{P}(|L_p(\vec{y})+H(p)|>\varepsilon)=0, \ \ \forall \varepsilon>0</script><p><strong>Proof</strong>: omit…</p>
</blockquote>
<h2 id="2-Typical-set"><a href="#2-Typical-set" class="headerlink" title="2. Typical set"></a>2. Typical set</h2><ul>
<li><p>Definition: $\mathcal{T}_\varepsilon(p;N)=\{\vec{y}\in\mathcal{Y}^N:|L_p(\vec{y})+H(p)|&lt;\varepsilon\}$</p>
</li>
<li><p>Properties</p>
<ul>
<li>WLLN $\Longrightarrow P\left(\vec{y}\in\mathcal{T}_\varepsilon(p;N)\right)\simeq1$,   $N$ large</li>
<li>$L_p(\vec{y})\simeq H(p) \Longrightarrow p_y(\vec{y})\simeq 2^{-NH(p)}$</li>
<li>$\Longrightarrow |\mathcal{T}_\varepsilon(p;N)|\simeq 2^{NH(p)}$</li>
<li>当 p 不是均匀分布的时候，$\frac{|\mathcal{T}_\varepsilon(p;N)|}{|\mathcal{Y}^N|}\to0$，也就是说典型集中元素(序列)个数在所有可能的元素(序列)中所占比例趋于 0，但是典型集中元素概率的和却趋近于 1</li>
</ul>
</li>
<li><p>Theorem</p>
<blockquote>
<p><strong>Asymptotic Equipartition Property(AEP)</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\lim_{N\to\infty}P(\mathcal{T}_\varepsilon(p;N))=1 \\</script><script type="math/tex; mode=display">
2^{-N(H(p)+\epsilon)} \leq p_{\mathrm{y}}(\boldsymbol{y}) \leq 2^{-N(H(p)-\epsilon)}, \forall \boldsymbol{y} \in \mathcal{T}_{\epsilon}(p ; N)</script></li>
<li>for a sufficient large $N$<script type="math/tex; mode=display">
(1-\epsilon) 2^{N(H(p)-\epsilon)} \leq\left|\mathcal{T}_{\epsilon}(p ; N)\right| \leq 2^{N(H(p)+\epsilon)}</script><strong>Proof</strong>: <script type="math/tex; mode=display">
\begin{aligned}\left|\mathcal{T}_{\epsilon}(p ; N)\right| &=\sum_{\boldsymbol{y} \in \mathcal{T}_{\epsilon}(p ; N)} 1 \\ &=2^{N(H(p)+\epsilon)} \sum_{\boldsymbol{y} \in \mathcal{T}_{\epsilon}(p ; N)} 2^{-N(H(p)+\epsilon)} \\ & \leq 2^{N(H(p)+\epsilon)} \sum_{\boldsymbol{y} \in \mathcal{T}_{\epsilon}(p ; N)} p_{\mathbf{y}}(\boldsymbol{y}) \\ &=2^{N(H(p)+\epsilon)} P\left\{\mathcal{T}_{\epsilon}(p ; N)\right\} \\ & \leq 2^{N(H(p)+\epsilon)} \end{aligned}</script></li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/typical_set.PNG" alt="typical_set"></p>
</blockquote>
</li>
</ul>
<h2 id="3-Divergence-varepsilon-typical-set"><a href="#3-Divergence-varepsilon-typical-set" class="headerlink" title="3. Divergence $\varepsilon$-typical set"></a>3. <strong>Divergence $\varepsilon$-typical set</strong></h2><ul>
<li><p>WLLN: $\vec{y}=[y_1,y_2,…,y_N]^T, \ \ \ \ y_i \sim p \ \ \ i.i.d$</p>
<script type="math/tex; mode=display">
L_{p | q}(\boldsymbol{y})=\frac{1}{N} \log \frac{p_{\mathbf{y}}(\boldsymbol{y})}{q_{\mathbf{y}}(\boldsymbol{y})}=\frac{1}{N} \sum_{n=1}^{N} \log \frac{p\left(y_{n}\right)}{q\left(y_{n}\right)} \\

\lim _{N \rightarrow \infty} \mathbb{P}\left(\left|L_{p | q}(\boldsymbol{y})-D(p \| q)\right|>\epsilon\right)=0</script><p><strong>Remarks</strong>: 前面只考虑的均值，这里还考虑了另一个分布</p>
</li>
<li><p>Definition: $\vec{\boldsymbol{y}}=[y_1,y_2,…,y_N]^T, \ \ \ \ y_i \sim p \ \ \ i.i.d$</p>
<script type="math/tex; mode=display">
\mathcal{T}_{\epsilon}(p | q ; N)=\left\{\boldsymbol{y} \in \mathcal{Y}^{N}:\left|L_{p | q}(\boldsymbol{y})-D(p \| q)\right| \leq \epsilon\right\}</script></li>
<li><p>Properties</p>
<ul>
<li>WLLN $\Longrightarrow q_{\mathbf{y}}(\boldsymbol{y}) \approx p_{\mathbf{y}}(\boldsymbol{y}) 2^{-N D(p | q)}$</li>
<li>$Q\left\{\mathcal{T}_{\epsilon}(p | q ; N)\right\} \approx 2^{-N D(p | q)} \to0$</li>
<li><strong>Remarks</strong>: p 的典型集可能是 q 的非典型集，在 $N$ 很大的时候，不同分布的 typical set 是正交的</li>
</ul>
</li>
<li><p>Theorem</p>
<script type="math/tex; mode=display">
(1-\epsilon) 2^{-N(D(p \| q)+\epsilon)} \leq Q\left\{\mathcal{T}_{\epsilon}(p \| q ; N)\right\} \leq 2^{-N(D(p \| q)-\epsilon)}</script></li>
</ul>
<h2 id="4-Large-deviation-of-sample-averages"><a href="#4-Large-deviation-of-sample-averages" class="headerlink" title="4. Large deviation of sample averages"></a>4. Large deviation of sample averages</h2><blockquote>
<p><strong>Theorem (Cram´er’s Theorem)</strong>: $\vec{\boldsymbol{y}}=[y_1,y_2,…,y_N]^T, \ \ \ y_i \sim q \ \ \ i.i.d$  with mean $\mu&lt;\infty$ and $\gamma&gt;\mu$</p>
<script type="math/tex; mode=display">
\lim _{N \rightarrow \infty}-\frac{1}{N} \log \mathbb{P}\left(\frac{1}{N} \sum_{n=1}^{N} y_{n} \geq \gamma\right)=E_{C}(\gamma)</script><p>where $E_C(\gamma)$ is referred as <strong>Chernoﬀ exponent</strong></p>
<script type="math/tex; mode=display">
E_{C}(\gamma) \triangleq D(p(\cdot ; x) \| q),\ \ \ p(\cdot ; x)=q(y) e^{x y-\alpha(x)}</script><p>and with $x&gt;0$ chosen such that </p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(\cdot;x)}[y]=\gamma</script><p><strong>Proof</strong>: </p>
<ol>
<li>$\begin{aligned} \mathbb{P}\left(\frac{1}{N} \sum_{n=1}^{N} y_{n} \geq \gamma\right) &amp;=\mathbb{P}\left(e^{x \sum_{n=1}^{N} y_{n}} \geq e^{N x \gamma}\right) \\ &amp; \leq e^{-N x \gamma} \mathbb{E}\left[e^{x \sum_{n=1}^{N} y_{n}}\right] \\ &amp;=e^{-N x \gamma}\left(\mathbb{E}\left[e^{x y}\right]\right)^{N} \\ &amp; \leq e^{-N\left(x_{<em>} \gamma-\alpha\left(x_{</em>}\right)\right)} \end{aligned}$</li>
<li>$\varphi(x)=x\gamma-\alpha(x)$ 是凸的，最大值取在 $\mathbb{E}_{p\left(\cdot ; x_{<em>}\right)}[y]=\dot{\alpha}\left(x_{</em>}\right)=\gamma$</li>
<li>可以证明 $x_{<em>} \gamma-\alpha\left(x_{</em>}\right)=x_{<em>} \dot{\alpha}\left(x_{</em>}\right)-\alpha\left(x_{<em>}\right)=D\left(p\left(\cdot ; x_{</em>}\right) | q\right)$</li>
<li>于是有 $\mathbb{P}\left(\frac{1}{N} \sum_{n=1}^{N} y_{n} \geq \gamma\right) \leq e^{-N E_{C}(\gamma)}$</li>
<li><em>下界的证明，暂时略…</em></li>
</ol>
<blockquote>
<p>用到的两个事实：$p(y;x)=q(y)\exp(xy-\alpha(x))$</p>
<ol>
<li>$D(p(y;x)||q(y))$ 随着 x 单调增加</li>
<li>$\mathbb{E}_{p(;x)}[y]$ 随着 x 单调增加</li>
</ol>
</blockquote>
<p><strong>Remarks</strong>:</p>
<ol>
<li>这个定理也相当于表达了 $\mathbb{P}\left(\frac{1}{N} \sum_{n=1}^{N} y_{n} \geq \gamma\right) \cong 2^{-N E_{\mathrm{C}}(\gamma)}$</li>
<li>相当于是分布 <strong>q</strong> 向由 $\mathbb{E}[y]=\sum_{n=1}^{N} y_{n} \geq \gamma$ 所定义的一个凸集中投影，恰好投影到边界(线性分布族) $\mathbb{E}[y]=\gamma$ 上，而 $q$ 向线性分布族的投影恰好就是 (10) 中的指数族表达式</li>
</ol>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/cramer_thm.PNG" alt="cramer_thm"></p>
</blockquote>
<h2 id="5-Types-and-type-classes"><a href="#5-Types-and-type-classes" class="headerlink" title="5. Types and type classes"></a>5. Types and type classes</h2><ul>
<li><p>Definition: $\vec{\boldsymbol{y}}=[y_1,y_2,…,y_N]^T$ (不关心真实服从的是哪个分布)</p>
<ul>
<li><strong>type</strong>(实质上就是一个<strong>经验分布</strong>)定义为</li>
</ul>
<script type="math/tex; mode=display">
\hat{p}(b ; \mathbf{y})=\frac{1}{N} \sum_{n=1}^{N} \mathbb{1}_{b}\left(y_{n}\right)=\frac{N_{b}(\mathbf{y})}{N}</script><ul>
<li>$\mathcal{P}_{N}^{y}$ 表示长度为 $N$ 的序列所有可能的 types</li>
<li><strong>type class</strong>: $\mathcal{T}_{N}^{y}(p)=\left\{\mathbf{y} \in y^{N}: \hat{p}(\cdot ; \mathbf{y}) \equiv p(\cdot)\right\},\ \ \ p\in\mathcal{P}_{N}^{y}$</li>
</ul>
</li>
<li><p>Exponential Rate Notation: $f(N) \doteq 2^{N \alpha}$</p>
<script type="math/tex; mode=display">
\lim _{N \rightarrow \infty} \frac{\log f(N)}{N}=\alpha</script><p><strong>Remarks</strong>: $\alpha$ 表示了指数上面关于 $N$ 的阶数(log、线性、二次 …)</p>
</li>
<li><p>Properties</p>
<ul>
<li>$\left|\mathcal{P}_{N}^{y}\right| \leq(N+1)^{|y|}$</li>
<li>$q^{N}(\mathbf{y})=2^{-N(D(\hat{p}(\cdot \mathbf{y}) | q)+H(\hat{p}(\cdot ; \mathbf{y})))}$<br>$p^{N}(\mathbf{y})=2^{-N H(p)} \quad \text { for } \mathbf{y} \in \mathcal{T}_{N}^{y}(p)$</li>
<li>$c N^{-|y|} 2^{N H(p)} \leq\left|\mathcal{T}_{N}^{y}(p)\right| \leq 2^{N H(p)}$</li>
</ul>
</li>
<li><p>Theorem</p>
<script type="math/tex; mode=display">
c N^{-|y|} 2^{-N D(p \| q)} \leq Q\left\{\mathcal{T}_{N}^{y}(p)\right\} \leq 2^{-N D(p \| q)} \\
Q\left\{\mathcal{T}_{N}^{y}(p)\right\} \doteq 2^{-N D(p \| q)}</script></li>
</ul>
<h2 id="6-Large-Deviation-Analysis-via-Types"><a href="#6-Large-Deviation-Analysis-via-Types" class="headerlink" title="6. Large Deviation Analysis via Types"></a>6. Large Deviation Analysis via Types</h2><ul>
<li>Definition: $\mathcal{R}=\left\{\mathbf{y} \in y^{N}: \hat{p}(\cdot ; \mathbf{y}) \in \mathcal{S} \cap \mathcal{P}_{N}^{y}\right\}$</li>
</ul>
<blockquote>
<p><strong>Sanov’s Theorem</strong>:</p>
<script type="math/tex; mode=display">
Q\left\{\mathrm{S} \cap \mathcal{P}_{N}^{y}\right\} \leq(N+1)^{|y|} 2^{-N D\left(p_{*} \| q\right)} \\
Q\left\{\mathrm{S} \cap \mathcal{P}_{N}^{y}\right\} \dot\leq 2^{-N D\left(p_{*} \| q\right)} \\
p_{*}=\underset{p \in \mathcal{S}}{\arg \min } D(p \| q)</script></blockquote>
<h2 id="7-Asymptotics-of-hypothesis-testing"><a href="#7-Asymptotics-of-hypothesis-testing" class="headerlink" title="7. Asymptotics of hypothesis testing"></a>7. Asymptotics of hypothesis testing</h2><ul>
<li>LRT: $L(\boldsymbol{y})=\frac{1}{N} \log \frac{p_{1}^{N}(\boldsymbol{y})}{p_{0}^{N}(\boldsymbol{y})}=\frac{1}{N} \sum_{n=1}^{N} \log \frac{p_{1}\left(y_{n}\right)}{p_{0}\left(y_{n}\right)} \frac{&gt;}{&lt;} \gamma$</li>
<li>$P_{F}=\mathbb{P}_{0}\left\{\frac{1}{N} \sum_{n=1}^{N} t_{n} \geq \gamma\right\} \approx 2^{-N D\left(p^{*} | p_{0}^{\prime}\right)}$</li>
<li>$P_{M}=1-P_{D} \approx 2^{-N D\left(p^{*} | p_{1}^{\prime}\right)}$</li>
<li>$D\left(p^{<em>} | p_{0}^{\prime}\right)-D\left(p^{</em>} | p_{1}^{\prime}\right)=\int p^{<em>}(t) \log \frac{p_{1}^{\prime}(t)}{p_{0}^{\prime}(t)} \mathrm{d} t=\int p^{</em>}(t) t \mathrm{d} t=\mathbb{E}_{p^{*}}[\mathrm{t}]=\gamma$</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/asymptotic.PNG" alt="asymptotic"></p>
<h2 id="8-Asymptotics-of-parameter-estimation"><a href="#8-Asymptotics-of-parameter-estimation" class="headerlink" title="8.Asymptotics of parameter estimation"></a>8.Asymptotics of parameter estimation</h2><blockquote>
<p><strong>Strong Law of Large Numbers(SLLN)</strong>: </p>
<script type="math/tex; mode=display">
\mathbb{P}\left(\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{n=1}^{N} w_{n}=\mu\right)=1</script><p><strong>Central Limit Theorem(CLT)</strong>:</p>
<script type="math/tex; mode=display">
\lim _{N \rightarrow \infty} \mathbb{P}\left(\frac{1}{\sqrt{N}} \sum_{n=1}^{N}\left(\frac{w_{n}-\mu}{\sigma}\right) \leq b\right)=\Phi(b)</script><p>以下三个强度依次递减</p>
<ol>
<li><p>依概率 1 收敛(SLLN)：$\mathsf{x}_{N} \stackrel{w . p .1}{\longrightarrow} a$</p>
</li>
<li><p>概率趋于 0(WLLN): </p>
</li>
<li>依分布收敛: $\mathsf{x}_{N} \stackrel{d}{\longrightarrow} p$</li>
</ol>
</blockquote>
<ul>
<li><p>Asymptotics of ML Estimation</p>
<blockquote>
<p><strong>Theorem</strong>:</p>
<script type="math/tex; mode=display">
\hat{x}_{N}=\arg \max _{x} L_{N}(x ; \mathbf{y})</script><p>在满足某些条件下(mild conditions)，有</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\hat{x}_{N} \stackrel{w \cdot p \cdot 1}{\longrightarrow} x_{0}} \\ 
{\sqrt{N}\left(\hat{x}_{N}-x_{0}\right) \stackrel{d}{\longrightarrow} \mathcal{N}\left(0, J_{y}\left(x_{0}\right)^{-1}\right)}\end{array}</script></blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>典型集</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(六) Modeling</title>
    <url>/2020/02/03/statistic/SI_Ch6_Modeling/</url>
    <content><![CDATA[<p>模型选择</p>
<a id="more"></a>
<h2 id="1-Modeling-problem"><a href="#1-Modeling-problem" class="headerlink" title="1. Modeling problem"></a>1. Modeling problem</h2><ul>
<li><p>formulation</p>
<ul>
<li><p>a set of distributions </p>
<script type="math/tex; mode=display">
\mathcal{P}=\left\{p_{\mathrm{y}}(\cdot ; x) \in \mathcal{P}^{y}: x \in \mathcal{X}\right\}</script></li>
<li><p>approximation</p>
<script type="math/tex; mode=display">
\min _{q \in \mathcal{P}^{y}} \max _{x \in \mathcal{X}} D\left(p_{\mathrm{y}}(\cdot ; x) \| q(\cdot)\right)</script></li>
</ul>
</li>
<li><p>solution</p>
</li>
</ul>
<blockquote>
<p><strong>Theorem</strong>: 对任意 $q \in \mathcal{P}^{y}$ 都存在一个混合模型 $q_w(\cdot) = \sum_{x \in \mathcal{X}} w(x) p_{y}(\cdot ; x)$ 满足</p>
<script type="math/tex; mode=display">
D\left(p_{y}(\cdot ; x) \| q_{w}(\cdot)\right) \leq D\left(p_{y}(\cdot ; x) \| q(\cdot)\right) \quad \text { for all } x \in \mathcal{X}</script><p><strong>Proof</strong>: 应用 Pythagoras 定理</p>
<p>然后很容易有</p>
<script type="math/tex; mode=display">
\max _{x \in \mathcal{X}} \min _{q \in \mathcal{P}^{y}} D\left(p_{y}(\cdot ; x) \| q(\cdot)\right)=\max _{x \in \mathcal{X}} 0=0 \\</script><script type="math/tex; mode=display">
\min _{q \in \mathcal{P}} \max _{x \in \mathcal{X}} D\left(p_{\mathrm{y}}(\cdot ; x) \| q(\cdot)\right)=\min _{q \in \mathcal{P}} \max _{w \in \mathcal{P}^{\mathcal{X}}} \sum_{x} w(x) D\left(p_{\mathrm{y}}(\cdot ; x) \| q(\cdot)\right)</script><p><strong>Theorem</strong> (Redundancy-Capacity Theorem): 以下等式成立，且两侧最优的 $w,q$ s是相同的</p>
<script type="math/tex; mode=display">
\begin{aligned} R^{+} \triangleq \min _{q \in \mathcal{P}^{\mathcal{Y}}} \max _{w \in \mathcal{P}^{\mathcal{X}}} & \sum_{x} w(x) D\left(p_{\mathrm{y}}(\cdot ; x) \| q(\cdot)\right) \\ &=\max _{w \in \mathcal{P}} \min _{q \in \mathcal{P}} \sum_{x} w(x) D\left(p_{\mathrm{y}}(\cdot ; x) \| q(\cdot)\right) \triangleq R^{-} \end{aligned}</script><p><strong>Proof</strong>: </p>
<ol>
<li>利用后面的 Equidistance property 证明 $R^+ \le R^-$</li>
<li>根据 minimax 和 maxmini 的性质，有 $R^+ \ge R^-$</li>
<li>一定有 $R^+ \ge R^-$</li>
<li>证明两个不等式的取等条件是在同样的 $w,q$ 处取到</li>
</ol>
</blockquote>
<h2 id="2-Model-capacity"><a href="#2-Model-capacity" class="headerlink" title="2. Model capacity"></a>2. Model capacity</h2><blockquote>
<p>首先计算 $R^-$ 内部的 min</p>
<script type="math/tex; mode=display">
\begin{aligned} & \min _{q \in \mathcal{P}^{\mathcal{Y}}} \sum_{x} w(x) D\left(p_{\mathbf{y}}(\cdot ; x) \| q(\cdot)\right) \\=& \min _{q \in \mathcal{P}^{\mathcal{Y}}} \sum_{x, y} w(x) p_{\mathbf{y}}(y ; x) \log \frac{p_{y}(y ; x)}{q(y)} \\=& \text { constant }-\max _{q \in \mathcal{P}^{\mathcal{Y}}} \sum_{y} q_{w}(y) \log q(y) \\=& \text { constant }-\max _{q \in \mathcal{P}^{\mathcal{Y}}} \mathbb{E}_{q_{w}}[\log q(y)] \end{aligned}</script><p>根据 Gibbs 不等式</p>
<script type="math/tex; mode=display">
q^*(\cdot) = q_{w}(\cdot) \triangleq \sum_{x \in \mathcal{X}} w(x) p_{y}(\cdot ; x)</script><p>再考虑 $R^-$ 外部的 max，此时可以转化为 <strong>Bayesian</strong> 角度！</p>
<script type="math/tex; mode=display">
\begin{aligned} R^{-} &=\max _{w \in \mathcal{P}^{\mathcal{X}}} \sum_{x} w(x) D\left(p_{y}(\cdot ; x) \| q_{w}(\cdot)\right) \\ &=\max _{w \in \mathcal{P}^{\mathcal{X}}} \sum_{x, y} w(x) p_{y}(y ; x) \log \frac{p_{y}(y ; x)}{\sum_{x^{\prime}} w\left(x^{\prime}\right) p_{y}\left(y ; x^{\prime}\right)} \\

&\overset{\text{Bayesian}}{=}\max _{p_{\mathbf{x}}} \sum_{x} p_{\mathbf{x}}(x) D\left(p_{y | \mathbf{x}}(\cdot | x) \| p_{y}(\cdot)\right) \\ &=\max _{p_{\mathbf{x}}} \sum_{x, y} p_{\mathbf{x}}(x) p_{\mathbf{y} | \mathbf{x}}(y | x) \log \frac{p_{y | x}(y | x)}{p_{\mathbf{y}}(y)} \\ &=\max _{p_{\mathbf{x}}} \sum_{x, y} p_{\mathbf{x}, \mathbf{y}}(x, y) \log \frac{p_{\mathbf{x}, \mathbf{y}}(x, y)}{p_{\mathbf{x}}(x) p_{y}(y)}=\max _{p_{\mathbf{x}}} I(x ; y)=C 
\end{aligned}</script><p><strong>Definition</strong>: 对一个模型 $p_{\mathsf{y|x}}$，有</p>
<script type="math/tex; mode=display">
C \triangleq \max _{p_{x}} I(x ; y)</script><ul>
<li><p><strong>Model capacity</strong>: C</p>
</li>
<li><p><strong>least informative prior</strong>: $p_x^*$</p>
</li>
</ul>
<p><strong>Theorem</strong>(Equidistance property): C对应的最优的 $p^<em>$ 和 $w^</em>$ 满足</p>
<script type="math/tex; mode=display">
D(p_y(\cdot;x)||q^*(\cdot)) \le C \ \ \ \ \ \forall x\in\mathcal{X}</script><p>其中等号对于满足 $w^*(x)&gt;0$ 的 x 成立</p>
<p><strong>Proof</strong>: </p>
<ol>
<li>$I(x,y)$ 关于 $p_x(a)\ \ \forall a$  是 concave 的</li>
<li>构造拉格朗日函数 $\mathcal{L}=I(x,y) - \lambda(\sum_x p_x(x)-1)$，也关于 $p_x(a)$ concave</li>
<li>$\min_{p_x}I(x,y)$ 的极值点应满足 $\left.\frac{\partial I(x ; y)}{\partial p_{x}(a)}\right|_{p_{x}=p_{x}^{<em>}}-\lambda=0, \quad \text { for all } a \in \mathcal{X} \text { such that } p_{x}^{</em>}(a)&gt;0$，或者 $\left.\frac{\partial I(x ; y)}{\partial p_{x}(a)}\right|_{p_{x}=p_{x}^{<em>}}-\lambda\le0, \quad \text { for all } a \in \mathcal{X} \text { such that } p_{x}^{</em>}(a)=0$</li>
<li>$\frac{\partial I(x ; y)}{\partial p_{x}(a)} = D\left(p_{y | x}(\cdot ; a) | p_{y}\right)-\log e$ 并根据 3 中取等号的特点恰好可以得到定理中的式子</li>
</ol>
</blockquote>
<h2 id="3-Inference-with-mixture-models"><a href="#3-Inference-with-mixture-models" class="headerlink" title="3. Inference with mixture models"></a>3. Inference with mixture models</h2><ul>
<li><p>Formulation: 有观测 $y_-$，想要预测 $y_+$</p>
</li>
<li><p>Solution</p>
<ul>
<li><p>根据前面得到的最优先验 $w^*$ 来估计 $y=[y_-,y_+]$ 的分布</p>
<script type="math/tex; mode=display">
q_{\mathbf{y}}^{*}(\mathbf{y})=\sum_{x} w^{*}(x) p_{\mathbf{y}}(\mathbf{y} ; x)</script></li>
<li><p>然后可以获得后验概率</p>
<script type="math/tex; mode=display">
\begin{aligned} q_{\mathrm{y}+| \mathrm{y}_{-}}^{*}\left(\cdot | y_{-}\right) & \triangleq \frac{q_{\mathrm{y}}^{*}\left(y_{+}, y_{-}\right)}{q_{\mathrm{y}-}^{*}\left(y_{-}\right)}=\frac{\sum_{x} w^{*}(x) p_{\mathrm{y}}\left(y_{+}, y_{-} ; x\right)}{\sum_{a} w^{*}(a) p_{\mathrm{y}_{-}}\left(y_{-} ; a\right)} \\ 
&=\sum_{x} w^{*}\left(x | y_{-}\right) p_{\mathrm{y}_{+} | y_{-}}\left(y_{+} | y_{-} ; x\right) \end{aligned}</script></li>
<li><p>相当于是做了 soft decision，因为 ML 估计中只会取 $p_{\mathrm{y}_{+} | y_{-}}(\cdot|y_-; \hat{x}_{ML})$ </p>
</li>
</ul>
</li>
</ul>
<h2 id="4-Maximum-entropy-distribution"><a href="#4-Maximum-entropy-distribution" class="headerlink" title="4. Maximum entropy distribution"></a>4. Maximum entropy distribution</h2><ul>
<li>最大熵等价于<strong>均匀分布</strong>向对应的模型集合上的 <strong>I-projection</strong><script type="math/tex; mode=display">
D(p \| U)=\sum_{y} p(y) \log p(y)+\log |\mathcal{Y}|=\log |\mathcal{Y}|-H(p) \\
p^{*}=\underset{p \in \mathcal{L}_{\mathrm{t}}}{\arg \max } H(p)=\underset{p \in \mathcal{L}_{\mathrm{t}}}{\arg \min } D(p \| U)</script></li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>熵</tag>
        <tag>信息容量</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(五)  EM algorithm</title>
    <url>/2020/02/03/statistic/SI_Ch5_EMAlgorithm/</url>
    <content><![CDATA[<p>EM 算法</p>
<a id="more"></a>
<h2 id="1-EM-ML-algorithm"><a href="#1-EM-ML-algorithm" class="headerlink" title="1. EM-ML algorithm"></a>1. EM-ML algorithm</h2><ul>
<li>formulation<ul>
<li>complete data : $\mathsf{z=[y,w]}$</li>
<li>observation : $\boldsymbol{y}$</li>
<li>hidden variable : $\boldsymbol{w}$</li>
<li>estimation : $\mathcal{x}$</li>
</ul>
</li>
<li>Derivation </li>
</ul>
<blockquote>
<p>期望获得 ML 估计，但是实际中 $p(y;x)$ 可能很难计算(比如 mixture gaussian model，相乘后再求和)</p>
<script type="math/tex; mode=display">
\hat{x}_{ML}(y)=\arg\max_x \ln p(y;x) \\</script><p>引入 complete data $\mathsf{z=[y,w]}$，令 $y=g(z)$</p>
<script type="math/tex; mode=display">
p(z;x)=\sum_y p(z|y;x)p(y;x)=p(z|g(z);x)p(g(z);x) \\
\hat{x}_{ML}(y) = \arg\max_x \ln p(z;x) - \ln p(z|y;x)</script><p>由于 $\hat{x}_{ML}(y)$ 与 z 无关，因此右边可以对 $p(z|y;x’)$ 求期望</p>
<script type="math/tex; mode=display">
\begin{align}
\ln p(y;x) &= \ln p(z;x) - \ln p(z|y;x) \\
&= \mathbb{E}[\ln p(z;x)|\mathsf{y}=y;x'] - \mathbb{E}[\ln p(z|y;x)|\mathsf{y}=y;x'] \\
&= U(x,x') + V(x,x')
\end{align}</script><p>其中 $V(x,x’)$ 根据 Gibbs 不等式可以知道恒有 $V(x,x’) \ge V(x’,x’)$</p>
<p>因此要使 $\ln p(y;x)$ 最大，只需要使 $U(x,x’)$ 最大(可以放松要求，只要每次$U(x,x’)$增大就可以了，这就是<strong>Generalized EM</strong>)</p>
<p><strong>E-step</strong>: compute $U(x,\hat{x}^{n-1})=\mathbb{E}[\ln p_\mathsf{z}(z;x)|\mathsf{y}=y;\hat{x}^{n-1}]$</p>
<p><strong>M-step</strong>: maximize $\hat{x}^n = \arg\max_x U(x,\hat{x}|^{n-1})$</p>
</blockquote>
<h3 id="EM-MAP-推导：待完成！"><a href="#EM-MAP-推导：待完成！" class="headerlink" title="EM-MAP 推导：待完成！"></a><strong>EM-MAP 推导</strong>：待完成！</h3><h2 id="2-EM-for-linear-exponential-family"><a href="#2-EM-for-linear-exponential-family" class="headerlink" title="2. EM for linear exponential family"></a>2. EM for linear exponential family</h2><ul>
<li>derivation</li>
</ul>
<blockquote>
<p>指数分布</p>
<script type="math/tex; mode=display">
p(z;x)=\exp\left(x^Tt(z)-\alpha(x)+\beta(z)\right) \\
U(x,x^n) = \mathbb{E}[\ln p(z;x)|y;x^n] \\</script><p>EM 算法迭代过程中每次要找 $U(x,x’)$ 的最大值点，因此有</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial x}U(x,\hat{x}^n) \Big|_{x=\hat{x}^{n+1}} = \mathbb{E}[t(z)|y;\hat{x}^n] - \mathbb{E}[\dot{\alpha}(x)|y;\hat{x}^n] =\mathbb{E}[t(z)|y;\hat{x}^n] -  \dot{\alpha}(x)|_{x=\hat{x}^{n+1}}=0</script><p>同时由于 linear exponential family 本身的性质，有</p>
<script type="math/tex; mode=display">
\mathbb{E}[t(z);\hat{x}^{n+1}] = \dot{\alpha}(x)|_{x=\hat{x}^{n+1}}</script><p>因此实际上每一步迭代过程中都满足</p>
<script type="math/tex; mode=display">
\mathbb{E}[t(z);\hat{x}^{n+1}] = \mathbb{E}[t(z)|y;\hat{x}^n]</script><p>最终收敛于不动点</p>
<script type="math/tex; mode=display">
\mathbb{E}[t(z);\hat{x}^{*}] = \mathbb{E}[t(z)|y;\hat{x}^*]</script><p>此时有</p>
<script type="math/tex; mode=display">
\frac{\partial \ln p(y;x)}{\partial x} = ... = \mathbb{E}[t(z)|y;\hat{x}^*]-\mathbb{E}[t(z);\hat{x}^*] = 0</script></blockquote>
<h2 id="3-Empirical-ditribution"><a href="#3-Empirical-ditribution" class="headerlink" title="3. Empirical ditribution"></a>3. Empirical ditribution</h2><ul>
<li>observation: $\boldsymbol{y}=[y_1,…,y_N]^T$</li>
<li>empirical ditribution: $\hat{p}_\mathsf{y}(b;\boldsymbol{y}) = \frac{1}{N}\sum_n \mathbb{I}_b(y_n)$</li>
</ul>
<blockquote>
<p><strong>Porperties 1</strong>: $\frac{1}{N}\sum_n f(y_n)=\sum_y f(y)\hat{p}_\mathsf{y}(y;\boldsymbol{y})$</p>
<p><strong>Properties 2</strong>: Let the set of models be $\mathcal{P}=\{p_y(\cdot;x),x\in \mathcal{X}\}$, then the <strong>ML</strong> can be written as </p>
<script type="math/tex; mode=display">
\hat{x}_{ML}(y)=\arg\min_{p\in\mathcal{P}}D(\hat{p}_y(\cdot;y) || p) = \arg\min_{x\in\mathcal{X}}D(\hat{p}_y(\cdot;y) || p(\cdot;x))</script><p>which is the <strong>reverse I-projection</strong></p>
<p><strong>Remark</strong>：</p>
<ol>
<li>这个性质表明<strong>最大似然</strong>实际上是在找与<strong>经验分布</strong>最接近(相似)的分布对应的参数</li>
<li><strong>给定经验分布(观察)后，实际上就相当于给定了一个线性族(想一下对应的 $t_k(y)$ 的如何表示，提示：用元素为1或0的矩阵)</strong>，这个在此处适用，在后面对 $p_z$ 的约束也适用</li>
<li>求 <strong>ML</strong> 就是在求<strong>逆投影(reverse I-proj)</strong>，这对后面理解 EM 算法的 alernating projcetions 有用</li>
</ol>
</blockquote>
<h2 id="4-EM-ML-Alternating-projections"><a href="#4-EM-ML-Alternating-projections" class="headerlink" title="4. EM-ML Alternating projections"></a>4. EM-ML Alternating projections</h2><blockquote>
<p> 根据 #3 中的性质2可以获得 ML 的表达式，但是该式子过于复杂，考虑</p>
<p> <strong>DPI</strong>(Data processing inequality): $y=g(z)$</p>
<script type="math/tex; mode=display">
 D(p(z)||q(z)) \ge D(p(y)||q(y)) \\
 "=" \iff \frac{p_z(z)}{q_z(z)} = \frac{p_y(g(z))}{q_y(g(z))}\ \ \ \ \forall z</script><p> 因此根据(12)式要想最小化 $D(\hat{p}_y(\cdot;\boldsymbol{y}) || p(y;x))$ 可以考虑最小化 $D(\hat{p}_z(\cdot;\boldsymbol{z}) || p(z;x))$</p>
<p> 因为 $p(y;x)$ 的表达式很可能很复杂，但是 $p(z;x)$ 可以简化很多</p>
<p> <strong>即最大似然转化为<em>最小化</em></strong></p>
<script type="math/tex; mode=display">
 \min D(\hat{p}_z(\cdot;z) || p(\cdot;x))</script><script type="math/tex; mode=display">
 \mathcal{P^Z}(y) \triangleq \left\{ \hat{p}_Z(\cdot): \sum_{g(c)=y} \hat{p}_z(c) = \hat{p}_y(b;\boldsymbol{y})\ \ \ \forall b\in \mathcal{y} \right\} \\</script><blockquote>
<p><strong>Remarks</strong>：这里最小化过程中对两个分布都要考虑：</p>
<ol>
<li>由于 $\hat{p}_z$ 实际上在一定约束下(<strong>线性分布族</strong>，参考 #3 中的 reverse I-proj)可以任取，因此要优化 $\hat{p}_z$ 使散度最小；</li>
<li>由于 $p(\cdot;x)$ 实际上就是我们要求的东西(我们要找到一个 x 使观测值 y 的似然最大)，因此也要优化 $p(\cdot;x)$ 使散度最小；</li>
</ol>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/EM-ML.jpg" alt="EM-ML"></p>
</blockquote>
<p> 要想最小化 (14) 式，可以分解为 2 步：</p>
<ol>
<li><p>第一步(<strong>I-projection</strong>)</p>
<script type="math/tex; mode=display">
\hat{p}_{z}^{*}(\cdot ; \hat{x}^{(n-1)})=\underset{\hat{p}_{z} \in \hat{\mathcal{P^Z}}(\mathbf{y})}{\arg \min } D\left(\hat{p}_{z}(\cdot) \| p_{z}(\cdot ; \hat{x}^{(n-1)})\right)</script></li>
<li><p>第二步(<strong>reverse I-projection</strong>)</p>
<script type="math/tex; mode=display">
\hat{x}_{ML}^{(n)} = \underset{x}{\arg \min } D\left(\hat{p}_{z}^{*}\left(\cdot ; \hat{x}^{(n-1)}\right) \| p_{z}(\cdot ; x)\right)</script><p>这实际上就是 EM-ML 算法，证明如下：</p>
<p>上面两步分别对应 EM 中的 E-step 和 M-step</p>
<p>E-step：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{1}{N}U(x,\hat{x}^{(n-1)}) &= \frac{1}{N}\mathbb{E}\left[\ln p(\boldsymbol{z};x)|\boldsymbol{y};\hat{x}^{(n-1)}\right] \\ 
&= \frac{1}{N}\sum_n \mathbb{E}\left[\ln p(z_n;x)|\boldsymbol{y};\hat{x}^{(n-1)}\right] \\
&= \frac{1}{N}\sum_n \sum_z \ln p(z;x) p\left(z|y_n;\hat{x}^{(n-1)}\right) \\
&= \sum_y \hat{p}_y(y;\boldsymbol{y}) \sum_z p\left(z|y;\hat{x}^{(n-1)}\right)\ln p(z;x) \\
&= \sum_y \hat{p}_y(y;\boldsymbol{y}) \sum_z \frac{\hat{p}^*(z;\hat{x}^{(n-1)})}{\hat{p}_y(g(z);\boldsymbol{y})} \ln p_z(z;x) \\
&= \sum_z \hat{p}^*(z;\hat{x}^{(n-1)}) \ln p_z(z;x) \\
&= -D\left(\hat{p}^*(z;\hat{x}^{(n-1)})||p(z;x)\right) - H\left(\hat{p}_Z^*(z;\hat{x}^{(n-1)})\right)
\end{align}</script><p>M-step：</p>
<p><strong>Remarks</strong>: </p>
</li>
<li><p>EM-ML 即在第二步中采用 ML 来估计 x，由于 ML 本身即与 reverse I-projection 等价，因此整体就是不断地在相互投影；</p>
</li>
<li><p>如果是 EM-MAP 就只需要将 M-step 中的 ML 估计换成 MAP 估计，但是由于 MAP 估计当中先验对于整个分布族会产生一个加权，计算复杂且没有闭式解</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/EM-MAP.jpg" alt="EM-MAP"></p>
</li>
</ol>
</blockquote>
<h2 id="5-Remarks"><a href="#5-Remarks" class="headerlink" title="5. Remarks"></a>5. Remarks</h2><blockquote>
<p>EM 算法实质上可以看作一个<strong>升维</strong>的处理过。这是指将低维空间中的 $\mathcal{Y}$ 映射到高维空间 $\mathcal{Z}$ 中</p>
<ol>
<li>根据 $y$ 的 empirical distribution，在 $\mathcal{P^Z}$ 中同样得到一个约束 $z$ 的线性族</li>
<li>由预先定义的模型 $p(z|\theta)$ 再指定另一个 $\mathcal{P^Z}$ 中的集合，比如线性指数族</li>
</ol>
<p>最终的目标是在 $\mathcal{P^Z}$ 空间中获得一个最大似然估计，即 $\hat{\theta}_{ML} = \arg\min D(\hat{p}_z || p(z,\theta))$。<br>因此整个 EM 算法就是重复下面的过程</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/EM-proj.jpg" alt="EM-proj"></p>
</blockquote>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>参数估计</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(四) Information Geometry</title>
    <url>/2020/02/03/statistic/SI_Ch4_InformationGeometry/</url>
    <content><![CDATA[<p>信息几何</p>
<a id="more"></a>
<h2 id="1-Generalized-Bayesian-decision"><a href="#1-Generalized-Bayesian-decision" class="headerlink" title="1. Generalized Bayesian decision"></a>1. Generalized Bayesian decision</h2><ul>
<li><p>Formulation</p>
<ul>
<li>Soft decision: $q_x(\cdot|y)$</li>
<li>Cost function: $C(x,q_x)$</li>
</ul>
</li>
<li><p>Cost function</p>
<ul>
<li><strong>proper</strong>: $p_{x|y}(\cdot|y)=\arg\min\limits_{\{q_x\ge0:\sum_a q(a)=1\}} E[C(x,q_x(\cdot))|\mathsf{y}=y]$</li>
<li><strong>local</strong>: $C(x,q_x)=\phi(x,q_x(x))$</li>
</ul>
</li>
<li><p>Log-loss criterion: $C(x,q)=-A\log q_x(x) + B(x), \ \ \ A&gt;0$</p>
<ul>
<li><em>proper</em> and <em>local</em></li>
</ul>
<blockquote>
<p><strong>Theorem</strong>: When the alphabet $\mathcal{X}$ consists of at least 3 values ($|\mathcal{X}| \triangleq L ≥ 3$), then the log-loss is the <strong>only</strong> smooth local, proper cost function. </p>
<p><strong>Proof</strong>: Let $q_{l} \triangleq q_{\times}\left(x_{l}\right), p_{l} \triangleq p_{x | y}\left(x_{l} | y\right), \phi_{l}(\cdot) \triangleq \phi\left(x_{l}, \cdot\right)$</p>
<ul>
<li><script type="math/tex; mode=display">
proper \Longrightarrow p_{1}, \ldots, p_{L}=\underset{\left\{q_{1}, \ldots, q_{L} \geq 0: \sum_{l=1}^{L} q_{l}=1\right\}} {\arg\min} \sum_{l=1}^{L} p_{l} \phi_{l}\left(q_{l}\right)</script></li>
<li><script type="math/tex; mode=display">
拉格朗日乘子法 \Longrightarrow 
p_{1}, \ldots, p_{L}=\underset{q_{1}, \ldots, q_{L}}{\arg \min } \varphi, \quad \text { with } \varphi=\sum_{l=1}^{L} p_{l} \phi_{l}\left(q_{l}\right)+\lambda\left(p_{1}, \ldots, p_{L}\right)\left[\sum_{l=1}^{L} q_{l}-1\right]</script></li>
<li><script type="math/tex; mode=display">
proper \Longrightarrow \left.\frac{\partial \varphi}{\partial q_{k}}\right|_{q=p_{l}, l=1, \ldots, L}=p_{k} \dot{\phi}_{k}\left(p_{k}\right)+\lambda\left(p_{1}, \ldots, p_{L}\right)=0, \quad k=1, \ldots, L</script></li>
<li><p>由 locality 可推出 $\lambda$ 为常数，$\phi_k(q)=-\lambda \ln q + c_k, \ \ \ k=1,…,L$</p>
</li>
</ul>
</blockquote>
</li>
<li><p>Gibbs inequality</p>
<script type="math/tex; mode=display">
if \ \ \ x\sim p_x(\cdot),\ \ \  \forall q(\cdot) \\
we\ \ have \ \ E_x[\log p(x)] \ge E[\log q(x)] \\
\sum_x p(x)\log p(x) \ge \sum_x p(x)\log q(x) \\
"=" \iff p(x)=q(x)</script></li>
</ul>
<h2 id="2-Discrete-information-theory"><a href="#2-Discrete-information-theory" class="headerlink" title="2. Discrete information theory"></a>2. Discrete information theory</h2><ul>
<li><strong>Entropy</strong>: $H(\mathrm{x}) \triangleq \min _{q_{\mathrm{x}}} \mathbb{E}\left[C\left(\mathrm{x}, q_{\mathrm{x}}\right)\right]$</li>
<li><strong>Conditional entropy</strong>: $H(\mathrm{x} | \mathrm{y}) \triangleq \sum_{y} p_{\mathrm{y}}(y) H(\mathrm{x} | \mathrm{y}=y)$<br>$H(x | y=y) \triangleq \min _{q_{x}} \mathbb{E}\left[C\left(x, q_{x}\right) | y=y\right]$</li>
<li><strong>Mutual information</strong>: $I(\mathrm{x} ; \mathrm{y}) \triangleq H(\mathrm{x})-H(\mathrm{x} | \mathrm{y}) = H(x)+H(y)-H(xy)$</li>
<li><strong>Conditional mutual information</strong>: $I(\mathrm{x} ; \mathrm{y} | \mathrm{z}) \triangleq H(\mathrm{x} | \mathrm{z})-H(\mathrm{x} | \mathrm{y}, \mathrm{z})$</li>
<li>Chain rule: $I(x ; y, z)=I(x ; z)+I(x ; y | z)$</li>
<li><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/venn.jpg" alt="venn"></p>
</li>
<li><p><strong>Information divergence</strong>(KL distance)</p>
<ul>
<li>Definition</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned} D\left(p_{\times} \| q_{\mathbf{x}}\right) & \triangleq \mathbb{E}_{p_{\mathbf{x}}}\left[-\log q_{\mathbf{x}}(\mathbf{x})\right]-\mathbb{E}_{p_{\mathbf{x}}}\left[-\log p_{\mathbf{x}}(\mathbf{x})\right] \\ &=\sum_{a} p_{\mathbf{x}}(a) \log \frac{p_{\mathbf{x}}(a)}{q_{\mathbf{x}}(a)} \end{aligned}</script><ul>
<li><p>Properties</p>
<ul>
<li><p>$\ge 0$（<em>只有 p=q 的时候才能取 = 吗？</em>）</p>
</li>
<li><p>$I(x;y) = D(p_{x,y}||p_x p_y)$</p>
</li>
<li><script type="math/tex; mode=display">
\lim _{\delta \rightarrow 0} \frac{D\left(p_{y}(\cdot ; x) \| p_{y}(\cdot ; x+\delta)\right)}{\delta^{2}}=\frac{1}{2} J_{y}(x)​</script></li>
</ul>
</li>
</ul>
</li>
<li><p>Data processing inequality (<strong>DPI</strong>)</p>
</li>
</ul>
<blockquote>
<p><strong>Theorem</strong>: if $x \leftrightarrow y \leftrightarrow t$ is a <strong>Markov</strong> chain, then </p>
<script type="math/tex; mode=display">
I(x;y) \ge I(x;t)</script><p>with “=” $\iff$ $x \leftrightarrow t \leftrightarrow y$ is a Markov chain</p>
<p><strong>Corollary</strong>: deterministic $g(\cdot)$, $I(x;y) \ge I(x;g(y))$</p>
<p><strong>Corollary</strong>: t=t(y) is <strong>sufficient</strong> $\iff I(x;y)=I(x;t)$</p>
<p><strong>Proof</strong>: 应用互信息链式法则</p>
<p><strong>Remark</strong>: 证明不等式的时候注意取等号的条件 $I(x;y|t)=0$</p>
<hr>
<p><strong>Theorem</strong>: 若 $q_{\mathrm{x}^{\prime}}(b)=\sum_{a \in \mathcal{X}} W(b | a) p_{\mathrm{x}}(a), \quad q_{\mathrm{y}^{\prime}}(b)=\sum_{a \in \mathcal{X}} W(b | a) p_{\mathrm{y}}(a)$<br>那么对任意 $W(\cdot|\cdot)$ 有 $D(q_{x’}||q_{y’}) \le D(p_x||p_y)$</p>
<p><strong>Proof</strong>: 待完成 …</p>
<p><strong>Theorem</strong>: 对确定性函数 $\phi(\cdot)$，$\mathsf{w}=\phi(\mathsf{z})$，有 $J_{\mathsf{w}}(x)=J_{\mathsf{z}}(x)$</p>
<p><strong>Proof</strong>: 待完成 …</p>
</blockquote>
<h2 id="3-Information-geometry"><a href="#3-Information-geometry" class="headerlink" title="3. Information geometry"></a>3. Information geometry</h2><ul>
<li><p>Probability simplex</p>
<ul>
<li>若字符集有 M 个字符，则概率单形为 <strong>M-1 维</strong>的超平面，且只位于第一象限</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/probability_simplex.jpg" alt="probability_simplex"></p>
</li>
<li><p>Boundary</p>
<ul>
<li>根据 p,q 是否位于边界(即存在 $p(y’)=0$) 可决定 $D(p||q)&lt;\infty$ 还是 $D(p||q)=\infty$</li>
</ul>
</li>
<li><p>Local information geometry</p>
<blockquote>
<p>取 $p_0 \in \text{int}(\mathcal{P^Y})$，对任意分布(向量) $p$ 定义其归一化表示</p>
<script type="math/tex; mode=display">
\phi(y)=\frac{p(y)-p_0(y)}{\sqrt{2p_0(y)}}</script><p>$p_0$ 的邻域被定义为一个球</p>
<script type="math/tex; mode=display">
\{p: |\phi_p(y)|\le B, \ \ \forall y \}</script><p>那么对小邻域内的两个分布 $p_1,p_2$ 有</p>
<script type="math/tex; mode=display">
D(p_1 || p_2) = \sum_y |\phi_1(y)-\phi_2(y)|^2(1+o(1)) \approx ||\phi_1-\phi_2||^2</script><p>证明：代入散度公式，应用泰勒级数展开化简。其中需要注意到</p>
<script type="math/tex; mode=display">
\sum_y \sqrt{2p_0(y)}\phi(y)=\sum_y p(y)-p_0(y) = 0</script><p><strong>Remark</strong>：直观理解就是小邻域内散度近似为欧氏距离</p>
</blockquote>
</li>
</ul>
<h2 id="4-Information-projection"><a href="#4-Information-projection" class="headerlink" title="4. Information projection"></a>4. Information projection</h2><ul>
<li><p>Definition: q 向<strong>闭集</strong> $\mathcal{P}$ 内的投影 $p*=\arg\min_{p\in\mathcal{P}}D(p||q)$</p>
<ul>
<li>存在性：由于 $D(p||q)$ 非负且对 p 连续，而 $\mathcal{P}$ 非空且为闭集，因此一定存在</li>
<li>唯一性：不一定唯一，但如果 $\mathcal{P}$ 为<strong>凸集</strong>，则 p* 唯一</li>
</ul>
</li>
<li><p>Pythagoras’ Theorem</p>
</li>
</ul>
<blockquote>
<p><strong>Theorem(Pythagoras’ Theorem)</strong>: p<em> 是 q 向非空<em>*闭凸集</em></em> $\mathcal{P}$ 上的投影，那么任意 $p\in\mathcal{P}$ 有</p>
<script type="math/tex; mode=display">
D(p||q) \ge D(p||p^*) + D(p^*||q)</script><p><strong>Proof</strong>: 取 $p_{\lambda}=\lambda p + (1-\lambda)p^* \in \mathcal{P}$</p>
<p>由投影定义可知 $\frac{\partial}{\partial \lambda} D(p_\lambda||q) \Big|_{\lambda=0} \ge 0$</p>
<p>代入化简可得证</p>
<p><strong>Remark</strong>: 直观理解就是不可能通过多次中间投影，使整体的<strong>KL距离</strong>(散度)减小</p>
<hr>
<p><strong>Corollary</strong>: 如果 q 不在 $\mathcal{P^y}$ 的边界上，那么其在<strong>线性分布族</strong> $\mathcal{P}$ 上的投影 $p^*$ 也不可能在 $\mathcal{P^y}$ 的边界上，除非 $\mathcal{P}$ 中的所有元素都在某个边界上</p>
<p><strong>Proof</strong>: 应用散度的 Boundary、毕达哥拉斯定理</p>
</blockquote>
<ul>
<li><p><strong>Linear families</strong></p>
<ul>
<li><p>Definition: $\mathcal{L}$ 是一个线性分布族，如果对于一组映射函数 $t(\cdot)=[t_1(), …, t_K()]^T$ 和对应的常数 $\bar t = [\bar t_1, …, \bar t_K]^T$，有 $\mathbb{E}_{p}\left[t_{i}(\mathrm{y})\right]=\bar{t}_{i}, \quad i=1, \ldots, K \quad$ for all $p \in \mathcal{L}$</p>
<script type="math/tex; mode=display">
\underbrace{\left[\begin{array}{ccc}{t_{1}(1)-\bar{t}_{1}} & {\cdots} & {t_{1}(M)-\bar{t}_{1}} \\ {\vdots} & {\ddots} & {\vdots} \\ {t_{K}(1)-\bar{t}_{K}} & {\cdots} & {t_{K}(M)-\bar{t}_{K}}\end{array}\right]}_{\triangleq \mathbf{T}}\left[\begin{array}{c}{p(1)} \\ {\vdots} \\ {p(M)}\end{array}\right]=\mathbf{0}</script></li>
<li><p>性质</p>
<ul>
<li>$\mathcal{L}$ 的维度为 M-rank(T)-1</li>
<li>$\mathcal{L}$ 是一个闭集、凸集</li>
<li>$p_1,p_2 \in \mathcal{L}$，那么 $p=\lambda p_{1}+(1-\lambda) p_{2} \in \mathcal{P}^{\mathcal{Y}}, \ \ \lambda\in R$，注意 $\lambda$ 可以取 [0,1] 之外的数</li>
</ul>
<blockquote>
<p><strong>Theorem(Pythagoras’ Identity)</strong>: q 向线性分布族 $\mathcal{L}$ 的投影 $p^*$ 满足以下性质</p>
<script type="math/tex; mode=display">
D(p \| q)=D\left(p \| p^{*}\right)+D\left(p^{*} \| q\right), \quad \text { for all } p \in \mathcal{L}</script><p><strong>Proof</strong>: 类似前面不等式的证明，只不过现在由于 $\lambda\in R$ 所以不等号变成了等号</p>
<p><strong>Theorem(Orthogonal families)</strong>:  $p<em>\in\mathcal{P^Y}$ 为任一分布，则向线性分布族 $\mathcal{L_t}(p^</em>)$ 的投影为 $p^<em>$ 的所有分布都属于一个指数分布 $\mathcal{\varepsilon}_t(p^</em>)$</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathbf{t}}\left(p^{*}\right) \triangleq\left\{p \in \mathcal{P}^{\mathcal{Y}}: \mathbb{E}_{p}[\mathbf{t}(\mathbf{y})]=\overline{\mathbf{t}} \triangleq \mathbb{E}_{p^{*}}[\mathbf{t}(\mathbf{y})]\right\} \\

\begin{aligned} \mathcal{E}_{\mathbf{t}}\left(p^{*}\right) \triangleq\left\{q \in \mathcal{P}^{\mathcal{Y}}: q(y)=p^{*}(y) \exp \left\{\mathbf{x}^{\mathrm{T}} \mathbf{t}(y)-\alpha(\mathbf{x})\right\}\right.\\ \text { for all }\left.y \in \mathcal{Y}, \text { some } \mathbf{x} \in \mathbb{R}^{K}\right\} \end{aligned}</script><p>其中需要注意的是 $\mathcal{L}_{\mathbf{t}}\left(p^{<em>}\right),\mathcal{E}_{\mathbf{t}}\left(p^{</em>}\right)$ 的表达形式并不唯一，括号内的 $p^*$ 均可以替换为对应集合内的任意一个其他分布，他们表示的是同一个集合</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/exp_projection_linear.jpg" alt="exp_projection_linear"></p>
<p><strong>Remarks</strong>：</p>
<ol>
<li>根据上面的定理，可以由 $t(\cdot), \bar t$ 求出 q 向线性分布族的投影 p*</li>
<li>在小邻域范围内，可以发现 $\mathcal{L}_{\mathbf{t}}\left(p^{<em>}\right),\mathcal{E}_{\mathbf{t}}\left(p^{</em>}\right)$ 的正规化表示 $\phi_\mathcal{L}^T \phi_\mathcal{E}=0$，即二者是正交的</li>
</ol>
</blockquote>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>信息几何</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(三) Exponential Family</title>
    <url>/2020/02/03/statistic/SI_Ch3_ExponentialFamily/</url>
    <content><![CDATA[<p>指数族</p>
<a id="more"></a>
<h2 id="1-Exponential-family"><a href="#1-Exponential-family" class="headerlink" title="1. Exponential family"></a>1. Exponential family</h2><ul>
<li><p>Definition</p>
<ul>
<li>PDF: $p(y;x)=\exp(\lambda(x)^T t(y)-\alpha(x)+\beta(y))$<br>$y\sim \varepsilon(x;\lambda(\cdot),t(\cdot),\beta(\cdot))$</li>
<li>nature statistic: $t(y)$</li>
<li>nature parameter: $\lambda(x)$</li>
<li>log-partition function: $\alpha(x)$</li>
<li>partition function: $Z(x)=\exp(\alpha(x))$</li>
<li>distribution: $\exp(\beta(y))$</li>
</ul>
</li>
<li><p><strong>正则条件(regular)</strong>：若分布族中的任意一个分布 $p(y;x)$ 都有其支集(support)与 x 无关，则为正则</p>
<ul>
<li>实质上是要求 CRB 正则条件中<strong>求导和积分可换序</strong> <script type="math/tex; mode=display">
\mathbb{E}\left[\frac{\partial}{\partial x}\ln p(y;x)\right]=\int\frac{\partial}{\partial x}p(y;x)dy = \frac{\partial}{\partial x}\int_a^b p(y;x)dy = 0</script></li>
</ul>
</li>
<li><p>指数分布族可以有多种获得方式</p>
<ul>
<li><p>很多分布本身可以写成指数分布族形式</p>
<ul>
<li>Bernulli distribution: $y\sim \mathcal{B}(x)$</li>
</ul>
<script type="math/tex; mode=display">
p(y;x)=x^y (1-x)^{(1-y)} \\
\ln p(y;x)=\left(\ln(\frac{x}{1-x})\right)y-(-\ln(1-x))</script><ul>
<li>Gaussian $y=[y_1,y_2]^T\sim \mathcal{N}(x,1)$<script type="math/tex; mode=display">
p(y;x)=\frac{1}{\sqrt{2\pi}}\exp\left((y_1+y_2)x-x^2-\frac{y_1^2+y_2^2}{2}\right)</script></li>
</ul>
</li>
<li><p>多个分布的<strong>几何均值</strong></p>
<script type="math/tex; mode=display">
p(y;x)=\frac{p_1^x(y)*p_2^{(1-x)}(y)}{Z(x)} \\
\ln p(y;x)=x\ln\left(\frac{p_1(y)}{p_2(y)}\right)-\ln Z(x)+\ln p_2(y)</script><ul>
<li>例如 $p_1(y)\sim \mathcal{B}(\frac{1}{1+e^{-1}}),  p_2(y)\sim \mathcal{B}(1/2)$<script type="math/tex; mode=display">
p(y;x)=(\frac{1}{1+e^{-1}})^{xy}(\frac{e^{-1}}{1+e^{-1}})^{x(1-y)}(1/2)^{(1-x)}\sim \mathcal{B}(\frac{1}{1+e^{-x}}) \\
\frac{p(y=1;x)}{p(y=0;x)}=e^x</script></li>
</ul>
</li>
<li><p><strong>Tilting</strong></p>
<script type="math/tex; mode=display">
p(y;x)=\frac{p(y)e^{xy}}{Z(x)} \\
\ln p(y;x)=xy - \ln Z(x) + \ln p(y)</script><ul>
<li>例如 $p(y)\sim \mathcal{N}(0,1)$，$p(y;x)\sim \mathcal{N}(x,1)$</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>linear exponential family</strong></p>
<ul>
<li>定义：$t(x)=x$，$\ln p(y;x)=x\ t(y) - \alpha(x)+\beta(y)$</li>
<li>性质：$\dot{\alpha}(x)=\mathbb{E}[t(y)], \ \ \dot{\dot{\alpha}}(x)=\mathbb{E}[t^2(y)]-\mathbb{E}[t(y)]^2=Var(t(y)) = J_y(x)$</li>
</ul>
<blockquote>
<p><strong>Proof</strong>：</p>
<script type="math/tex; mode=display">
\begin{align}
Z(x) &= e^{\alpha(x)}=\int e^{x t(y)+\beta(y)}dy \\
\frac{\partial}{\partial x}Z(x) &= e^{\alpha(x)}\cdot \dot\alpha(x) = \int t(y)e^{xt(y)+\beta(y)}dy \\
\dot{\alpha}(x) &= \int t(y)p(y;x)dy = \mathbb{E}[t(y)]
\end{align}</script><script type="math/tex; mode=display">
\dot{\dot{\alpha}}(x)=\int t(y)\cdot p(y;x)\cdot (t(y)-\dot{\alpha}(x))dy \\
J_y(x) = \mathbb{E}\left[-\frac{\partial^2}{\partial x^2} \ln p(y;x)\right]=\dot{\dot{\alpha}}(x)</script></blockquote>
</li>
<li><p>指数族分布与有效统计量(efficient statistics)</p>
<ul>
<li><strong>必要条件</strong>：若有效统计量存在，则可以写成指数族分布形式，且有<script type="math/tex; mode=display">
t(x)=\int^x J_y(u)du, \ \ \ \alpha(x)=\int^x u J_y(u) du</script></li>
</ul>
<blockquote>
<p><strong>Proof</strong>：</p>
<script type="math/tex; mode=display">
\begin{align}
\hat {x}_{eff}(y) &= x+\frac{1}{J_y(x)}\frac{\partial}{\partial x}\ln p(y;x) \\
\frac{\partial}{\partial x}\ln p(y;x) &= J_y(x)\hat{x}_{eff}(y) - x J_y(x) \\
\ln p(y;x) &= \int^x J_y(u)du \cdot \hat{x}_{ML}(y) - \int^x u J_y(u) du
\end{align}</script></blockquote>
<ul>
<li><strong>充分条件</strong>：对于<strong>线性指数分布族</strong>，若有 $J_y(x)$ 不依赖于 x，也即 $J_y(x)$ 等于一个常数时，有效统计量存在</li>
</ul>
<blockquote>
<p><strong>Proof</strong>：$J_y(x)=J$</p>
<script type="math/tex; mode=display">
\dot{\dot{\alpha}}(x)=J, \ \ \ \dot{\alpha}(x)=Jx-c \\
\hat x_{eff}(y) = x + \frac{1}{J}\frac{\partial}{\partial x}\ln p(y;x) = x + \frac{1}{J} (t(y)-\dot{\alpha}(x)) = x + \frac{1}{J}(t(y)-Jx+c)=\frac{t(y)}{J}+\frac{c}{J}</script><p>由于 </p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial x}\ln p(y;x)|_{x=\hat x_{ML}} = 0 = t(y) - \dot{\alpha}(x)|_{x=\hat x_{ML}}</script><p>有</p>
<script type="math/tex; mode=display">
\hat x_{eff}(y) = c/J + \frac{1}{J}\dot{\alpha}(x)|_{x=\hat x_{ML}} = \hat x_{ML}(y)</script></blockquote>
</li>
</ul>
<h2 id="2-Sufficient-statistics"><a href="#2-Sufficient-statistics" class="headerlink" title="2. Sufficient statistics"></a>2. Sufficient statistics</h2><h3 id="2-1-Non-Bayesian-case"><a href="#2-1-Non-Bayesian-case" class="headerlink" title="2.1 Non-Bayesian case"></a>2.1 Non-Bayesian case</h3><ul>
<li>Definition：t(y) 是关于分布 $p_{\mathsf{y}}(\cdot;x)$ 的充分统计量，如果 $p(y|t(y);x)$ 与 x 无关</li>
</ul>
<blockquote>
<p><strong>Theorem 1</strong>(likelihood characterization)：</p>
<p>$t(y)$ is sufficient w.r.t $p(y;x)$ $\iff \ \frac{p_{y}(y;x)}{p_t(t(y);x)}$ doesn’t depend on x, for all x and y</p>
<p><strong>Proof</strong>：omit…</p>
<p><strong>Theorem 2</strong>(Neyman Factorization theorem)：</p>
<p>$t(y)$ is sufficient w.r.t $p(y;x)$ $\iff \ 存在a(\cdot,\cdot)和b(\cdot)使得 \ \  p(y;x)=a\left(t(y),x\right) \cdot b(y)$</p>
<p><strong>Proof</strong>：omit…</p>
</blockquote>
<ul>
<li><strong>minimum sufficient statistic</strong>：$t^<em>$ 是 minimal 的，如果对任意其他充分统计量 t ，都存在 g() 使得 $t^</em>=g(t)$</li>
<li><strong>complete</strong>：$t^<em>$ 是 complete 的如果对任意函数 $\phi(\cdot)$，有 $E[\phi(t^</em>(y))]=0 \ \ \forall x \iff \phi(\cdot) \equiv 0$</li>
</ul>
<blockquote>
<p><strong>Theorem</strong>：complete $\Longrightarrow$ minimal</p>
<p><strong>Proof</strong>：假设 t 为complete，s 为 minimal，存在 $s=g(t)$，$E[t]=E\left[E\left[t|s=s\right]\right]$</p>
<p>$E[t|s=s]=f(s)=f(g(t))=\tilde{f}(t)$</p>
<p>取 $\phi(t)=t-\tilde{f}(t)$，有 $E[\phi(t)] = 0$</p>
<p>根据 complete 的定义，有 $\phi(t)\equiv0 \Longrightarrow t = \tilde{f}(t)=f(s)$</p>
<p>故 t 也是 minimal</p>
</blockquote>
<h3 id="2-2-Bayesian-case"><a href="#2-2-Bayesian-case" class="headerlink" title="2.2 Bayesian case"></a>2.2 Bayesian case</h3><ul>
<li>Definition：t(y) 是关于分布 $p_{\mathsf{y,x}}(\cdot,\cdot)$ 的充分统计量，如果 $p_{\mathsf{y|t,x}}(y|t(y),x)=p_\mathsf{y|t}(y|t(y))$ 与 x 无关</li>
</ul>
<blockquote>
<p><strong>Theorem</strong>(Belief characterization)：</p>
<p>$t(y)$ is sufficient w.r.t $p(y,x)$ $\iff \ p(x|y)=p(x|t(y))$, for all x and y</p>
<p><strong>Proof</strong>：omit…</p>
<p><strong>Theorem</strong>(Neyman Factorization theorem)：</p>
<p>$t(y)$ is sufficient w.r.t $p(y,x)$ $\iff \ p(y|x)=p(t(y)|x)\cdot p(y|t(y))$, for all x and y</p>
<p><strong>Proof</strong>：omit…</p>
</blockquote>
<h2 id="3-Conjugate-priors"><a href="#3-Conjugate-priors" class="headerlink" title="3. Conjugate priors"></a>3. Conjugate priors</h2><ul>
<li>Idea: Given a model $p_\mathsf{y|x}$, look for a family of prior $p_\mathsf{x}$ such that the induced posterior $p_\mathsf{x|y}$ also in this family</li>
<li>Definition: a family of distribution $q(\cdot;\theta)$ is <strong>conjugate</strong> to a model $p_{y|x}$ if <ul>
<li>$p_{y|x}(y_1,…,y_N|x) \propto q(x;\theta)$</li>
<li>$q(x;\theta_1)q(x;\theta_2)\propto q(x;\theta_3)$</li>
</ul>
</li>
<li><strong>Theorem</strong>: 对于采样数 N，联合分布 $p^N_{y|x}()$ 有充分统计量，且其维度不依赖于 N，则对该模型存在共轭先验分布</li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>指数族</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(二) Estimation Problem</title>
    <url>/2020/02/03/statistic/SI_Ch2_Estimation/</url>
    <content><![CDATA[<p>参数估计</p>
<a id="more"></a>
<h2 id="1-Bayesian-parameter-estimation"><a href="#1-Bayesian-parameter-estimation" class="headerlink" title="1. Bayesian parameter estimation"></a>1. Bayesian parameter estimation</h2><ul>
<li><p>Formulation</p>
<ul>
<li>Prior distribution $p_{\mathsf{x}}(\cdot)$</li>
<li>Observation $p_{\mathsf{y|x}}(\cdot|\cdot)$</li>
<li>Cost $C(a,\hat a)$</li>
</ul>
</li>
<li><p>Solution</p>
<ul>
<li>$\hat x(\cdot) = \arg\min_{f(\cdot)} \mathbb E[C(x,f(y))]$</li>
<li>$\hat{\mathbf{x}}(\mathbf{y})=\underset{\mathbf{a}}{\arg \min } \int_{\mathcal{X}} C(\mathbf{x}, \mathbf{a}) p_{\mathbf{x} | \mathbf{y}}(\mathbf{x} | \mathbf{y}) \mathrm{d} \mathbf{x}$</li>
</ul>
</li>
<li><p>Specific case</p>
<ul>
<li><p><strong>MAE</strong>(Minimum absolute-error)</p>
<ul>
<li>$C(a,\hat a)=|a-\hat a|$</li>
<li>$\hat x$ is the <strong>median</strong> of the belief $p_{\mathsf{x|y}}(x|y)$</li>
</ul>
</li>
<li><p><strong>MAP</strong>(Maximum a posteriori)</p>
<ul>
<li><script type="math/tex; mode=display">C(a,\hat a) = \left\{
\begin{array}{ll}{1,} & {|a-\hat a|>\varepsilon} \\ 
{0,} & {otherwise}\end{array}\right.</script></li>
<li>$\hat x_{MAP}(y) = \arg \max_a p_{\mathsf{x|y}}(a|y)$</li>
</ul>
</li>
<li><p><strong>BLS</strong>(Bayes’ least-squares)</p>
<ul>
<li><p>$C(a,\hat a)=||a-\hat a||^2$</p>
</li>
<li><p>$\hat x_{BLS}(y) = \mathbb E [\mathsf{x|y}]$</p>
</li>
<li><p>proposition</p>
<ul>
<li><p>unbiased:  $b = \mathbb E[\mathsf{e(x,y)}]=E[\mathsf{\hat x(y)-x}]=0$</p>
</li>
<li><p>误差的协方差矩阵就是 <strong>belief</strong>（后验分布？）的协方差阵的期望</p>
<script type="math/tex; mode=display">
\Lambda_{BLS}=\mathbb E[\mathsf{\Lambda_{x|y}(y)}]</script></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Orthogonality</p>
<script type="math/tex; mode=display">
      \hat x(\cdot)\ is\ BLS \iff \mathbb E\left[ \mathsf{[\hat x(y)-x]g^T(y)}\right]=0</script><pre><code>&gt; **Proof**:  omit
</code></pre></li>
</ul>
<h2 id="2-Linear-least-square-estimation"><a href="#2-Linear-least-square-estimation" class="headerlink" title="2. Linear least-square estimation"></a>2. Linear least-square estimation</h2><ul>
<li><p>Drawback of BLS $\hat x_{BLS}(y)=E[x|y]$</p>
<ul>
<li>requires posterior $p(x|y)$, which needs $p(x)$ and $p(y|x)$</li>
<li>calculating posterior is complicated</li>
<li>estimator is <strong>nonlinear</strong></li>
</ul>
</li>
<li><p>Definition of LLS</p>
<ul>
<li>$\hat {\mathbf{x}}_{LLS}(y) = \arg \min\limits_{f(\cdot) \in \mathcal{B}} E\left[||\mathsf{x-f(y)}||^2\right] \\ \mathcal{B}=\{f(\cdot):f(y)=Ay+d\}$</li>
<li>注意 $\hat {\mathbf{x}}(\mathsf{y})$ 是一个随机变量，是关于 $\mathsf{y}$ 的一个函数</li>
<li>LLS 与 BLS 都是假设 x 为一个随机变量，有先验分布，不同之处在于 LLS 要求估计函数为关于观测值 y 的线性函数，因此 LLS 只需要知道二阶矩，而 BLS 需要知道后验均值</li>
</ul>
</li>
<li><p>Property</p>
<ul>
<li><p>Orthogonality</p>
<script type="math/tex; mode=display">
\hat {\mathbf{x}}(\cdot)\ is\ LLS \iff E[\hat {\mathbf{x}}(\mathsf{y})-\mathsf{x}]=0\ \ and\ \ E[(\hat {\mathbf{x}}(\mathsf{y})-\mathsf{x})\mathsf{y}^T]=0</script></li>
<li><p>推论：由正交性可得到</p>
<ul>
<li>$\hat x_{LLS}(y)=\mu_X+\Lambda_{xy}\Lambda_y^{-1}(y-\mu_y)$</li>
<li>$\Lambda_{\mathrm{LLS}} \triangleq \mathbb{E}\left[\left(\mathbf{x}-\hat{\mathbf{x}}_{\mathrm{LLS}}(\mathbf{y})\right)\left(\mathbf{x}-\hat{\mathbf{x}}_{\mathrm{LLS}}(\mathbf{y})\right)^{\mathrm{T}}\right]=\Lambda_{\mathrm{x}}-\Lambda_{\mathrm{xy}} \Lambda_{\mathrm{y}}^{-1} \Lambda_{\mathrm{xy}}^{\mathrm{T}}$</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Proof</strong>: x 可以是向量</p>
<p>$\Longrightarrow$：反证法</p>
<ol>
<li><p>suppose $E[\hat x_{LLS}(y)-x]=\mathbb{b} \ne 0$，take $\hat x’=\hat x_{LLS} - b$<br>then $E\left[||\hat x’ - x||^2\right]=E\left[||\hat x - x||^2\right]-b^2 &lt; E\left[||\hat x - x||^2\right]$<br>与 LLS 的定义矛盾；</p>
</li>
<li><p>$e=\hat x(y)-x$<br>Take $\hat x’ = \hat x_{LLS} - \Lambda_{ey}\Lambda_y^{-1}(y-\mu_y)$</p>
<script type="math/tex; mode=display">
\begin{align}
M &= E\left[(\hat x' -x)(\hat x' -x)^T \right] \\
&= E\left[(\hat x-x)(\hat x-x)^T\right]-\Lambda_{ey}\Lambda_y^{-1}\Lambda_{ey}^T
\end{align}</script><p>由于 $E\left[||\mathsf{x-f(y)}||^2\right] = tr\{M\}$，LLS 的 MSE 应当最小<br>由于 $\Lambda_y$ 正定，因此应有 $\Lambda_{ey}\Lambda_y^{-1}\Lambda_{ey}^T=0$<br>故 $E\left[(\hat x-\mu_x)(y-\mu_y)^T \right]=0 \Longrightarrow E[(\hat {\mathbf{x}}(\mathsf{y})-\mathsf{x})\mathsf{y}^T]=0$</p>
</li>
</ol>
<p>$\Longleftarrow$：suppose another linear estimator $\hat x’$</p>
<script type="math/tex; mode=display">
\begin{align}
E\left[(\hat x'-x)(\hat x'-x)^T\right] &= E[(\hat x'-\hat x+\hat x-x)(\hat x'-\hat x+\hat x-x)^T] \\
&= E[(\hat x'-\hat x)(\hat x'-\hat x)^T] + E[(\hat x-x)(\hat x-x)^T] \\&\ \ \ \ \  - 2E[(\hat x-x)(\hat x'-\hat x)^T] \\
&= E[(\hat x'-\hat x)(\hat x'-\hat x)^T] + E[(\hat x-x)(\hat x-x)^T]
\end{align}</script><p>第三个等号是由于 $\hat x’-\hat x = A’y+d’$</p>
<p>同样的根据上面 $MSE=tr\{M\}$ 可得到 $\hat x$ 有最小的 MSE</p>
</blockquote>
</li>
<li><p>联合高斯分布的情况</p>
<ul>
<li>定理：如果 x 和 y 是联合高斯分布的，那么 <script type="math/tex; mode=display">
\hat x_{BLS}(y) = \hat x_{LLS}(y)</script></li>
</ul>
<blockquote>
<p>证明：$e_{LLS}=\hat x_{LLS}-x$ 也是高斯分布</p>
<p>由于 $E[e_{LLS}\ y^T]=0$，故 $e_{LLS}$ 与 y 相互独立</p>
<p>$E[e_{LLS}|y]=E[e_{LLS}]=0 \to E[\hat x_{LLS}|y]=\hat x_{LLS} = E[x|y]$</p>
</blockquote>
<ul>
<li>通常如果只有联合二阶矩信息，那么 LLS 是 minmax</li>
</ul>
</li>
</ul>
<h2 id="3-Non-Bayesian-formulation"><a href="#3-Non-Bayesian-formulation" class="headerlink" title="3. Non-Bayesian formulation"></a>3. Non-Bayesian formulation</h2><ul>
<li>Formulation<ul>
<li>observation: distribution of y <strong>parameterized</strong> by x,   $p_\mathsf{y}(\mathbf{y;x})$<br>not <strong>conditioned</strong> on x,   $p_\mathsf{y|x}(\mathbf{y|x})$<br>此时 x 不再是一个随机变量，而是未知的一个参数</li>
<li>bias: $b(x)=E[\hat x(y)-\mathbf{x}]$</li>
<li>误差协方差矩阵 $\Lambda_{\mathrm{e}}(\mathrm{x})=\mathbb{E}\left[(\mathrm{e}(\mathrm{x}, \mathrm{y})-\mathrm{b}(\mathrm{x}))(\mathrm{e}(\mathrm{x}, \mathrm{y})-\mathrm{b}(\mathrm{x}))^{\mathrm{T}}\right]$</li>
</ul>
</li>
<li><p><strong>有效(valid)</strong>估计器不应当显式地依赖于 x</p>
</li>
<li><p><strong>MVU</strong>: Minimum-variance unbiased estimator</p>
<ul>
<li>在 MMSE 条件下最优估计就是 MVU 估计<script type="math/tex; mode=display">
\begin{align}
MSE &= E[e^2]=E[(\hat x-x)^2]=E[(\hat x-\mu_{\hat x}+\mu_{\hat x }-x)^2] \\
& =E[(\hat x-\mu_{\hat x})^2]+b^2= \Lambda_{\hat x}(x) + b^2\\
\end{align}</script></li>
</ul>
</li>
<li><p>MVU 可能不存在</p>
<ul>
<li>可能不存在无偏估计，即 $\mathcal{A}=\varnothing$</li>
<li>存在无偏估计 $\mathcal{A} \ne \varnothing$，但是不存在某个估计量在所有情况（任意 x）下都是最小方差</li>
</ul>
</li>
</ul>
<h2 id="4-CRB"><a href="#4-CRB" class="headerlink" title="4. CRB"></a>4. CRB</h2><p><strong>定理</strong>：满足正规条件时</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[\frac{\partial}{\partial x} \ln p_{y}(\mathbf{y} ; x) \right] = 0 \ \ \ \ for \ all \ \ x</script><p>有</p>
<script type="math/tex; mode=display">
\lambda_{\hat x}(X) \ge \frac{1}{J_y(x)}</script><p>其中 Fisher 信息为</p>
<script type="math/tex; mode=display">
J_{y}(x)=\mathbb{E}\left[\left(\frac{\partial}{\partial x} \ln p_{y}(\mathbf{y} ; x)\right)^{2}\right]=-\mathbb{E}\left[\frac{\partial^{2}}{\partial x^{2}} \ln p_{y}(\mathbf{y} ; x)\right]</script><p><strong>证明</strong>：取 $f(y)=\frac{\partial}{\partial x} \ln p_{y}(\mathbf{y} ; x)$，有 $E[f(y)]=0$</p>
<script type="math/tex; mode=display">
cov(e(y),f(y))=\int (\hat x(y)-x)\frac{\partial}{\partial x} p_{y}(\mathbf{y} ; x)dy=1</script><script type="math/tex; mode=display">
1=cov(e,f)\le Var(e)Var(f)</script><p><strong>备注</strong></p>
<ul>
<li>正规条件不满足时，CRB 不存在</li>
<li>Fisher 信息可以看作 $p_{y}(\mathbf{y} ; x)$ 的曲率</li>
</ul>
<h2 id="4-有效估计量"><a href="#4-有效估计量" class="headerlink" title="4. 有效估计量"></a>4. 有效估计量</h2><ul>
<li><p>定义：可以达到 CRB 的无偏估计量</p>
</li>
<li><p>有效估计量一定是 MVU 估计量</p>
</li>
<li><p>MVU 估计量不一定是有效估计量，也即 CRB 不一定是紧致（tight）的，有时没有估计量可以对所有的 x 达到 CRB</p>
</li>
<li><p>性质：（唯一的、无偏的，可以达到 CRB）</p>
<script type="math/tex; mode=display">
\hat x \ \ is \ \ efficient \iff \hat x(y)=x+\frac{1}{J_y(x)}\frac{\partial}{\partial x} \ln p_{y}(\mathbf{y} ; x)</script></li>
</ul>
<blockquote>
<p><strong>证明</strong>：有效估计量 $\iff$ 可以达到 CRB $\iff$ 取等号 $Var(e)Var(f)=1$ $\iff$ 取等号 $e(y)=k(x)f(y)$ $\iff$ $e(y)=x+k(X)f(y)$</p>
<script type="math/tex; mode=display">
\frac{1}{J_y(x)}=E[e^2(y)]=k(x)E[e(y)f(y)]=k(x)</script></blockquote>
<h2 id="5-ML-estimation"><a href="#5-ML-estimation" class="headerlink" title="5. ML estimation"></a>5. ML estimation</h2><ul>
<li>Definition<script type="math/tex; mode=display">
\hat x_{ML}(\cdot)=\arg\max_{a} p(y|a)</script></li>
</ul>
<blockquote>
<p><strong>Proposition</strong>: if efficient estimator exists, it’s ML estimator</p>
<script type="math/tex; mode=display">
\hat x_{eff}(\cdot)=\hat x_{ML}(\cdot)</script><p><strong>Proof</strong>:</p>
<script type="math/tex; mode=display">
\hat x_{eff}(y)=x+\frac{1}{J_y(x)}\frac{\partial}{\partial x}\ln p(y;x)</script><p>由于有效(valid)估计器不应当依赖于 x，因此上式中 x 取任意一个值都应当是相等的，可取 $\hat x_{ML}(y)$</p>
<script type="math/tex; mode=display">
\hat x_{eff}(y)=\hat x_{ML}(y) + \frac{1}{J_y(x)}\frac{\partial \ln p(y;x)}{\partial x}\Big|_{x=\hat x_{ML}}=\hat x_{ML}(y)</script><p><strong>备注</strong>：反之不一定成立，即 ML 估计器不一定是有效的，比如有时候全局的有效估计器(efficient estimator)不存在，也即此时按公式计算得到的 $\hat x_{eff}(y)$ 实际上是依赖于 x 的，那么此时就不存在一个全局最优的估计器，此时的 ML 估计器也没有任何好的特性。</p>
</blockquote>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>参数估计</tag>
      </tags>
  </entry>
  <entry>
    <title>统计推断(一) Hypothesis Test</title>
    <url>/2020/02/03/statistic/SI_Ch1_Hypothesis%20Test/</url>
    <content><![CDATA[<p>假设检验</p>
<a id="more"></a>
<h2 id="1-Binary-Bayesian-hypothesis-testing"><a href="#1-Binary-Bayesian-hypothesis-testing" class="headerlink" title="1. Binary Bayesian hypothesis testing"></a>1. Binary Bayesian hypothesis testing</h2><h3 id="1-0-Problem-Setting"><a href="#1-0-Problem-Setting" class="headerlink" title="1.0 Problem Setting"></a>1.0 Problem Setting</h3><ul>
<li>Hypothesis<ul>
<li>Hypothesis space $\mathcal{H}=\{H_0, H_1\}$</li>
<li>Bayesian approach: Model the valid hypothesis as an RV H</li>
<li>Prior $P_0 = p_\mathsf{H}(H_0), P_1=p_\mathsf{H}(H_1)=1-P_0$</li>
</ul>
</li>
<li>Observation<ul>
<li>Observation space $\mathcal{Y}$</li>
<li>Observation Model $p_\mathsf{y|H}(\cdot|H_0), p_\mathsf{y|H}(\cdot|H_1)$</li>
</ul>
</li>
<li>Decision rule $f:\mathcal{Y\to H}$</li>
<li>Cost function $C: \mathcal{H\times H} \to \mathbb{R}$<ul>
<li>Let $C_{ij}=C(H_j,H_i), correct hypo is H_j$</li>
<li>$C$ is <em>valid</em> if $C_{jj}&lt;C_{ij}$</li>
</ul>
</li>
<li>Optimum decision rule $\hat{H}(\cdot) = \arg\min\limits_{f(\cdot)}\mathbb{E}[C(\mathsf{H},f(\mathsf{y}))]$</li>
</ul>
<h3 id="1-1-Binary-Bayesian-hypothesis-testing"><a href="#1-1-Binary-Bayesian-hypothesis-testing" class="headerlink" title="1.1 Binary Bayesian hypothesis testing"></a>1.1 Binary Bayesian hypothesis testing</h3><blockquote>
<p><strong>Theorem</strong>: The optimal Bayes’ decision takes the form</p>
<script type="math/tex; mode=display">
L(\mathsf{y}) \triangleq 
\frac{p_\mathsf{y|H}(\cdot|H_1)}{p_\mathsf{y|H}(\cdot|H_0)}
\overset{H_1} \gtreqless
\frac{P_0}{P_1} \frac{C_{10}-C_{00}}{C_{01}-C_{11}}
\triangleq \eta</script><p><strong>Proof</strong>: </p>
<script type="math/tex; mode=display">
\begin{align} 
\varphi(f) &=\mathbb{E} [C(H, f(y))] \\
&= \int_{y*} \mathbb{E} [C(H,f(y^*) | \mathsf{y}=y^*)] \\
\end{align}</script><p>Given $y^*$</p>
<ul>
<li>if $f(y^<em>)=H_0$,  $\mathbb{E}=C_{00}p_{\mathsf{H|y}}(H_0|y^</em>)+C_{01}p_{\mathsf{H|y}}(H_1|y^*)$</li>
<li>if $f(y^<em>)=H_1$,  $\mathbb{E}=C_{10}p_{\mathsf{H|y}}(H_0|y^</em>)+C_{11}p_{\mathsf{H|y}}(H_1|y^*)$</li>
</ul>
<p>So</p>
<script type="math/tex; mode=display">
\frac{p_\mathsf{H|y}(H_1|y^*)}{p_\mathsf{H|y}(H_0|y^*)}
\overset{H_1} \gtreqless
\frac{C_{10}-C_{00}}{C_{01}-C_{11}}</script><p><strong>备注</strong>：证明过程中，注意贝叶斯检验为<strong>确定性检验</strong>，因此对于某个确定的 y，$f(y)=H_1$ 的概率要么为 0 要么为 1。因此对代价函数求期望时，把 H 看作是随机变量，而把 $f(y)$ 看作是确定的值来<strong>分类讨论</strong></p>
</blockquote>
<h4 id="Special-cases"><a href="#Special-cases" class="headerlink" title="Special cases"></a>Special cases</h4><ul>
<li>Maximum a posteriori (MAP) <ul>
<li>$C_{00}=C_{11}=0,C_{01}=C_{10}=1$</li>
<li>$\hat{H}(y)==\arg\max\limits_{H\in\{H_0,H_1\}} p_\mathsf{H|y}(H|y)$</li>
</ul>
</li>
<li>Maximum likelihood (ML) <ul>
<li>$C_{00}=C_{11}=0,C_{01}=C_{10}=1, P_0=P_1=0.5$</li>
<li>$\hat{H}(y)==\arg\max\limits_{H\in\{H_0,H_1\}} p_\mathsf{y|H}(y|H)$</li>
</ul>
</li>
</ul>
<h3 id="1-2-Likelyhood-Ratio-Test"><a href="#1-2-Likelyhood-Ratio-Test" class="headerlink" title="1.2 Likelyhood Ratio Test"></a>1.2 Likelyhood Ratio Test</h3><p>Generally, LRT</p>
<script type="math/tex; mode=display">
L(\mathsf{y}) \triangleq 
\frac{p_\mathsf{y|H}(\cdot|H_1)}{p_\mathsf{y|H}(\cdot|H_0)}
\overset{H_1} \gtreqless
\eta</script><ul>
<li>Bayesian formulation gives a method of calculating $\eta$</li>
<li>$L(y)$ is a <strong>sufficient statistic</strong> for the decision problem</li>
<li>$L(y)$ 的可逆函数也是充分统计量</li>
</ul>
<blockquote>
<p><strong>充分统计量</strong></p>
</blockquote>
<h3 id="1-3-ROC"><a href="#1-3-ROC" class="headerlink" title="1.3 ROC"></a>1.3 ROC</h3><ul>
<li>Detection probability $P_D = P(\hat{H}=H_1 | \mathsf{H}=H_1)$</li>
<li>False-alarm probability $P_F = P(\hat{H}=H_1 | \mathsf{H}=H_0)$</li>
</ul>
<p><strong>性质（重要！）</strong></p>
<ul>
<li>LRT 的 ROC 曲线是单调不减的</li>
<li></li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/roc.jpg" alt="ROC"></p>
<h2 id="2-Non-Bayesian-hypo-test"><a href="#2-Non-Bayesian-hypo-test" class="headerlink" title="2. Non-Bayesian hypo test"></a>2. Non-Bayesian hypo test</h2><ul>
<li>Non-Bayesian 不需要<strong>先验概率</strong>或者<strong>代价函数</strong></li>
</ul>
<h3 id="Neyman-Pearson-criterion"><a href="#Neyman-Pearson-criterion" class="headerlink" title="Neyman-Pearson criterion"></a>Neyman-Pearson criterion</h3><script type="math/tex; mode=display">
\max_{\hat{H}(\cdot)}P_D \ \ \ s.t. P_F\le \alpha</script><blockquote>
<p><strong>Theorem</strong>(Neyman-Pearson Lemma)：NP 准则的最优解由 LRT 得到，其中 $\eta$ 由以下公式得到</p>
<script type="math/tex; mode=display">
P_F=P(L(y)\ge\eta | \mathsf{H}=H_0) = \alpha</script><p><strong>Proof</strong>：<br><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/np_proof.jpg" alt="proof"></p>
<p><strong>物理直观</strong>：同一个 $P_F$ 时 LRT 的 $P_D$ 最大。物理直观来看，LRT 中判决为 H1 的区域中 $\frac{p(y|H_1)}{p(y|H_0)}$ 都尽可能大，因此 $P_F$ 相同时 $P_D$ 可最大化</p>
<p><strong>备注</strong>：NP 准则最优解为 LRT，原因是</p>
<ul>
<li>同一个 $P_F$ 时， LRT 的 $P_D$ 最大</li>
<li>LRT 取不同的 $\eta$ 时，$P_F$ 越大，则 $P_D$ 也越大，即 ROC 曲线单调不减</li>
</ul>
</blockquote>
<h2 id="3-Randomized-test"><a href="#3-Randomized-test" class="headerlink" title="3. Randomized test"></a>3. Randomized test</h2><h3 id="3-1-Decision-rule"><a href="#3-1-Decision-rule" class="headerlink" title="3.1 Decision rule"></a>3.1 Decision rule</h3><ul>
<li><p>Two deterministic decision rules $\hat{H’}(\cdot),\hat{H’’}(\cdot)$</p>
</li>
<li><p>Randomized decision rule $\hat{H}(\cdot)$ by time-sharing</p>
<script type="math/tex; mode=display">
\hat{\mathrm{H}}(\cdot)=\left\{\begin{array}{ll}{\hat{H}^{\prime}(\cdot),} & {\text { with probability } p} \\ {\hat{H}^{\prime \prime}(\cdot),} & {\text { with probability } 1-p}\end{array}\right.</script><ul>
<li>Detection prob $P_D=pP_D’+(1-p)P_D’’$</li>
<li>False-alarm prob $P_F=pP_F’+(1-P)P_F’’$</li>
</ul>
</li>
<li><p>A randomized decision rule is <strong>fully described</strong> by $p_{\mathsf{\hat{H}|y}}(H_m|y)$ for m=0,1</p>
</li>
</ul>
<h3 id="3-2-Proposition"><a href="#3-2-Proposition" class="headerlink" title="3.2 Proposition"></a>3.2 Proposition</h3><ol>
<li><p>Bayesian case: <strong>cannot</strong> achieve a lower <em>Bayes’ risk</em> than the optimum LRT</p>
<blockquote>
<p><strong>Proof</strong>: Risk <strong>for each y</strong> is linear in $p_{\mathrm{H} | \mathbf{y}}\left(H_{0} | \mathbf{y}\right)$,  so the minima is achieved at 0 or 1,  which degenerate to deterministic decision</p>
<script type="math/tex; mode=display">
\begin{align}
\varphi(\mathbf{y})&=\sum_{i, j} C_{i j} \mathbb{P}\left(\mathrm{H}=H_{j}, \hat{\mathrm{H}}=H_{i} | \mathbf{y}=\mathbf{y}\right)=\sum_{i, j} C_{i j} p_{\mathrm{\hat H} | y}\left(H_{i} | \mathbf{y}\right) p_{\mathrm{H} | \mathbf{y}}\left(H_{j} | \mathbf{y}\right) \\
&= \Delta(y) + P(\hat H=H_0|y) \frac{P(y|H_0)}{p(y)}P_1(C_{01}-C_{11})(L(y)-\eta)
\end{align}</script></blockquote>
</li>
<li><p>Neyman-Pearson case: </p>
<ol>
<li><strong>continuous-valued</strong>: For a given $P_F$ constraint, randomized test <strong>cannot</strong> achieve <strong>a larger $P_D$</strong> than optimum LRT</li>
<li><strong>discrete-valued</strong>: For a given $P_F$ constraint, randomized test <strong>can</strong> achieve <strong>a larger $P_D$</strong> than optimum LRT. Furthermore, the optimum rand test corresponds to simple <strong>time-sharing</strong> between the <strong>two LRTs nearby</strong></li>
</ol>
</li>
</ol>
<h3 id="3-3-Efficient-frontier"><a href="#3-3-Efficient-frontier" class="headerlink" title="3.3 Efficient frontier"></a>3.3 Efficient frontier</h3><p>Boundary of region of achievable $(P_D,P_F)$ operation points</p>
<ul>
<li><strong>continuous-valued</strong>: ROC of LRT</li>
<li><strong>discrete-valued</strong>:  LRT points and the straight line segments</li>
</ul>
<p><strong>Facts</strong></p>
<ul>
<li>$P_D \ge P_F$</li>
<li>efficient frontier is <strong>concave</strong> function</li>
<li>$\frac{dP_D}{dP_F}=\eta$</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/efficient_frontier.jpg" alt="efficient frontier"></p>
<h2 id="4-Minmax-hypo-testing"><a href="#4-Minmax-hypo-testing" class="headerlink" title="4. Minmax hypo testing"></a>4. Minmax hypo testing</h2><p>prior: unknown,   cost fun: known</p>
<h3 id="4-1-Decision-rule"><a href="#4-1-Decision-rule" class="headerlink" title="4.1 Decision rule"></a>4.1 Decision rule</h3><ul>
<li><p>minmax approach</p>
<script type="math/tex; mode=display">
\hat H(\cdot)=\arg\min_{f(\cdot)}\max_{p\in[0,1]} \varphi(f,p)</script></li>
<li><p>optimal decision rule</p>
<script type="math/tex; mode=display">
\hat H(\cdot)=\hat{H}_{p_*}(\cdot) \\
p_* = \arg\max_{p\in[0,1]} \varphi(\hat H_p, p)</script><blockquote>
<p>要想证明上面的最优决策，首先引入 mismatch Bayes decision</p>
<script type="math/tex; mode=display">
\hat{\mathrm{H}}_q(y)=\left\{
\begin{array}{ll}{H_1,} & {L(y) \ge \frac{1-q}{q}\frac{C_{10}-C_{00}}{C_{01}-C_{11}}} \\ 
{H_0,} & {otherwise}\end{array}\right.</script><p>代价函数如下，可得到 $\varphi(\hat H_q,p)$ 与概率 $p$ 成线性关系</p>
<script type="math/tex; mode=display">
\varphi(\hat H_q,p)=(1-p)[C_{00}(1-P_F(q))+C_{10}P_F(q)] + p[C_{01}(1-P_D(q))+C_{11}P_D(q)]</script><p><strong>Lemma</strong>:  Max-min inequality</p>
<script type="math/tex; mode=display">
\max_x\min_y g(x,y) \le \min_y\max_x g(x,y)</script><p><strong>Theorem</strong>:  </p>
<script type="math/tex; mode=display">
\min_{f(\cdot)}\max_{p\in[0,1]}\varphi(f,p)=\max_{p\in[0,1]}\min_{f(\cdot)}\varphi(f,p)</script><p><strong>Proof of Lemma</strong>: Let $h(x)=\min_y g(x,y)$</p>
<script type="math/tex; mode=display">
\begin{aligned} 
g(x) &\leq f(x, y), \forall x \forall y \\ 
\Longrightarrow \max _{x} g(x) & \leq \max _{x} f(x, y), \forall y \\ \Longrightarrow \max _{x} g(x) & \leq \min _{y} \max _{x} f(x, y) 
\end{aligned}</script><p><strong>Proof of Thm</strong>:  先取 $\forall p_1,p_2 \in [0,1]$，可得到</p>
<script type="math/tex; mode=display">
\varphi(\hat H_{p_1},p_1)=\min_f \varphi(f,p_1) \le \max_p \min_f \varphi(f,p) \le \min_f \max_p \varphi(f, p) \le \max_p \varphi(\hat H_{p_2}, p)</script><p>由于 $p_1,p_2$ 任取时上式都成立，因此可以取 $p_1=p_2=p_*=\arg\max_p \varphi(\hat H_p, p)$</p>
<p>要想证明定理则只需证明 $\varphi(\hat H_{p_<em>},p_</em>)=\max_p \varphi(\hat H_{p_*}, p)$</p>
<p>由前面可知 $\varphi(\hat H_q,p)$ 与 $p$ 成线性关系，因此要证明上式</p>
<ul>
<li>若 $p_<em> \in (0,1)$，只需 $\left.\frac{\partial \varphi\left(\hat{H}_{q^{</em>}}, p\right)}{\partial p}\right|_{\text {for any } p}=0$，等式自然成立</li>
<li>若 $p_<em> = 1$，只需 $\left.\frac{\partial \varphi\left(\hat{H}_{q^{</em>}}, p\right)}{\partial p}\right|_{\text {for any } p} &gt; 0$，最优解就是 $p=1$；$q_*=0$ 同理</li>
</ul>
<p>根据下面的引理，可以得到最优决策就是 Bayes 决策 $p_<em>=\arg\max_p \varphi(\hat H_p, p)$，其中 $p_</em>$ 满足</p>
<script type="math/tex; mode=display">
\begin{aligned} 0 &=\frac{\partial \varphi\left(\hat{H}_{p_{*}}, p\right)}{\partial p} \\ &=\left(C_{01}-C_{00}\right)-\left(C_{01}-C_{11}\right) P_{\mathrm{D}}\left(p_{*}\right)-\left(C_{10}-C_{00}\right) P_{\mathrm{F}}\left(p_{*}\right) \end{aligned}</script><p><strong>Lemma</strong>: </p>
<script type="math/tex; mode=display">
\left.\frac{\mathrm{d} \varphi\left(\hat{H}_{p}, p\right)}{\mathrm{d} p}\right|_{p=q}=\left.\frac{\partial \varphi\left(\hat{H}_{q}, p\right)}{\partial p}\right|_{p=q}=\left.\frac{\partial \varphi\left(\hat{H}_{q}, p\right)}{\partial p}\right|_{\text {for any } p}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/mismatch_bayes_risk.jpg" alt="bayes risk"></p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>假设检验</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows任务栏右侧小图标显示不完整</title>
    <url>/2020/01/08/tools/taskbar/</url>
    <content><![CDATA[<a id="more"></a>
<p>之前发现windows电脑右侧小图标显示不完整</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/taskbar_before.png" alt="before"></p>
<p>百度了一下，只需要新建一个<code>bat</code>文件，内容是</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /d %userprofile%\AppData\Local\Microsoft\Windows\Explorer</span><br><span class="line">taskkill /f /im explorer.exe</span><br><span class="line">attrib -h iconcache_*.db</span><br><span class="line">del iconcache_*.db /a</span><br><span class="line">start explorer</span><br><span class="line">pause</span><br></pre></td></tr></table></figure>
<p>以管理员身份运行后就好了，中间屏幕会闪一下蓝屏2s，不用担心。然后就可以变成下面这样</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/taskbar_after.png" alt="after"></p>
<p>参考资料：<a href="https://jingyan.baidu.com/article/ae97a64608ba0cbbfd461d8f.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/ae97a64608ba0cbbfd461d8f.html</a></p>
]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title>Git 工作原理(二)</title>
    <url>/2019/12/29/git/git-principle-2/</url>
    <content><![CDATA[<blockquote>
<p>在上一篇文章<a href="https://glooow1024.github.io/2019/12/29/git/git-principle-1/#more" target="_blank" rel="noopener">Git工作原理（一）</a>中，我们介绍了简单的 git 版本控制，上一篇文章中并没有牵涉到分支 branch 相关的内容，这篇文章将会介绍：当我们创建不同的分支时，git 做了些什么？</p>
</blockquote>
<a id="more"></a>
<p>[TOC]</p>
<h2 id="1-master-分支"><a href="#1-master-分支" class="headerlink" title="1. master 分支"></a>1. <code>master</code> 分支</h2><p>上一篇文章结束后，我们查看仓库的历史</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git ls-files --stage</span></span><br><span class="line">100644 445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 0       a.txt</span><br><span class="line">100644 c200906efd24ec5e783bee7f23b5d7c941b0c12c 0       dir/b.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> git cat-file --batch-check --batch-all-objects</span></span><br><span class="line">0ed6427de6990a17351bf0e0fd648b642e15f967 tree 63</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 commit 175</span><br><span class="line">445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 blob 7</span><br><span class="line">58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</span><br><span class="line">aef06e3d27cc6b17730daf473499ab58b68e772d tree 33</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e commit 223</span><br><span class="line">b02b1164ed5a571b723cb25d978780b15d826d62 tree 63</span><br><span class="line">c200906efd24ec5e783bee7f23b5d7c941b0c12c blob 4</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">log</span></span></span><br><span class="line">commit b00f88d6e09fd9e767fc3246c971bf0d14f0621e (HEAD -&gt; master)</span><br><span class="line">Author: Glooow1024 &lt;glooow1024@gmail.com&gt;</span><br><span class="line">Date:   Sun Dec 29 16:07:08 2019 +0800</span><br><span class="line"></span><br><span class="line">    commit 2</span><br><span class="line"></span><br><span class="line">commit 38f74e0a07955212bdb02699f6d73cd7420cd823</span><br><span class="line">Author: Glooow1024 &lt;glooow1024@gmail.com&gt;</span><br><span class="line">Date:   Sun Dec 29 15:42:55 2019 +0800</span><br><span class="line"></span><br><span class="line">    commit 1</span><br><span class="line">    </span><br><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p b00f</span></span><br><span class="line">tree 0ed6427de6990a17351bf0e0fd648b642e15f967</span><br><span class="line">parent 38f74e0a07955212bdb02699f6d73cd7420cd823</span><br><span class="line">author Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800</span><br><span class="line">committer Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800</span><br><span class="line"></span><br><span class="line">commit 2</span><br></pre></td></tr></table></figure>
<p>当前 <code>.git/</code> 文件夹下我们主要关注以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.git</span><br><span class="line">├── HEAD</span><br><span class="line">├── refs</span><br><span class="line">│   └── tags</span><br><span class="line">│   └── heads</span><br><span class="line">│   	└── master</span><br><span class="line">├── logs</span><br><span class="line">│   └── HEAD</span><br><span class="line">│   └── refs</span><br><span class="line">│   	└── heads</span><br><span class="line">│   		└── master</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="1-1-git-HEAD"><a href="#1-1-git-HEAD" class="headerlink" title="1.1 .git/HEAD"></a>1.1 <code>.git/HEAD</code></h3><p>如果查看当前的 <code>HEAD</code> 就可以看到他指向了仓库 <code>master</code> 最后一次 <code>commit</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/HEAD</span></span><br><span class="line">ref: refs/heads/master</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat .git/refs/heads/master</span></span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e</span><br></pre></td></tr></table></figure>
<h3 id="1-2-git-logs"><a href="#1-2-git-logs" class="headerlink" title="1.2 .git/logs"></a>1.2 <code>.git/logs</code></h3><p>如果再查看 <code>.git/logs/</code> 中的文件，可以看到这里也保存一个 <code>HEAD</code>，但是内容跟之前差很多</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ./.git/logs/HEAD</span></span><br><span class="line">0000000000000000000000000000000000000000 38f74e0a07955212bdb02699f6d73cd7420cd823 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800    commit (initial): commit 1</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800    commit: commit 2</span><br></pre></td></tr></table></figure>
<p>第一列是上一次提交的 <code>commit</code> 哈希值，第二列是本次 <code>commit</code> 哈希值，后面是用户信息，最后一列则是每次 <code>commit</code> 的附加的消息 message。</p>
<p>再看 <code>logs</code> 中的 <code>master</code>，可以看到跟前面 <code>HEAD</code> 中的内容一样 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/logs/refs/heads/master</span></span><br><span class="line">0000000000000000000000000000000000000000 38f74e0a07955212bdb02699f6d73cd7420cd823 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800    commit (initial): commit 1</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800    commit: commit 2</span><br></pre></td></tr></table></figure>
<h2 id="2-Git-的分支管理"><a href="#2-Git-的分支管理" class="headerlink" title="2. Git 的分支管理"></a>2. Git 的分支管理</h2><h3 id="2-1-git-branch"><a href="#2-1-git-branch" class="headerlink" title="2.1 git branch"></a>2.1 <code>git branch</code></h3><p>现在让我们新建一个分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git branch br1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat ./.git/HEAD</span></span><br><span class="line">ref: refs/heads/master</span><br></pre></td></tr></table></figure>
<p><code>HEAD</code> 的内容当然没有改变，因为我们没有转移到新的分支。但是 <code>.git/</code> 发生了哪些变化呢？他变成了这样</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.git</span><br><span class="line">├── HEAD</span><br><span class="line">├── refs</span><br><span class="line">│   └── tags</span><br><span class="line">│   └── heads</span><br><span class="line">│   	└── br1</span><br><span class="line">│   	└── master</span><br><span class="line">├── logs</span><br><span class="line">│   └── HEAD</span><br><span class="line">│   └── refs</span><br><span class="line">│   	└── heads</span><br><span class="line">│   		└── br1</span><br><span class="line">│   		└── master</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>来看看内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ./.git/refs/heads/br1</span></span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat .git/logs/refs/heads/br1</span></span><br><span class="line">0000000000000000000000000000000000000000 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577615247 +0800    branch: Created from master</span><br></pre></td></tr></table></figure>
<p><code>refs/</code> 中的 <code>br1</code> 只是记录了产生分支的最近一次 <code>commit</code> 的哈希值，而 <code>logs/</code> 中的信息第一列变成了空，也就是说分支后不能向更早的版本回退，最多回退到分支时的那个版本，同时最后一项信息记录了这个分支信息。</p>
<blockquote>
<p><strong>敲黑板！重点</strong></p>
<ul>
<li><code>.git/refs/master</code> 只记录了当前分支最后一个 <code>commit</code> 的哈希值；</li>
<li><code>.git/logs/refs/master</code> 记录了当前分支所有 <code>commit</code> 的哈希值，构成一个可以向旧版本回溯的单向链表；</li>
<li><code>.git/HEAD</code> 只记录了当前工作区所在分支对应的 <code>refs/</code> 中的文件；</li>
<li><code>.git/logs/HEAD</code> 记录了 <code>.git/HEAD</code> 的变化历程；</li>
</ul>
</blockquote>
<h3 id="2-2-git-checkout"><a href="#2-2-git-checkout" class="headerlink" title="2.2 git checkout"></a>2.2 <code>git checkout</code></h3><p>现在让我们转移到新的分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout br1</span></span><br><span class="line">Switched to branch 'br1'</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat ./.git/HEAD</span></span><br><span class="line">ref: refs/heads/br1</span><br></pre></td></tr></table></figure>
<p>只是修改了当前的 <code>HEAD</code> 就表示我们转移到了新的分支，原来的主分支 <code>master</code> 还在 <code>ref: refs/heads/master</code> 中保存，所以不会有信息丢失。</p>
<p>假如我们在分支上修改了内容呢？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'hello'</span> &gt;&gt; a.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat a.txt</span></span><br><span class="line">hahaha</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>
<p>然后查看暂存区 <code>index</code> 文件，可以看到 <code>a.txt</code> 的哈希值已经改变了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git ls-files --stage</span></span><br><span class="line">100644 63db897b879aac027311451ea6d8158daab3ac39 0       a.txt</span><br><span class="line">100644 c200906efd24ec5e783bee7f23b5d7c941b0c12c 0       dir/b.txt</span><br></pre></td></tr></table></figure>
<h3 id="2-3-git-commit"><a href="#2-3-git-commit" class="headerlink" title="2.3 git commit"></a>2.3 <code>git commit</code></h3><p>然后我们提交一哈，不出意外的话用 <code>$ git cat-file --batch-check --batch-all-objects</code> 会发现多了一个 <code>commit</code> 和一个 <code>tree</code> 文件，这是我们在上一篇文章中所讲的，忘记的可以再回顾一下。</p>
<p>再看当前 <code>br1</code> 已经被修改了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/refs/heads/br1</span></span><br><span class="line">4b60ac6ea7ebab972920f84bd07de3d20d7d5804</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat .git/logs/refs/heads/br1</span></span><br><span class="line">0000000000000000000000000000000000000000 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577615247 +0800	branch: Created from master</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e 4b60ac6ea7ebab972920f84bd07de3d20d7d5804 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577616495 +0800	commit: commit br 1</span><br></pre></td></tr></table></figure>
<p>重要的是 <code>logs/HEAD</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/logs/HEAD</span></span><br><span class="line">0000000000000000000000000000000000000000 38f74e0a07955212bdb02699f6d73cd7420cd823 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800	commit (initial): commit 1</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800	commit: commit 2</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577615776 +0800	checkout: moving from master to br1</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e 4b60ac6ea7ebab972920f84bd07de3d20d7d5804 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577616495 +0800	commit: commit br 1</span><br></pre></td></tr></table></figure>
<p>可以看到这里记录了我们转移分支并提交的记录。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><blockquote>
<p><strong>敲黑板！重点</strong></p>
<ol>
<li>实质上我们的每个 branch 都相当于维护了一个 <code>commit</code> 文件的单项链表；</li>
<li>命令 <code>git branch</code> 实际上就是在 <code>.git/refs/heads</code> 和 <code>.git/logs/refs/heads</code> 分别创建一个对应的文件，文件名就是分支名，文件中保存了这个分支对应的 <code>commit</code> 链表的各项；</li>
<li>我们通过 <code>git checkout</code> 实际上就是修改 <code>.git/HEAD</code> 使其指向对应的分支在 <code>.git/refs/</code> 中的文件；</li>
</ol>
</blockquote>
<h3 id="2-4-git-merge"><a href="#2-4-git-merge" class="headerlink" title="2.4 git merge"></a>2.4 <code>git merge</code></h3><p>现在让我们合并一下分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git merge br1</span></span><br><span class="line">Updating b00f88d..4b60ac6</span><br><span class="line">Fast-forward</span><br><span class="line"> a.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>我们再来看一下 <code>HEAD</code> 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/logs/HEAD</span></span><br><span class="line">0000000000000000000000000000000000000000 38f74e0a07955212bdb02699f6d73cd7420cd823 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800    commit (initial): commit 1</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800    commit: commit 2</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577615776 +0800    checkout: moving from master to br1</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e 4b60ac6ea7ebab972920f84bd07de3d20d7d5804 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577616495 +0800    commit: commit br 1</span><br><span class="line">4b60ac6ea7ebab972920f84bd07de3d20d7d5804 b00f88d6e09fd9e767fc3246c971bf0d14f0621e Glooow1024 &lt;glooow1024@gmail.com&gt; 1577625498 +0800    checkout: moving from br1 to master</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e 4b60ac6ea7ebab972920f84bd07de3d20d7d5804 Glooow1024 &lt;glooow1024@gmail.com&gt; 1577625944 +0800    merge br1: Fast-forward</span><br></pre></td></tr></table></figure>
<p>这里的 <code>merge</code> 过程实际上就是把 master 对应的指针移到了 <code>br1</code> 对应的指针处，看下图很容易理解（图片来自于 <a href="https://git-scm.com/book/zh/v2/Git-%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6）" target="_blank" rel="noopener">https://git-scm.com/book/zh/v2/Git-%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6）</a></p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/basic-branching-3.png" alt="git branch"></p>
<p>删除分支实质上也就是删除了分支文件 <code>.git/refs/heads/br1</code> 和 <code>.git/logs/refs/heads/br1</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git branch -d br1</span></span><br></pre></td></tr></table></figure>
<h2 id="3-Git-的标签管理"><a href="#3-Git-的标签管理" class="headerlink" title="3. Git 的标签管理"></a>3. Git 的标签管理</h2><p>我们先看一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">log</span> --pretty=oneline</span></span><br><span class="line">4b60ac6ea7ebab972920f84bd07de3d20d7d5804 (HEAD -&gt; master) commit br 1</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e commit 2</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 commit 1</span><br></pre></td></tr></table></figure>
<h3 id="3-1-git-tag"><a href="#3-1-git-tag" class="headerlink" title="3.1 git tag"></a>3.1 <code>git tag</code></h3><p>打个标签</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git tag -a v0.1 b00f</span></span><br></pre></td></tr></table></figure>
<p>然后就会发现 <code>.git/refs/tags/</code> 中多了一个 <code>v0.1</code> 文件，实际上就是一个指针（<code>commit</code> 文件的哈希值），跟 <code>.git/refs/heads/master</code> 中的指针没有区别</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat .git/refs/tags/v0.1</span></span><br><span class="line">3ad673290fa12aecc5bb66e7c7d3f83157914957</span><br></pre></td></tr></table></figure>
<p>需要注意的是增加 tag 并不会修改 <code>.git/logs</code> 中的文件，因为这个文件夹是维护版本更新历史的，打标签并没有产生新的版本。</p>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git 工作原理(一)</title>
    <url>/2019/12/29/git/git-principle-1/</url>
    <content><![CDATA[<p>今天讲一下 Git 工作原理，本文主要目的不是讲解如何使用各种 git 命令，而是关于一些常用 git 命令之后仓库中会出现什么变化，以及 git 是如何进行仓库的版本控制的。</p>
<a id="more"></a>
<p>[TOC]</p>
<h2 id="1-Git-的文件管理"><a href="#1-Git-的文件管理" class="headerlink" title="1. Git 的文件管理"></a>1. Git 的文件管理</h2><h3 id="1-1-Git-文件存储"><a href="#1-1-Git-文件存储" class="headerlink" title="1.1 Git 文件存储"></a>1.1 Git 文件存储</h3><p>git 是怎么管理文件的呢？首先我们需要理解 git 对文件的保存逻辑，这很重要：</p>
<blockquote>
<p><strong>git 根据文件内容来管理文件，而不是文件名</strong>！</p>
<p>比如某个仓库中，完全相同的两个文件我们保存了 2 份，只不过文件名不同，那么在 git 看来这两个是同一个文件，他会根据文件内容经过SHA1算法计算出一个对应的哈希值，然后根据哈希值来索引文件。</p>
<p>注：这么做的好处是显而易见的，相同的两个文件不再需要多存储一份。</p>
</blockquote>
<p>这是怎么体现出来的呢？其实 git 会把我们仓库中的原始文件压缩成二进制文件，然后都保存在 <code>.git/objects/</code> 文件夹下，每个文件的命名就是计算出来的哈希值。我们可以用如下命令来查看 git 给我们保存的所有文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git cat-file --batch-check -batch-all-objects</span><br></pre></td></tr></table></figure>
<p>现在我们新建一个仓库试试吧</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git init</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'111'</span> &gt; a.txt</span></span><br><span class="line">mkdir dir</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'222'</span> &gt; dir/b.txt</span></span><br></pre></td></tr></table></figure>
<p>好，我们来看看 <code>.git/objects/</code> 文件夹。咦？怎么是空的？什么也没有。哦我们只是修改了工作区，还没添加修改呢，来添加一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git add .</span></span><br></pre></td></tr></table></figure>
<p>好，现在来看看 <code>.git/objects/</code> 吧</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file --batch-check --batch-all-objects</span></span><br><span class="line">58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</span><br><span class="line">c200906efd24ec5e783bee7f23b5d7c941b0c12c blob 4</span><br></pre></td></tr></table></figure>
<h3 id="1-2-Git-文件类型"><a href="#1-2-Git-文件类型" class="headerlink" title="1.2 Git 文件类型"></a>1.2 Git 文件类型</h3><p>应用上述命令后，三列分别表示文件的哈希值、<strong>文件类型</strong>、长度。注意这里出现了 git 的文件类型，主要有 4 种：<code>blob</code>、<code>tree</code>、<code>commit</code>、<code>tag</code>。</p>
<p>我们可以用以下命令查看文件类型</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git cat-file -t 58c9</span><br></pre></td></tr></table></figure>
<p>可以用以下命令查看内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git cat-file -p 58c9</span><br></pre></td></tr></table></figure>
<p>前面我们只看到了 <code>blob</code> 文件，什么情况下会出现 <code>tree</code> 和  <code>commit</code> 呢？注意我们现在只是新建 txt 并 add 了，还没有提交，那我们 commit 一下吧</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">"commit 1"</span></span></span><br></pre></td></tr></table></figure>
<p>然后再查看一下 <code>.git/objects/</code> 文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file --batch-check --batch-all-objects</span></span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 commit 175</span><br><span class="line">58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</span><br><span class="line">aef06e3d27cc6b17730daf473499ab58b68e772d tree 33</span><br><span class="line">b02b1164ed5a571b723cb25d978780b15d826d62 tree 63</span><br><span class="line">c200906efd24ec5e783bee7f23b5d7c941b0c12c blob 4</span><br></pre></td></tr></table></figure>
<p>咦，他们出现了！！！让我们分别看看他们是什么东西~</p>
<h4 id="1-2-1-blob-文件"><a href="#1-2-1-blob-文件" class="headerlink" title="1.2.1 blob 文件"></a>1.2.1 <code>blob</code> 文件</h4><p>如果是 <code>blob</code> 文件我们可以直接看到文件的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p 58c9</span></span><br><span class="line">111</span><br></pre></td></tr></table></figure>
<h4 id="1-2-2-tree-文件"><a href="#1-2-2-tree-文件" class="headerlink" title="1.2.2 tree 文件"></a>1.2.2 <code>tree</code> 文件</h4><p>如果是 <code>tree</code> 文件我们可以看到目录信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p b02b</span></span><br><span class="line">100644 blob 58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c    a.txt</span><br><span class="line">040000 tree aef06e3d27cc6b17730daf473499ab58b68e772d    dir</span><br></pre></td></tr></table></figure>
<p>四列内容分别为文件权限、文件类型、哈希值、文件名。再查看最后一个 <code>tree</code> 文件的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p aef0</span></span><br><span class="line">100644 blob c200906efd24ec5e783bee7f23b5d7c941b0c12c    b.txt</span><br></pre></td></tr></table></figure>
<p>这不就是我们工作空间中的文件结构嘛！</p>
<blockquote>
<p>注意我们工作空间中的文件对应的<strong>文件名</strong>和权限等信息是保存在 <code>tree</code> 类型的文件中，而不是 <code>blob</code> 文件中哦！</p>
</blockquote>
<h4 id="1-2-3-commit-文件"><a href="#1-2-3-commit-文件" class="headerlink" title="1.2.3 commit 文件"></a>1.2.3 <code>commit</code> 文件</h4><p>让我们再看看这个 <code>commit</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p 38f7</span></span><br><span class="line">tree b02b1164ed5a571b723cb25d978780b15d826d62</span><br><span class="line">author Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800</span><br><span class="line">committer Glooow1024 &lt;glooow1024@gmail.com&gt; 1577605375 +0800</span><br><span class="line"></span><br><span class="line">commit 1</span><br></pre></td></tr></table></figure>
<p>其实 <code>commit</code> 就是指向了当前仓库的根目录所对应的 <code>tree</code> 文件嘛。</p>
<h4 id="1-2-4-tag-文件"><a href="#1-2-4-tag-文件" class="headerlink" title="1.2.4 tag 文件"></a>1.2.4 <code>tag</code> 文件</h4><p>这个我们以后再说。</p>
<h4 id="1-2-5-index-文件"><a href="#1-2-5-index-文件" class="headerlink" title="1.2.5 index 文件"></a>1.2.5 <code>index</code> 文件</h4><p>细心的话可以发现我们 add 以后 <code>.git/</code> 文件夹下的 <code>index</code> 文件也会被修改，我们可以用以下命令查看他的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git ls-files --stage</span></span><br><span class="line">100644 58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c 0       a.txt</span><br><span class="line">100644 c200906efd24ec5e783bee7f23b5d7c941b0c12c 0       dir/b.txt</span><br></pre></td></tr></table></figure>
<p>其实它的内容跟 <code>tree</code> 是类似的，不过多了一列，这个我们后面再说</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><blockquote>
<p><strong>敲黑板</strong>！各种文件的作用是</p>
<ul>
<li><code>blob</code>：工作区的任何文件都会被 git 压缩后以 <code>blob</code> 形式保存一个副本在 <code>.git/objects/</code> 文件夹下，文件名就是计算的哈希值；</li>
<li><code>tree</code>：这个其实就是目录文件，描述当前文件夹结构；注意文件名和权限等信息是保存在 <code>tree</code> 文件中而不是 <code>blob</code> 文件，后者只保存文件内容；</li>
<li><code>commit</code>：记录提交的信息，指向仓库当前根目录的 <code>tree</code> 文件；</li>
<li><code>tag</code>：记录标签信息。</li>
</ul>
</blockquote>
<h2 id="2-Git-的版本控制"><a href="#2-Git-的版本控制" class="headerlink" title="2. Git 的版本控制"></a>2. Git 的版本控制</h2><h3 id="2-1-git-add-会发生什么"><a href="#2-1-git-add-会发生什么" class="headerlink" title="2.1 git add 会发生什么"></a>2.1 <code>git add</code> 会发生什么</h3><p>假如我们现在修改了其中一个文件呢？比如</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'hahaha'</span> &gt; a.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git add .</span></span><br></pre></td></tr></table></figure>
<p>再看一下 <code>.git/objects/</code> 文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file --batch-check --batch-all-objects</span></span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 commit 175</span><br><span class="line">445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 blob 7</span><br><span class="line">58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</span><br><span class="line">aef06e3d27cc6b17730daf473499ab58b68e772d tree 33</span><br><span class="line">b02b1164ed5a571b723cb25d978780b15d826d62 tree 63</span><br><span class="line">c200906efd24ec5e783bee7f23b5d7c941b0c12c blob 4</span><br></pre></td></tr></table></figure>
<p>增加了什么？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 blob 7</span><br></pre></td></tr></table></figure>
<p>我们来看看这个文件，他就是一个 <code>blob</code> 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p 445a</span></span><br><span class="line">hahaha</span><br></pre></td></tr></table></figure>
<p>看来就是把我们修改后的文件又存储了一个 <code>blob</code> 文件，注意修改前的文件并没有删除哦，也就是修改前的 <code>a.txt</code> 对应的 <code>58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</code> 还在，方便我们以后版本回退嘛。</p>
<p>那么我们再看看 <code>index</code> 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git ls-files --stage</span></span><br><span class="line">100644 445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 0       a.txt</span><br><span class="line">100644 c200906efd24ec5e783bee7f23b5d7c941b0c12c 0       dir/b.txt</span><br></pre></td></tr></table></figure>
<p>咦，<code>a.txt</code> 文件的指针已经修改了！！！指向了最新的 <code>blob</code> 文件。好了我们知道了</p>
<blockquote>
<p><code>index</code> 文件在执行 <code>git add</code> 之后就会被修改，总是保存最新的仓库根目录信息。事实上，这也<strong>是我们下次 <code>commit</code> 所要提交的信息</strong>！！！</p>
<p>事实上，<code>index</code> 就是我们常说的 git 的<strong>暂存区</strong>！！！</p>
</blockquote>
<h3 id="2-2-git-commit-会发生什么"><a href="#2-2-git-commit-会发生什么" class="headerlink" title="2.2 git commit 会发生什么"></a>2.2 <code>git commit</code> 会发生什么</h3><p>如果再提交一下修改呢？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">"commit 2"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git cat-file --batch-check --batch-all-objects</span></span><br><span class="line">0ed6427de6990a17351bf0e0fd648b642e15f967 tree 63</span><br><span class="line">38f74e0a07955212bdb02699f6d73cd7420cd823 commit 175</span><br><span class="line">445a69c00e48288ac420a2ead9ae5a1cb4cd36d4 blob 7</span><br><span class="line">58c9bdf9d017fcd178dc8c073cbfcbb7ff240d6c blob 4</span><br><span class="line">aef06e3d27cc6b17730daf473499ab58b68e772d tree 33</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e commit 223</span><br><span class="line">b02b1164ed5a571b723cb25d978780b15d826d62 tree 63</span><br><span class="line">c200906efd24ec5e783bee7f23b5d7c941b0c12c blob 4</span><br></pre></td></tr></table></figure>
<p>好了，新增加的有</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">0ed6427de6990a17351bf0e0fd648b642e15f967 tree 63</span><br><span class="line">b00f88d6e09fd9e767fc3246c971bf0d14f0621e commit 223</span><br></pre></td></tr></table></figure>
<p>我们可以很容易推断，新的 <code>tree</code> 文件就描述了更新后的根目录信息，可以看出 <code>a.txt</code> 文件的指针（哈希值）变了，但是由于 dir 文件夹中的内容没有任何修改，所以他的指针不变</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p 0ed6</span></span><br><span class="line">100644 blob 445a69c00e48288ac420a2ead9ae5a1cb4cd36d4    a.txt</span><br><span class="line">040000 tree aef06e3d27cc6b17730daf473499ab58b68e772d    dir</span><br></pre></td></tr></table></figure>
<p>我们再看看新的 <code>commit</code> 文件，可以发现还多了一个 <code>parent</code> 选项，也就是指向了上一次提交对应的的 <code>commit</code> 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git cat-file -p b00f</span></span><br><span class="line">tree 0ed6427de6990a17351bf0e0fd648b642e15f967</span><br><span class="line">parent 38f74e0a07955212bdb02699f6d73cd7420cd823</span><br><span class="line">author Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800</span><br><span class="line">committer Glooow1024 &lt;glooow1024@gmail.com&gt; 1577606828 +0800</span><br><span class="line"></span><br><span class="line">commit 2</span><br></pre></td></tr></table></figure>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><blockquote>
<p><strong>重点来了，敲黑板</strong>！</p>
<ul>
<li><code>git add</code><ul>
<li>对每个修改后的文件压缩，并保存一个新的 <code>blob</code> 文件在 <code>.git/objects/</code> 文件夹下，用哈希值命名文件；</li>
<li>修改 <code>.git/index</code> 保存最新的根目录信息；</li>
</ul>
</li>
<li><code>git commit</code><ul>
<li>生成新的 <code>tree</code> 文件保存在 <code>.git/objects/</code> 目录下，记录新的仓库文件结构信息；</li>
<li>生成新的 <code>commit</code> 文件保存在 <code>.git/objects/</code> 目录下，指向当前最新的根目录的 <code>tree</code> 文件；同时该文件中还存在以一项 <code>parent</code> 指向上一次的 <code>commit</code> 文件；</li>
</ul>
</li>
</ul>
<p>实际上，只需要<strong>1 个 <code>commit</code>，若干个 <code>tree</code> 和若干个 <code>blob</code> 文件</strong>就可以<strong>完整描述</strong>仓库的当前提交版本。因此 git 只需要用一个单向链表记录 <code>commit</code> 文件之间的指向关系，就可以描述版本变化。</p>
</blockquote>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Particle Filter</title>
    <url>/2019/12/10/statistic/particle%20filter/</url>
    <content><![CDATA[<p>今天我们来讲一下粒子滤波算法，这也是我在学习过程中的一个笔记，由于有很多的个人理解，有不对的地方欢迎大家批评指正。</p>
<a id="more"></a>
<p>[TOC]</p>
<h2 id="1-贝叶斯滤波"><a href="#1-贝叶斯滤波" class="headerlink" title="1. 贝叶斯滤波"></a>1. 贝叶斯滤波</h2><p>贝叶斯滤波是我们理解粒子滤波的基础。假设我们有如下图的隐马尔科夫模型(Hidden Markov Model)，并且有如下的系统方程与观测方程，我们只有观测值 $\boldsymbol{y}_{1:T}$，但是现在想要根据观测值估计隐变量 $\boldsymbol{x}_{1:T}$，也就是<strong>滤波</strong>所要做的事情。</p>
<script type="math/tex; mode=display">
\begin{align}
\text{System Model}&: x_{k}=f_{k}\left(x_{k-1}, v_{k-1}\right) \\
\text{Observation Model}&: \mathrm{y}_{k}=h_{k}\left(\mathrm{x}_{k}, \mathrm{n}_{k}\right)
\end{align}</script><p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/pf_hmm.PNG" alt="hmm"></p>
<p>假设我们现在已经处于 $t_{k-1}$，且已经获得了 $p({x}_{k-1}|\boldsymbol{y}_{1:k-1})$，这个概率分布的含义是什么呢？我们现在已经有了前 $k-1$ 个时刻的观测值 $\boldsymbol{y}_{1:k-1}$，并且根据这些观测值估计了 $k-1$ 时刻隐变量 $x_{k-1}$ 的<strong>后验概率</strong>分布，也即 $p({x}_{k-1}|\boldsymbol{y}_{1:k-1})$。那么接下来到 $k$ 时刻，我们又获得了一个观测 $y_k$，要怎么估计新的后验概率分布 $p({x}_{k}|\boldsymbol{y}_{1:k})$ 呢？我们通过<strong>预测</strong>和<strong>更新</strong>两个阶段来估计，下面我就解释一下这两个阶段的含义。</p>
<h3 id="1-1-预测"><a href="#1-1-预测" class="headerlink" title="1.1 预测"></a>1.1 预测</h3><p>前面说了我们有系统模型和观测模型，首先根据系统模型，我们有了 $k-1$ 时刻的后验，就可以根据 $x_{k-1}$ 到 $x_k$ 的转移模型得到 $x_k$ 对应的概率分布，这个过程就叫做<strong>预测(Update)</strong>。通过预测阶段，我们可以获得<strong>先验概率</strong>分布(注意这里我们将 $p({x}_k|\boldsymbol{y}_{1:k-1})$ 称为先验概率分布)</p>
<script type="math/tex; mode=display">
\begin{align}p({x}_k|\boldsymbol{y}_{1:k-1})=\int p({x}_k|x_{k-1})p(x_{k-1}|\boldsymbol{y}_{1:k-1})dx_{k-1}\end{align}</script><p>也就是说，我们即使没有观测值，也能对 $x_k$ 的分布进行估计，但是由于没有利用观测值 $y_k$ 带来的信息，因此这个估计是不准确的，下面我们就需要用观测值对这个先验概率分布进行纠正，也就是 <strong>更新</strong>阶段。</p>
<h3 id="1-2-更新"><a href="#1-2-更新" class="headerlink" title="1.2 更新"></a>1.2 更新</h3><p>有了<strong>先验</strong>，又有了观测，根据贝叶斯公式，我们可以很容易得到<strong>后验概率</strong>分布 $p\left(x_{k} | \boldsymbol{y}_{1: k}\right)$。怎么理解下面一个式子呢？我们有似然函数 $p\left(y_{k} | x_{k}\right)$，现在有了观测 $y_k$，那么似然值越大，表明对应的 $x_k$ 的概率应该也越大， 就是用<strong>似然函数</strong>对<strong>先验概率</strong>进行<strong>加权</strong>就得到了<strong>后验概率</strong>。</p>
<script type="math/tex; mode=display">
\begin{align}
p\left(x_{k} | \boldsymbol{y}_{1: k}\right)&=\frac{p\left(y_{k} | x_{k},\boldsymbol{y}_{1: k-1}\right) p\left(x_{k} | \boldsymbol{y}_{1: k-1}\right)}{p\left(y_{k} | \boldsymbol{y}_{1: k-1}\right)} \\
&\propto p\left(y_{k} | x_{k},\boldsymbol{y}_{1: k-1}\right) p\left(x_{k} | \boldsymbol{y}_{1: k-1}\right) \\
&= p\left(y_{k} | x_{k}\right) p\left(x_{k} | \boldsymbol{y}_{1: k-1}\right)
\end{align}</script><blockquote>
<p>怎么理解这个加权呢？举个栗子</p>
<p>比如</p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总结起来，我们滤波分为两个阶段</p>
<script type="math/tex; mode=display">
\begin{align}
\text{Prediction}&: p({x}_k|\boldsymbol{y}_{1:k-1})=\int p({x}_k|x_{k-1})p(x_{k-1}|\boldsymbol{y}_{1:k-1})dx_{k-1} \\
\text{Update}&: p\left(x_{k} | \boldsymbol{y}_{1: k}\right)\propto p\left(y_{k} | x_{k}\right) p\left(x_{k} | \boldsymbol{y}_{1: k-1}\right)
\end{align}</script><p>可以看到，上面的预测阶段含有<strong>积分</strong>，这在实际当中往往是很难计算的，而对于那些非常规的PDF，甚至不能给出解析解。解决这种问题一般有两种思路：</p>
<ul>
<li>建立简单的模型，获得解析解，如卡尔曼滤波(Kalman Filter)及EKF、UKF等；</li>
<li>建立复杂的模型，获得近似解，如粒子滤波(Particle Filter)等。</li>
</ul>
<h2 id="2-卡尔曼滤波"><a href="#2-卡尔曼滤波" class="headerlink" title="2. 卡尔曼滤波"></a>2. 卡尔曼滤波</h2><p>卡尔曼滤波的思路就是将系统建模<strong>线性模型</strong>，隐变量、控制变量、控制噪声与观测噪声均为高斯分布，那么观测变量也是随机变量，整个模型中所有的随机变量都是<strong>高斯</strong>的！高斯分布是我们最喜欢的分布，因为在前面贝叶斯滤波的<strong>预测阶段</strong>我们可以不用积分了，只需要计算均值和协方差就可以了！根据系统模型和观测模型，我们可以获得滤波的闭式解，这就是卡尔曼滤波的思想。</p>
<script type="math/tex; mode=display">
\begin{align}
\text{System Model}&: \mathrm{x}_{k}=A\mathrm{x}_{k-1} + B\mathrm{u}_{k-1} + v_{k-1} \\
\text{Observation Model}&: \mathrm{y}_{k}=C\mathrm{x}_{k}+ \mathrm{n}_{k}
\end{align}</script><p>但实际中要求线性系统模型往往是很困难的，对于非线性模型，如果我们还想应用卡尔曼滤波该怎么办呢？线性近似，也就是求一个雅可比矩阵(Jacobi)，这就是扩展卡尔曼滤波(EKF)。</p>
<h2 id="3-大数定律"><a href="#3-大数定律" class="headerlink" title="3. 大数定律"></a>3. 大数定律</h2><p>粒子滤波是什么是意思呢？我们可以用大量的采样值来描述一个概率分布，当我们按照一个概率分布进行采样的时候，某个点的概率密度越高，这个点被采到的概率越大，当采样数目足够大(趋于无穷)的时候，粒子出现的频率就可以用来表示对应的分布，实际中我们可以理解一个粒子就是一个采样。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/pf_particle.png" alt="particle"></p>
<p>但是对于<strong>离散型</strong>的随机变量还好，可以很容易的求出来状态空间中每个状态的频率。但如果对于<strong>连续分布</strong>，其实是很不方便的，所幸实际中我们也不需要获得概率分布的全部信息，一般只需要求出<strong>期望</strong>就可以了。</p>
<p>下面为了公式书写和推导方便，以离散型随机变量为例，假设状态空间为 $\mathcal{Z}=\{1,2,…,K\}$，有 $N$ 个采样样本 $z_i \in \mathcal{Z}$，服从概率分布 $q(\mathbf{z})$。我们将根据 $N$ 个采样样本 $\boldsymbol{z}_{1:N}$ 得到的分布记为<strong>经验分布</strong> $\hat{p}(b|\boldsymbol{z}_{1:N}) = \frac{1}{N}\sum_i \mathbb{I}(b-z_i)$，其中 $\mathbb{I}(\cdot)$ 为指示函数，那么当 $N$ 足够大的时候，经验分布 $\hat{p}(b|\boldsymbol{z}_{1:N})$ 就足够接近真实分布 $q(\mathbf{z})$。现在我们想估计随机变量 $\mathsf{t}=g(\mathsf{z})$ 的期望，即</p>
<script type="math/tex; mode=display">
\begin{align}
\mathbb{E}[\mathsf{t}] &= \mathbb{E}[g(\mathsf{z})]=\sum_{b\in\mathcal{Z}}g(b)q(b) \notag\\
&\approx \sum_b g(b)\hat{p}(b|\boldsymbol{z}_{1:N}) \notag\\
&= \sum_b g(b)\frac{1}{N}\sum_i \mathbb{I}(b-z_i) \notag\\
&= \frac{1}{N}\sum_i g(z_i)
\end{align}</script><p>也就是说，我们只需要对样本进行<strong>简单求和</strong>就可以了！连续型随机变量也是类似的，所以在后面的粒子滤波中，我们利用粒子估计了概率密度分布，要想求期望就只需要进行求和平均就可以了。</p>
<h2 id="4-粒子滤波简单实例"><a href="#4-粒子滤波简单实例" class="headerlink" title="4. 粒子滤波简单实例"></a>4. 粒子滤波简单实例</h2><p>好了，有了前面的预备知识，我们可以先看一个粒子滤波的简单例子。</p>
<p>假设我们现在有采样好的 $N$ 个粒子 $\{x_{k-1}^i\}_{i=1}^N$，他们服从分布 $p({x}_{k-1}|\boldsymbol{y}_{1:k-1})$，那么我们现在如何进行贝叶斯滤波中的<strong>预测</strong>和<strong>更新</strong>阶段呢？</p>
<h3 id="4-1-预测"><a href="#4-1-预测" class="headerlink" title="4.1 预测"></a>4.1 预测</h3><p>回顾一下预测的公式</p>
<script type="math/tex; mode=display">
\text{Prediction}: p({x}_k|\boldsymbol{y}_{1:k-1})=\int p({x}_k|x_{k-1})p(x_{k-1}|\boldsymbol{y}_{1:k-1})dx_{k-1} \\</script><p>想一下：只要我们让每个粒子 $x_{k-1}^i$ “<strong>进化</strong>”一步，也就是说按照分布 $p(x_k|x_{k-1})$ 进行采样，使得 $x_k^i \sim p(x_k|x_{k-1}^i)$，这样我们就获得了 $N$ 个新的粒子 $\{x_k^i\}_{i=1}^N$。很容易验证，如果 $\{x_{k-1}^i\}_{i=1}^N$ 能够很好的表示 $p({x}_{k-1}|\boldsymbol{y}_{1:k-1})$ 的话，那么 $\{x_k^i\}_{i=1}^N$ 也能很好的表示 $p({x}_k|\boldsymbol{y}_{1:k-1})$(这一部分可以凭直观感觉来理解，这是一个很自然的事情，也可以用前面的经验分布的思路进行公式推导)。</p>
<p>这样做的好处是什么呢？我们<strong>避免了积分</strong>！只需要对一个已知的分布 $p(x_k|x_{k-1}^i)$ 进行采样，而这个分布是我们假设的系统模型，可以是很简单的，因此采样也很方便。</p>
<h3 id="4-2-更新"><a href="#4-2-更新" class="headerlink" title="4.2 更新"></a>4.2 更新</h3><p>通过预测我们获得了 $x_k^i \sim p(x_k|x_{k-1}^i)$，这 $N$ 个粒子就描述了<strong>先验概率</strong>分布。接下来就是更新，再回顾一下更新的公式</p>
<script type="math/tex; mode=display">
\text{Update}: p\left(x_{k} | \boldsymbol{y}_{1: k}\right)\propto p\left(y_{k} | x_{k}\right) p\left(x_{k} | \boldsymbol{y}_{1: k-1}\right)</script><p>更新是什么呢？前面提到了：更新就是用<strong>似然函数</strong>对<strong>先验概率</strong>进行<strong>加权</strong>就得到了<strong>后验概率</strong>。如果简单的把 $x_k^i$ 带入到上面的公式里，就可以得到下面的式子</p>
<script type="math/tex; mode=display">
p\left(x_{k}^i | \boldsymbol{y}_{1: k}\right)\propto p\left(y_{k} | x_{k}^i\right) p\left(x_{k}^i | \boldsymbol{y}_{1: k-1}\right)</script><p>实际上就表示每个<strong>粒子</strong>有不同的<strong>权重</strong>。</p>
<blockquote>
<p>这里怎么理解呢？想一下前面提到的<strong>经验分布</strong>，我们只需要用粒子出现的<strong>频率</strong>来表示对应的概率，实际上就是在统计频率的时候每个粒子的权重都是相同的，出现一次计数就加一。</p>
<p>而这里的区别是什么呢？由于我们有一个观测值 $y_k$，根据 $y_k$ 来反推，某一个粒子 $i_1$ 的导致 $y_k$ 出现的概率更大，那么我们在统计频率的时候，就给这个粒子加一个更大的权重，以表示我们更相信/重视这个粒子。</p>
</blockquote>
<p>那么问题来了，前面我们说了滤波过程中要想求期望，只需要简单求和就可以了</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathsf{t}] = \frac{1}{N}\sum_i g(z_i)</script><p>现在粒子有了权重怎么办呢，同理，加个权就好了(推导也很简单，相信大家都会)，下面的式子里对权重进行了归一化</p>
<script type="math/tex; mode=display">
\mathbb{E}[\mathsf{t}] = \sum_i \frac{w_i}{\sum_j w_j}g(z_i)</script><h3 id="4-3-递推"><a href="#4-3-递推" class="headerlink" title="4.3 递推"></a>4.3 递推</h3><p>前面只讲了一次预测和一次更新的过程，注意到我们前面只是从 $k-1$ 到 $k$ 时刻的滤波过程，但每一轮循环原理都是相同的。</p>
<p>不过细心的朋友们可能会觉得不太对劲，前面一个阶段的例子里，预测之前我们有采样 $\{x_{k-1}^i\}_{i=1}^N$，推导过程中我们是默认这些粒子的权重都是相同的，然后我们进行了预测和更新，但是更新之后我们给每个粒子加权了呀，到下一个阶段怎么办？！！！也很简单，预测阶段不必要求每个粒子的权重都相同，也加上一个权重就好了。也就是说我们时时刻刻都保存有每个粒子的权重信息。</p>
<p>这样我们就可以得到一个完整的粒子滤波算法了！但是还有问题！</p>
<h3 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h3><p>但是呢，有人发现了上面的算法不好啊！不是因为麻烦，而是滤波到最后会发现只有少数一部分粒子有很大的权重，而其他绝大部分粒子的权重几乎为 0，这就意味着对于那些“不重要”的粒子，他们在我们求期望的时候贡献并不大，但是我们却花费了大量的计算精力来维护这些粒子，这样不好不好！于是就有人提出了<strong>重采样</strong>。</p>
<p>重采样什么意思呢？你们粒子的权重不是不同吗，那我就采取手段给你们整相同了！简单理解，有两个粒子，粒子 $a$ 的权重是 8，粒子 $b$ 权重是 2，那我就把粒子 $a$ 复制 8 份，粒子 $b$ 复制 2 份，这样得到的 10 个粒子权重就都是 1 了。但是另一个问题是一开始我们只有 2 个粒子，这样弄完我们就有 10 个粒子了，如果一直这么做我们的粒子数会越来越多，算力跟不上。那就从这 10 个粒子里均匀分布地随机抽 2 个！那么粒子 $a$ 的概率就是 $0.8$，这就成了。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/pf_resample.png" alt="resample"></p>
<p>至此，加上重采样我们就获得了一个完整的比较好用的粒子滤波算法</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/pf_sir.PNG" alt="sir"></p>
<h2 id="5-粒子滤波原理推导"><a href="#5-粒子滤波原理推导" class="headerlink" title="5. 粒子滤波原理推导"></a>5. 粒子滤波原理推导</h2><p>但是还有一个问题，就是前面我们直接说有 $N$ 个粒子 $x_{k-1}^i \sim p({x}_{k-1}|\boldsymbol{y}_{1:k-1})$，但是第 1 步(刚开始)的时候我们怎么获得这些粒子来描述 $p(x_1|y_1)$ 啊？如果是高斯分布还好，我们可以很方便的对高斯分布进行采样，但是如果是一个特别特别特别复杂的分布呢？我们甚至不能写出解析表达式，更没办法按照这个分布进行随机采样，那我们是不是就凉了？不！我们前面不是讲了粒子可以有权重嘛。</p>
<p>假如我们现在有另一个分布 $q(\mathbf{x}_{1:k}|\mathbf{y}_{1:k})$，这个分布是我们自己指定的，可以很简单，而且我们可以按照这个分布来进行采样，那么我们就有 $x_{k-1}^i \sim q(\mathbf{x}_{1:k}|\mathbf{y}_{1:k})$，那么我们怎么用这些粒子来表示我们想要得到的真正的分布 $p(\mathbf{x}_{0:k}|\mathbf{y}_{1:k})$ 呢？再加个权就行了，就像前面用似然函数进行加权。</p>
<script type="math/tex; mode=display">
\begin{align}
p(\mathbf{x}_{0:k}|\mathbf{y}_{1:k})&\approx \sum_{i=1}^{N}w_k^i\delta(\mathbf{x}_{0:k}-\mathbf{x}_{0:k}^i) \notag \\
w_{k}^{i} &\propto \frac{p\left(\mathbf{x}_{0: k}^{i} | \mathbf{y}_{1: k}\right)}{q\left(\mathbf{x}_{0: k}^{i} | \mathbf{y}_{1: k}\right)}
\notag\end{align}</script><p>再考虑前面提到的的<strong>预测</strong>和<strong>更新</strong>两个递推进行的阶段，就有</p>
<script type="math/tex; mode=display">
\begin{align}q\left(\mathbf{x}_{0: k} | \mathbf{y}_{1: k}\right)&=q\left(\mathbf{x}_{k} | \mathbf{x}_{0: k-1}, \mathbf{y}_{1: k}\right) q\left(\mathbf{x}_{0: k-1} | \mathbf{y}_{1: k-1}\right) \notag\\ 
            &= q\left(\mathbf{x}_{k} | \mathbf{x}_{k-1}, \mathbf{y}_{ k}\right) q\left(\mathbf{x}_{0: k-1} | \mathbf{y}_{1: k-1}\right) \notag \\
            w_{k}^{i} &\propto w_{k-1}^{i} \frac{p\left(\mathbf{y}_{k} | \mathbf{x}_{k}^{i}\right) p\left(\mathbf{x}_{k}^{i} | \mathbf{x}_{k-1}^{i}\right)}{q\left(\mathbf{x}_{k}^{i} | \mathbf{x}_{k-1}^{i}, \mathbf{y}_{k}\right)} \label{eq_sis}
            \end{align}</script><p>然后我们就得到了一个更加 universal/generic 的粒子滤波算法(下面图片所示算法中没加重采样步骤)。</p>
<p><img src="https://raw.githubusercontent.com/Glooow1024/ImgHosting/master/hexo/2019/pf_sis.PNG" alt="sis"></p>
<h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>这篇文章主要是自己学习粒子滤波之后的一些理解，主要参考了文章 <a href="https://ieeexplore.ieee.org/document/978374" target="_blank" rel="noopener">A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking</a>，但是这篇文章中是从 general 的情况开始讲，然后举了一些常用的特例，个人感觉不利于初学者理解，因此本文写作过程中的思路是从简单的特例开始再到更一般的情况。</p>
<p>最后一部分 <a href="## 粒子滤波原理推导">粒子滤波原理推导</a> 其实写的并没有很清楚，主要是因为自己太懒，到最后懒得一个个手敲公式了，如果想更清楚地了解其中的细节，可以阅读上面那篇论文。</p>
<h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><ol>
<li>M. S. Arulampalam, S. Maskell, N. Gordon and T. Clapp, “A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking,” in <em>IEEE Transactions on Signal Processing</em>, vol. 50, no. 2, pp. 174-188, Feb. 2002.</li>
</ol>
]]></content>
      <categories>
        <category>统计推断</category>
      </categories>
      <tags>
        <tag>滤波算法</tag>
        <tag>粒子滤波</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/09/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Adobe Acrobat Pro DC更新后提示登录激活问题</title>
    <url>/2002/08/05/software/acrobat/</url>
    <content><![CDATA[<p>Adobe Acrobat Pro DC 更新之后不能直接用 AMTEmu v0.9.2 激活了。</p>
<p>不过只需要修改以下注册表再重新激活就可以了。</p>
<p>通过 <code>win+R</code> 输入 <code>regedit</code> 打开注册表，在以下位置处</p>
<figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">[HKEY_LOCAL_MACHINE<span class="symbol">\S</span>OFTWARE<span class="symbol">\W</span>OW6432Node<span class="symbol">\A</span>dobe<span class="symbol">\A</span>dobe Acrobat<span class="symbol">\D</span>C<span class="symbol">\A</span>ctivation]</span><br></pre></td></tr></table></figure>
<p>创建一个 <code>DWORD(32位)</code> 类型的项，数值为十六进制 0x00000001。</p>
<p>然后就可以用 AMTEmu v0.9.2 重新激活了。</p>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Adobe</tag>
      </tags>
  </entry>
</search>
